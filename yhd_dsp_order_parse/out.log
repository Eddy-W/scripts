hive -e "
create table if not exists user_wangwentao_yhd_bidt_0317 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-17' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-17' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-17' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403181440_820792535.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_685929, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_685929
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_685929
2014-03-18 14:40:38,481 Stage-1 map = 0%,  reduce = 0%
2014-03-18 14:40:49,159 Stage-1 map = 5%,  reduce = 0%
2014-03-18 14:40:50,206 Stage-1 map = 31%,  reduce = 0%
2014-03-18 14:40:51,286 Stage-1 map = 42%,  reduce = 0%
2014-03-18 14:40:52,296 Stage-1 map = 44%,  reduce = 0%
2014-03-18 14:40:57,350 Stage-1 map = 46%,  reduce = 0%
2014-03-18 14:40:58,416 Stage-1 map = 74%,  reduce = 0%
2014-03-18 14:40:59,427 Stage-1 map = 82%,  reduce = 0%
2014-03-18 14:41:00,437 Stage-1 map = 87%,  reduce = 0%
2014-03-18 14:41:01,580 Stage-1 map = 89%,  reduce = 0%
2014-03-18 14:41:03,650 Stage-1 map = 91%,  reduce = 0%
2014-03-18 14:41:04,661 Stage-1 map = 95%,  reduce = 0%
2014-03-18 14:41:10,884 Stage-1 map = 97%,  reduce = 0%
2014-03-18 14:41:41,163 Stage-1 map = 98%,  reduce = 0%
2014-03-18 14:41:43,179 Stage-1 map = 99%,  reduce = 0%
2014-03-18 14:41:51,301 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:42:51,771 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:43:52,570 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:44:59,025 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:45:59,414 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:46:59,912 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:48:00,097 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:49:02,388 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:50:18,302 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:51:19,023 Stage-1 map = 100%,  reduce = 0%
2014-03-18 14:51:23,084 Stage-1 map = 100%,  reduce = 33%
2014-03-18 14:51:24,097 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_685929
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686009, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686009
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686009
2014-03-18 14:51:31,521 Stage-5 map = 0%,  reduce = 0%
2014-03-18 14:51:35,541 Stage-5 map = 1%,  reduce = 0%
2014-03-18 14:51:36,552 Stage-5 map = 3%,  reduce = 0%
2014-03-18 14:51:37,559 Stage-5 map = 4%,  reduce = 0%
2014-03-18 14:51:38,564 Stage-5 map = 6%,  reduce = 0%
2014-03-18 14:51:39,578 Stage-5 map = 11%,  reduce = 0%
2014-03-18 14:51:40,628 Stage-5 map = 16%,  reduce = 0%
2014-03-18 14:51:41,645 Stage-5 map = 30%,  reduce = 0%
2014-03-18 14:51:42,653 Stage-5 map = 39%,  reduce = 0%
2014-03-18 14:51:43,668 Stage-5 map = 47%,  reduce = 0%
2014-03-18 14:51:44,674 Stage-5 map = 54%,  reduce = 0%
2014-03-18 14:51:45,764 Stage-5 map = 62%,  reduce = 0%
2014-03-18 14:51:46,785 Stage-5 map = 66%,  reduce = 0%
2014-03-18 14:51:47,981 Stage-5 map = 70%,  reduce = 0%
2014-03-18 14:51:49,055 Stage-5 map = 76%,  reduce = 0%
2014-03-18 14:51:50,106 Stage-5 map = 79%,  reduce = 0%
2014-03-18 14:51:51,149 Stage-5 map = 83%,  reduce = 0%
2014-03-18 14:51:52,475 Stage-5 map = 86%,  reduce = 0%
2014-03-18 14:51:53,483 Stage-5 map = 88%,  reduce = 0%
2014-03-18 14:51:54,512 Stage-5 map = 90%,  reduce = 0%
2014-03-18 14:51:55,522 Stage-5 map = 91%,  reduce = 0%
2014-03-18 14:51:58,526 Stage-5 map = 92%,  reduce = 0%
2014-03-18 14:52:02,110 Stage-5 map = 93%,  reduce = 0%
2014-03-18 14:52:05,270 Stage-5 map = 94%,  reduce = 0%
2014-03-18 14:52:11,633 Stage-5 map = 95%,  reduce = 0%
2014-03-18 14:52:19,961 Stage-5 map = 96%,  reduce = 0%
2014-03-18 14:52:29,512 Stage-5 map = 97%,  reduce = 0%
2014-03-18 14:52:54,598 Stage-5 map = 98%,  reduce = 0%
2014-03-18 14:52:57,133 Stage-5 map = 100%,  reduce = 0%
2014-03-18 14:53:03,071 Stage-5 map = 100%,  reduce = 9%
2014-03-18 14:53:04,341 Stage-5 map = 100%,  reduce = 18%
2014-03-18 14:53:05,510 Stage-5 map = 100%,  reduce = 33%
2014-03-18 14:53:06,523 Stage-5 map = 100%,  reduce = 37%
2014-03-18 14:53:07,531 Stage-5 map = 100%,  reduce = 70%
2014-03-18 14:53:08,543 Stage-5 map = 100%,  reduce = 78%
2014-03-18 14:53:09,550 Stage-5 map = 100%,  reduce = 82%
2014-03-18 14:53:10,557 Stage-5 map = 100%,  reduce = 92%
2014-03-18 14:53:11,563 Stage-5 map = 100%,  reduce = 93%
2014-03-18 14:53:12,573 Stage-5 map = 100%,  reduce = 97%
2014-03-18 14:53:13,580 Stage-5 map = 100%,  reduce = 99%
2014-03-18 14:53:16,629 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686009
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 527
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686033, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686033
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686033
2014-03-18 14:53:33,029 Stage-7 map = 0%,  reduce = 0%
2014-03-18 14:53:43,359 Stage-7 map = 1%,  reduce = 0%
2014-03-18 14:53:44,423 Stage-7 map = 3%,  reduce = 0%
2014-03-18 14:53:45,474 Stage-7 map = 4%,  reduce = 0%
2014-03-18 14:53:46,496 Stage-7 map = 5%,  reduce = 0%
2014-03-18 14:53:47,914 Stage-7 map = 8%,  reduce = 0%
2014-03-18 14:53:49,189 Stage-7 map = 11%,  reduce = 0%
2014-03-18 14:53:52,442 Stage-7 map = 12%,  reduce = 0%
2014-03-18 14:53:56,510 Stage-7 map = 13%,  reduce = 0%
2014-03-18 14:54:03,237 Stage-7 map = 14%,  reduce = 0%
2014-03-18 14:54:09,748 Stage-7 map = 15%,  reduce = 0%
2014-03-18 14:54:13,163 Stage-7 map = 16%,  reduce = 0%
2014-03-18 14:54:15,404 Stage-7 map = 17%,  reduce = 0%
2014-03-18 14:54:18,439 Stage-7 map = 18%,  reduce = 0%
2014-03-18 14:54:21,751 Stage-7 map = 19%,  reduce = 0%
2014-03-18 14:54:24,873 Stage-7 map = 20%,  reduce = 0%
2014-03-18 14:54:26,901 Stage-7 map = 21%,  reduce = 0%
2014-03-18 14:54:29,966 Stage-7 map = 22%,  reduce = 0%
2014-03-18 14:54:30,980 Stage-7 map = 23%,  reduce = 0%
2014-03-18 14:54:34,033 Stage-7 map = 24%,  reduce = 0%
2014-03-18 14:54:35,044 Stage-7 map = 25%,  reduce = 0%
2014-03-18 14:54:37,062 Stage-7 map = 26%,  reduce = 0%
2014-03-18 14:54:38,072 Stage-7 map = 27%,  reduce = 0%
2014-03-18 14:54:39,095 Stage-7 map = 28%,  reduce = 0%
2014-03-18 14:54:40,106 Stage-7 map = 29%,  reduce = 0%
2014-03-18 14:54:42,133 Stage-7 map = 31%,  reduce = 0%
2014-03-18 14:54:43,145 Stage-7 map = 32%,  reduce = 0%
2014-03-18 14:54:44,159 Stage-7 map = 33%,  reduce = 0%
2014-03-18 14:55:18,496 Stage-7 map = 34%,  reduce = 0%
2014-03-18 14:55:20,217 Stage-7 map = 44%,  reduce = 0%
2014-03-18 14:55:21,309 Stage-7 map = 71%,  reduce = 0%
2014-03-18 14:55:23,467 Stage-7 map = 72%,  reduce = 0%
2014-03-18 14:55:25,553 Stage-7 map = 73%,  reduce = 0%
2014-03-18 14:55:27,583 Stage-7 map = 74%,  reduce = 0%
2014-03-18 14:55:28,608 Stage-7 map = 75%,  reduce = 0%
2014-03-18 14:55:29,637 Stage-7 map = 76%,  reduce = 0%
2014-03-18 14:55:30,708 Stage-7 map = 77%,  reduce = 0%
2014-03-18 14:55:31,777 Stage-7 map = 78%,  reduce = 0%
2014-03-18 14:55:32,850 Stage-7 map = 79%,  reduce = 0%
2014-03-18 14:55:33,886 Stage-7 map = 81%,  reduce = 0%
2014-03-18 14:55:36,114 Stage-7 map = 82%,  reduce = 0%
2014-03-18 14:55:37,151 Stage-7 map = 83%,  reduce = 0%
2014-03-18 14:55:38,238 Stage-7 map = 84%,  reduce = 0%
2014-03-18 14:55:40,521 Stage-7 map = 85%,  reduce = 0%
2014-03-18 14:55:41,551 Stage-7 map = 86%,  reduce = 0%
2014-03-18 14:55:42,570 Stage-7 map = 87%,  reduce = 0%
2014-03-18 14:55:45,721 Stage-7 map = 88%,  reduce = 0%
2014-03-18 14:55:47,776 Stage-7 map = 89%,  reduce = 0%
2014-03-18 14:55:52,146 Stage-7 map = 90%,  reduce = 0%
2014-03-18 14:55:57,284 Stage-7 map = 91%,  reduce = 0%
2014-03-18 14:56:03,458 Stage-7 map = 92%,  reduce = 0%
2014-03-18 14:56:10,747 Stage-7 map = 93%,  reduce = 0%
2014-03-18 14:56:18,258 Stage-7 map = 94%,  reduce = 0%
2014-03-18 14:56:24,505 Stage-7 map = 95%,  reduce = 0%
2014-03-18 14:56:36,821 Stage-7 map = 96%,  reduce = 0%
2014-03-18 14:56:51,110 Stage-7 map = 97%,  reduce = 0%
2014-03-18 14:58:02,437 Stage-7 map = 97%,  reduce = 0%
2014-03-18 14:58:04,121 Stage-7 map = 99%,  reduce = 0%
2014-03-18 14:58:12,663 Stage-7 map = 99%,  reduce = 6%
2014-03-18 14:58:13,674 Stage-7 map = 99%,  reduce = 11%
2014-03-18 14:58:14,692 Stage-7 map = 99%,  reduce = 14%
2014-03-18 14:58:15,708 Stage-7 map = 99%,  reduce = 15%
2014-03-18 14:58:16,719 Stage-7 map = 99%,  reduce = 22%
2014-03-18 14:58:17,730 Stage-7 map = 99%,  reduce = 29%
2014-03-18 14:58:18,741 Stage-7 map = 99%,  reduce = 31%
2014-03-18 14:58:21,779 Stage-7 map = 99%,  reduce = 32%
2014-03-18 14:58:41,222 Stage-7 map = 100%,  reduce = 32%
2014-03-18 14:59:08,830 Stage-7 map = 100%,  reduce = 33%
2014-03-18 15:00:09,143 Stage-7 map = 100%,  reduce = 33%
2014-03-18 15:02:11,754 Stage-7 map = 100%,  reduce = 33%
2014-03-18 15:02:23,948 Stage-7 map = 100%,  reduce = 50%
2014-03-18 15:02:25,708 Stage-7 map = 100%,  reduce = 86%
2014-03-18 15:02:26,785 Stage-7 map = 100%,  reduce = 100%
2014-03-18 15:03:30,991 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686033
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686109, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686109
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686109
2014-03-18 15:03:41,836 Stage-6 map = 0%,  reduce = 0%
2014-03-18 15:03:49,310 Stage-6 map = 18%,  reduce = 0%
2014-03-18 15:03:50,320 Stage-6 map = 36%,  reduce = 0%
2014-03-18 15:03:52,053 Stage-6 map = 55%,  reduce = 0%
2014-03-18 15:03:54,993 Stage-6 map = 64%,  reduce = 0%
2014-03-18 15:04:03,682 Stage-6 map = 73%,  reduce = 0%
2014-03-18 15:04:22,993 Stage-6 map = 100%,  reduce = 0%
2014-03-18 15:04:30,444 Stage-6 map = 100%,  reduce = 30%
2014-03-18 15:04:39,614 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686109
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686148, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686148
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686148
2014-03-18 15:06:45,718 Stage-8 map = 0%,  reduce = 0%
2014-03-18 15:06:49,760 Stage-8 map = 1%,  reduce = 0%
2014-03-18 15:06:58,404 Stage-8 map = 10%,  reduce = 0%
2014-03-18 15:07:00,090 Stage-8 map = 16%,  reduce = 0%
2014-03-18 15:07:01,418 Stage-8 map = 21%,  reduce = 0%
2014-03-18 15:07:02,510 Stage-8 map = 25%,  reduce = 0%
2014-03-18 15:07:03,687 Stage-8 map = 28%,  reduce = 0%
2014-03-18 15:07:04,796 Stage-8 map = 32%,  reduce = 0%
2014-03-18 15:07:05,928 Stage-8 map = 37%,  reduce = 0%
2014-03-18 15:07:07,014 Stage-8 map = 41%,  reduce = 0%
2014-03-18 15:07:08,052 Stage-8 map = 45%,  reduce = 0%
2014-03-18 15:07:09,089 Stage-8 map = 48%,  reduce = 0%
2014-03-18 15:07:11,208 Stage-8 map = 54%,  reduce = 0%
2014-03-18 15:07:16,890 Stage-8 map = 63%,  reduce = 0%
2014-03-18 15:07:18,605 Stage-8 map = 74%,  reduce = 0%
2014-03-18 15:07:20,071 Stage-8 map = 81%,  reduce = 0%
2014-03-18 15:07:21,429 Stage-8 map = 83%,  reduce = 0%
2014-03-18 15:07:22,817 Stage-8 map = 86%,  reduce = 0%
2014-03-18 15:07:23,894 Stage-8 map = 87%,  reduce = 0%
2014-03-18 15:07:24,968 Stage-8 map = 89%,  reduce = 0%
2014-03-18 15:07:26,095 Stage-8 map = 90%,  reduce = 0%
2014-03-18 15:07:28,740 Stage-8 map = 91%,  reduce = 0%
2014-03-18 15:07:30,575 Stage-8 map = 93%,  reduce = 0%
2014-03-18 15:07:34,099 Stage-8 map = 98%,  reduce = 0%
2014-03-18 15:07:36,040 Stage-8 map = 100%,  reduce = 0%
2014-03-18 15:07:41,738 Stage-8 map = 100%,  reduce = 33%
2014-03-18 15:07:47,548 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686148
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686194, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686194
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686194
2014-03-18 15:10:32,939 Stage-2 map = 0%,  reduce = 0%
2014-03-18 15:10:34,129 Stage-2 map = 100%,  reduce = 0%
2014-03-18 15:11:49,794 Stage-2 map = 100%,  reduce = 0%
2014-03-18 15:11:55,019 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686194
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686250, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686250
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686250
2014-03-18 15:12:01,349 Stage-3 map = 0%,  reduce = 0%
2014-03-18 15:12:11,269 Stage-3 map = 100%,  reduce = 0%
2014-03-18 15:12:24,743 Stage-3 map = 100%,  reduce = 33%
2014-03-18 15:13:28,497 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686250
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0317
8 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-18_14-40-03_231_4811224158344364500/-ext-10000
OK
Time taken: 2010.093 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0317 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-17' and dd>='2014-03-16' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-17' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-17' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403181513_1558500085.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686281, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686281
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686281
2014-03-18 15:14:52,831 Stage-1 map = 0%,  reduce = 0%
2014-03-18 15:14:54,747 Stage-1 map = 12%,  reduce = 0%
2014-03-18 15:15:09,484 Stage-1 map = 32%,  reduce = 0%
2014-03-18 15:15:12,199 Stage-1 map = 98%,  reduce = 0%
2014-03-18 15:15:20,147 Stage-1 map = 99%,  reduce = 0%
2014-03-18 15:15:22,888 Stage-1 map = 99%,  reduce = 33%
2014-03-18 15:15:29,553 Stage-1 map = 100%,  reduce = 33%
2014-03-18 15:15:38,756 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686281
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686354, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686354
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686354
2014-03-18 15:15:45,279 Stage-3 map = 0%,  reduce = 0%
2014-03-18 15:15:50,083 Stage-3 map = 1%,  reduce = 0%
2014-03-18 15:15:51,913 Stage-3 map = 3%,  reduce = 0%
2014-03-18 15:15:53,112 Stage-3 map = 18%,  reduce = 0%
2014-03-18 15:15:54,396 Stage-3 map = 32%,  reduce = 0%
2014-03-18 15:15:56,282 Stage-3 map = 57%,  reduce = 0%
2014-03-18 15:16:35,312 Stage-3 map = 75%,  reduce = 0%
2014-03-18 15:16:38,423 Stage-3 map = 95%,  reduce = 0%
2014-03-18 15:16:39,821 Stage-3 map = 98%,  reduce = 0%
2014-03-18 15:16:45,508 Stage-3 map = 99%,  reduce = 0%
2014-03-18 15:16:52,411 Stage-3 map = 100%,  reduce = 0%
2014-03-18 15:16:54,879 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686354
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686403, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686403
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686403
2014-03-18 15:17:08,980 Stage-4 map = 0%,  reduce = 0%
2014-03-18 15:17:13,588 Stage-4 map = 12%,  reduce = 0%
2014-03-18 15:17:15,512 Stage-4 map = 30%,  reduce = 0%
2014-03-18 15:17:17,906 Stage-4 map = 41%,  reduce = 0%
2014-03-18 15:17:19,321 Stage-4 map = 60%,  reduce = 0%
2014-03-18 15:17:20,504 Stage-4 map = 73%,  reduce = 0%
2014-03-18 15:17:21,699 Stage-4 map = 84%,  reduce = 0%
2014-03-18 15:17:22,800 Stage-4 map = 88%,  reduce = 0%
2014-03-18 15:17:23,988 Stage-4 map = 92%,  reduce = 0%
2014-03-18 15:17:25,791 Stage-4 map = 95%,  reduce = 0%
2014-03-18 15:17:27,548 Stage-4 map = 97%,  reduce = 0%
2014-03-18 15:17:28,876 Stage-4 map = 98%,  reduce = 0%
2014-03-18 15:17:32,311 Stage-4 map = 99%,  reduce = 0%
2014-03-18 15:17:35,470 Stage-4 map = 99%,  reduce = 33%
2014-03-18 15:17:36,575 Stage-4 map = 100%,  reduce = 33%
2014-03-18 15:17:42,266 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686403
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686456, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686456
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686456
2014-03-18 15:17:55,137 Stage-2 map = 0%,  reduce = 0%
2014-03-18 15:18:00,789 Stage-2 map = 100%,  reduce = 0%
2014-03-18 15:18:13,073 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686456
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0317
22 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-18_15-13-35_029_2278413721081623675/-ext-10000
OK
Time taken: 285.904 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0317 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0317 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403181518_2077547685.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686489, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686489
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686489
2014-03-18 15:18:30,644 Stage-3 map = 0%,  reduce = 0%
2014-03-18 15:18:41,240 Stage-3 map = 100%,  reduce = 0%
2014-03-18 15:19:37,876 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686489
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686539, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686539
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686539
2014-03-18 15:20:33,476 Stage-1 map = 0%,  reduce = 0%
2014-03-18 15:20:52,950 Stage-1 map = 67%,  reduce = 0%
2014-03-18 15:20:58,714 Stage-1 map = 100%,  reduce = 0%
2014-03-18 15:21:09,711 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686539
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686573, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686573
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686573
2014-03-18 15:23:51,666 Stage-2 map = 0%,  reduce = 0%
2014-03-18 15:24:06,114 Stage-2 map = 100%,  reduce = 0%
2014-03-18 15:24:45,293 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686573
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17
8 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17
OK
Time taken: 383.133 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17/* > /home/group_dataanalysis/yhd_Report/res-2014-03-17/output-1
14/03/18 15:24:46 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/18 15:24:46 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0317 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403181524_656913785.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686636, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686636
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686636
2014-03-18 15:24:57,111 Stage-1 map = 0%,  reduce = 0%
2014-03-18 15:25:10,297 Stage-1 map = 50%,  reduce = 0%
2014-03-18 15:26:20,679 Stage-1 map = 50%,  reduce = 0%
2014-03-18 15:26:49,394 Stage-1 map = 100%,  reduce = 0%
2014-03-18 15:26:57,851 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686636
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686750, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686750
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686750
2014-03-18 15:27:04,929 Stage-2 map = 0%,  reduce = 0%
2014-03-18 15:27:12,104 Stage-2 map = 50%,  reduce = 0%
2014-03-18 15:27:22,051 Stage-2 map = 100%,  reduce = 0%
2014-03-18 15:27:29,275 Stage-2 map = 100%,  reduce = 33%
2014-03-18 15:27:31,376 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686750
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_686800, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_686800
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_686800
2014-03-18 15:28:25,414 Stage-3 map = 0%,  reduce = 0%
2014-03-18 15:28:39,666 Stage-3 map = 100%,  reduce = 0%
2014-03-18 15:28:48,428 Stage-3 map = 100%,  reduce = 17%
2014-03-18 15:28:51,037 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_686800
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17
OK
Time taken: 276.498 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-17/* > /home/group_dataanalysis/yhd_Report/res-2014-03-17/output-2
14/03/18 15:29:25 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/18 15:29:25 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395127766.3
total time is :  0.18813586235 seconds
size is       :  7711 KB
speed is      :  0.040986337765 MB/s
140317LCJB98 20
140317WCLMSP 3
140317FCKC18 3
140317JCL0WJ 34
140317XCKRFK 20
140317WCLLXY 3
140317CCF1CX 34
140317RCFWGQ 20
140317UCM894 20
140317BCL7EK 3
140317JCL7J7 20
140317CCLT0J 38
140317FCJR9L 20
140317HCE77V 34
140317WCKUAV 20
140317ECK35N 20
140317LCN2A4 20
140317XCK7FN 20
140317BCJG7Q 20
140317QCKY8P 3
140317RCNM4X 20
140317JCFYM1 20
#######################SAVE DATE TO FILE#########################
20140318_152926
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0318 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-18' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-18' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-18' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403191440_237335508.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710377, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710377
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710377
2014-03-19 14:41:45,620 Stage-1 map = 0%,  reduce = 0%
2014-03-19 14:41:49,385 Stage-1 map = 10%,  reduce = 0%
2014-03-19 14:41:50,530 Stage-1 map = 22%,  reduce = 0%
2014-03-19 14:41:52,347 Stage-1 map = 39%,  reduce = 0%
2014-03-19 14:41:54,750 Stage-1 map = 65%,  reduce = 0%
2014-03-19 14:41:55,892 Stage-1 map = 83%,  reduce = 0%
2014-03-19 14:41:57,018 Stage-1 map = 84%,  reduce = 0%
2014-03-19 14:42:01,367 Stage-1 map = 91%,  reduce = 0%
2014-03-19 14:42:02,423 Stage-1 map = 96%,  reduce = 0%
2014-03-19 14:42:04,469 Stage-1 map = 97%,  reduce = 0%
2014-03-19 14:42:06,735 Stage-1 map = 98%,  reduce = 0%
2014-03-19 14:42:12,310 Stage-1 map = 98%,  reduce = 33%
2014-03-19 14:42:39,327 Stage-1 map = 99%,  reduce = 33%
2014-03-19 14:42:55,740 Stage-1 map = 100%,  reduce = 33%
2014-03-19 14:43:00,093 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710377
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 11
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710403, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710403
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710403
2014-03-19 14:43:15,179 Stage-5 map = 0%,  reduce = 0%
2014-03-19 14:43:33,743 Stage-5 map = 1%,  reduce = 0%
2014-03-19 14:43:35,117 Stage-5 map = 2%,  reduce = 0%
2014-03-19 14:43:36,310 Stage-5 map = 5%,  reduce = 0%
2014-03-19 14:43:37,379 Stage-5 map = 12%,  reduce = 0%
2014-03-19 14:43:38,568 Stage-5 map = 22%,  reduce = 0%
2014-03-19 14:43:39,609 Stage-5 map = 24%,  reduce = 0%
2014-03-19 14:43:40,688 Stage-5 map = 27%,  reduce = 0%
2014-03-19 14:43:41,710 Stage-5 map = 30%,  reduce = 0%
2014-03-19 14:43:42,743 Stage-5 map = 33%,  reduce = 0%
2014-03-19 14:43:43,791 Stage-5 map = 36%,  reduce = 0%
2014-03-19 14:43:44,923 Stage-5 map = 38%,  reduce = 0%
2014-03-19 14:43:46,085 Stage-5 map = 43%,  reduce = 0%
2014-03-19 14:43:47,263 Stage-5 map = 46%,  reduce = 0%
2014-03-19 14:43:48,457 Stage-5 map = 50%,  reduce = 0%
2014-03-19 14:43:49,494 Stage-5 map = 54%,  reduce = 0%
2014-03-19 14:43:51,552 Stage-5 map = 56%,  reduce = 0%
2014-03-19 14:43:52,564 Stage-5 map = 57%,  reduce = 0%
2014-03-19 14:43:54,412 Stage-5 map = 59%,  reduce = 0%
2014-03-19 14:43:55,665 Stage-5 map = 60%,  reduce = 0%
2014-03-19 14:43:56,715 Stage-5 map = 61%,  reduce = 0%
2014-03-19 14:43:57,899 Stage-5 map = 63%,  reduce = 0%
2014-03-19 14:43:58,955 Stage-5 map = 65%,  reduce = 0%
2014-03-19 14:43:59,998 Stage-5 map = 66%,  reduce = 0%
2014-03-19 14:44:01,006 Stage-5 map = 67%,  reduce = 0%
2014-03-19 14:44:02,015 Stage-5 map = 68%,  reduce = 0%
2014-03-19 14:44:03,023 Stage-5 map = 69%,  reduce = 0%
2014-03-19 14:44:06,280 Stage-5 map = 70%,  reduce = 0%
2014-03-19 14:44:14,557 Stage-5 map = 81%,  reduce = 0%
2014-03-19 14:44:25,435 Stage-5 map = 93%,  reduce = 0%
2014-03-19 14:44:32,244 Stage-5 map = 99%,  reduce = 0%
2014-03-19 14:44:35,208 Stage-5 map = 100%,  reduce = 0%
2014-03-19 14:44:39,490 Stage-5 map = 100%,  reduce = 10%
2014-03-19 14:44:41,087 Stage-5 map = 100%,  reduce = 30%
2014-03-19 14:44:43,744 Stage-5 map = 100%,  reduce = 33%
2014-03-19 14:45:07,795 Stage-5 map = 100%,  reduce = 39%
2014-03-19 14:45:09,057 Stage-5 map = 100%,  reduce = 50%
2014-03-19 14:45:10,694 Stage-5 map = 100%,  reduce = 80%
2014-03-19 14:45:12,169 Stage-5 map = 100%,  reduce = 93%
2014-03-19 14:45:13,391 Stage-5 map = 100%,  reduce = 97%
2014-03-19 14:45:15,635 Stage-5 map = 100%,  reduce = 98%
2014-03-19 14:46:07,257 Stage-5 map = 100%,  reduce = 100%
2014-03-19 14:47:12,812 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710403
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 489
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710451, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710451
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710451
2014-03-19 14:48:33,351 Stage-7 map = 0%,  reduce = 0%
2014-03-19 14:48:42,485 Stage-7 map = 1%,  reduce = 0%
2014-03-19 14:48:49,660 Stage-7 map = 2%,  reduce = 0%
2014-03-19 14:48:57,745 Stage-7 map = 3%,  reduce = 0%
2014-03-19 14:49:04,791 Stage-7 map = 4%,  reduce = 0%
2014-03-19 14:49:10,852 Stage-7 map = 5%,  reduce = 0%
2014-03-19 14:49:21,682 Stage-7 map = 7%,  reduce = 0%
2014-03-19 14:49:22,734 Stage-7 map = 9%,  reduce = 0%
2014-03-19 14:49:27,824 Stage-7 map = 10%,  reduce = 0%
2014-03-19 14:49:30,860 Stage-7 map = 11%,  reduce = 0%
2014-03-19 14:49:34,912 Stage-7 map = 12%,  reduce = 0%
2014-03-19 14:49:38,964 Stage-7 map = 13%,  reduce = 0%
2014-03-19 14:49:43,006 Stage-7 map = 14%,  reduce = 0%
2014-03-19 14:49:46,056 Stage-7 map = 15%,  reduce = 0%
2014-03-19 14:49:52,141 Stage-7 map = 16%,  reduce = 0%
2014-03-19 14:49:55,215 Stage-7 map = 17%,  reduce = 0%
2014-03-19 14:49:59,344 Stage-7 map = 18%,  reduce = 0%
2014-03-19 14:50:04,404 Stage-7 map = 19%,  reduce = 0%
2014-03-19 14:50:09,467 Stage-7 map = 20%,  reduce = 0%
2014-03-19 14:50:13,524 Stage-7 map = 21%,  reduce = 0%
2014-03-19 14:50:16,560 Stage-7 map = 22%,  reduce = 0%
2014-03-19 14:50:21,601 Stage-7 map = 23%,  reduce = 0%
2014-03-19 14:50:25,652 Stage-7 map = 24%,  reduce = 0%
2014-03-19 14:50:28,700 Stage-7 map = 25%,  reduce = 0%
2014-03-19 14:50:34,176 Stage-7 map = 26%,  reduce = 0%
2014-03-19 14:50:35,232 Stage-7 map = 27%,  reduce = 0%
2014-03-19 14:50:38,307 Stage-7 map = 28%,  reduce = 0%
2014-03-19 14:50:41,348 Stage-7 map = 29%,  reduce = 0%
2014-03-19 14:50:45,455 Stage-7 map = 30%,  reduce = 0%
2014-03-19 14:50:48,595 Stage-7 map = 31%,  reduce = 0%
2014-03-19 14:50:50,613 Stage-7 map = 32%,  reduce = 0%
2014-03-19 14:50:53,823 Stage-7 map = 33%,  reduce = 0%
2014-03-19 14:50:56,871 Stage-7 map = 34%,  reduce = 0%
2014-03-19 14:51:00,108 Stage-7 map = 35%,  reduce = 0%
2014-03-19 14:51:02,283 Stage-7 map = 36%,  reduce = 0%
2014-03-19 14:51:27,658 Stage-7 map = 42%,  reduce = 0%
2014-03-19 14:51:28,825 Stage-7 map = 47%,  reduce = 0%
2014-03-19 14:51:31,875 Stage-7 map = 48%,  reduce = 0%
2014-03-19 14:51:34,923 Stage-7 map = 49%,  reduce = 0%
2014-03-19 14:51:47,691 Stage-7 map = 51%,  reduce = 0%
2014-03-19 14:51:48,750 Stage-7 map = 54%,  reduce = 0%
2014-03-19 14:51:51,894 Stage-7 map = 55%,  reduce = 0%
2014-03-19 14:51:54,483 Stage-7 map = 56%,  reduce = 0%
2014-03-19 14:51:57,594 Stage-7 map = 57%,  reduce = 0%
2014-03-19 14:51:58,967 Stage-7 map = 58%,  reduce = 0%
2014-03-19 14:52:00,091 Stage-7 map = 59%,  reduce = 0%
2014-03-19 14:52:01,193 Stage-7 map = 60%,  reduce = 0%
2014-03-19 14:52:02,355 Stage-7 map = 61%,  reduce = 0%
2014-03-19 14:52:04,565 Stage-7 map = 62%,  reduce = 0%
2014-03-19 14:52:05,957 Stage-7 map = 63%,  reduce = 0%
2014-03-19 14:52:06,992 Stage-7 map = 64%,  reduce = 0%
2014-03-19 14:52:08,013 Stage-7 map = 65%,  reduce = 0%
2014-03-19 14:52:10,034 Stage-7 map = 66%,  reduce = 0%
2014-03-19 14:52:12,314 Stage-7 map = 67%,  reduce = 0%
2014-03-19 14:52:14,360 Stage-7 map = 68%,  reduce = 0%
2014-03-19 14:52:15,370 Stage-7 map = 69%,  reduce = 0%
2014-03-19 14:52:17,400 Stage-7 map = 70%,  reduce = 0%
2014-03-19 14:52:51,481 Stage-7 map = 71%,  reduce = 0%
2014-03-19 14:53:40,409 Stage-7 map = 85%,  reduce = 0%
2014-03-19 14:53:41,950 Stage-7 map = 91%,  reduce = 0%
2014-03-19 14:54:50,052 Stage-7 map = 91%,  reduce = 0%
2014-03-19 14:55:20,843 Stage-7 map = 98%,  reduce = 0%
2014-03-19 14:55:22,000 Stage-7 map = 100%,  reduce = 0%
2014-03-19 14:55:29,191 Stage-7 map = 100%,  reduce = 1%
2014-03-19 14:56:57,389 Stage-7 map = 100%,  reduce = 1%
2014-03-19 14:56:59,138 Stage-7 map = 100%,  reduce = 2%
2014-03-19 14:58:04,214 Stage-7 map = 100%,  reduce = 2%
2014-03-19 14:58:06,981 Stage-7 map = 100%,  reduce = 7%
2014-03-19 14:58:08,079 Stage-7 map = 100%,  reduce = 8%
2014-03-19 14:59:22,318 Stage-7 map = 100%,  reduce = 8%
2014-03-19 14:59:24,844 Stage-7 map = 100%,  reduce = 9%
2014-03-19 14:59:31,195 Stage-7 map = 100%,  reduce = 10%
2014-03-19 14:59:33,244 Stage-7 map = 100%,  reduce = 11%
2014-03-19 14:59:34,343 Stage-7 map = 100%,  reduce = 14%
2014-03-19 14:59:35,434 Stage-7 map = 100%,  reduce = 16%
2014-03-19 14:59:36,503 Stage-7 map = 100%,  reduce = 17%
2014-03-19 14:59:37,756 Stage-7 map = 100%,  reduce = 18%
2014-03-19 14:59:41,875 Stage-7 map = 100%,  reduce = 19%
2014-03-19 14:59:42,912 Stage-7 map = 100%,  reduce = 20%
2014-03-19 14:59:44,980 Stage-7 map = 100%,  reduce = 21%
2014-03-19 14:59:46,008 Stage-7 map = 100%,  reduce = 22%
2014-03-19 14:59:47,024 Stage-7 map = 100%,  reduce = 23%
2014-03-19 14:59:48,079 Stage-7 map = 100%,  reduce = 25%
2014-03-19 14:59:49,094 Stage-7 map = 100%,  reduce = 26%
2014-03-19 14:59:50,144 Stage-7 map = 100%,  reduce = 27%
2014-03-19 14:59:53,296 Stage-7 map = 100%,  reduce = 28%
2014-03-19 14:59:55,329 Stage-7 map = 100%,  reduce = 30%
2014-03-19 14:59:56,373 Stage-7 map = 100%,  reduce = 32%
2014-03-19 14:59:57,433 Stage-7 map = 100%,  reduce = 33%
2014-03-19 15:00:01,544 Stage-7 map = 100%,  reduce = 34%
2014-03-19 15:00:04,948 Stage-7 map = 100%,  reduce = 35%
2014-03-19 15:00:06,084 Stage-7 map = 100%,  reduce = 37%
2014-03-19 15:00:07,111 Stage-7 map = 100%,  reduce = 38%
2014-03-19 15:00:08,132 Stage-7 map = 100%,  reduce = 39%
2014-03-19 15:00:11,225 Stage-7 map = 100%,  reduce = 40%
2014-03-19 15:00:12,245 Stage-7 map = 100%,  reduce = 43%
2014-03-19 15:00:13,267 Stage-7 map = 100%,  reduce = 44%
2014-03-19 15:00:14,285 Stage-7 map = 100%,  reduce = 45%
2014-03-19 15:00:15,308 Stage-7 map = 100%,  reduce = 49%
2014-03-19 15:00:16,325 Stage-7 map = 100%,  reduce = 52%
2014-03-19 15:00:17,445 Stage-7 map = 100%,  reduce = 53%
2014-03-19 15:00:18,472 Stage-7 map = 100%,  reduce = 54%
2014-03-19 15:00:21,598 Stage-7 map = 100%,  reduce = 55%
2014-03-19 15:00:23,752 Stage-7 map = 100%,  reduce = 57%
2014-03-19 15:00:24,834 Stage-7 map = 100%,  reduce = 71%
2014-03-19 15:00:25,869 Stage-7 map = 100%,  reduce = 74%
2014-03-19 15:00:26,887 Stage-7 map = 100%,  reduce = 76%
2014-03-19 15:00:27,949 Stage-7 map = 100%,  reduce = 78%
2014-03-19 15:00:28,975 Stage-7 map = 100%,  reduce = 79%
2014-03-19 15:00:30,032 Stage-7 map = 100%,  reduce = 80%
2014-03-19 15:00:31,072 Stage-7 map = 100%,  reduce = 81%
2014-03-19 15:00:32,184 Stage-7 map = 100%,  reduce = 82%
2014-03-19 15:00:33,232 Stage-7 map = 100%,  reduce = 83%
2014-03-19 15:01:37,588 Stage-7 map = 100%,  reduce = 84%
2014-03-19 15:01:40,478 Stage-7 map = 100%,  reduce = 99%
2014-03-19 15:01:41,812 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710451
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710541, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710541
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710541
2014-03-19 15:01:55,057 Stage-6 map = 0%,  reduce = 0%
2014-03-19 15:02:01,791 Stage-6 map = 36%,  reduce = 0%
2014-03-19 15:02:06,542 Stage-6 map = 45%,  reduce = 0%
2014-03-19 15:02:09,130 Stage-6 map = 64%,  reduce = 0%
2014-03-19 15:02:10,545 Stage-6 map = 82%,  reduce = 0%
2014-03-19 15:03:38,378 Stage-6 map = 100%,  reduce = 0%
2014-03-19 15:05:30,823 Stage-6 map = 100%,  reduce = 0%
2014-03-19 15:05:56,681 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710541
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710576, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710576
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710576
2014-03-19 15:07:02,847 Stage-2 map = 0%,  reduce = 0%
2014-03-19 15:07:21,909 Stage-2 map = 100%,  reduce = 0%
2014-03-19 15:07:35,371 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710576
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710614, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710614
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710614
2014-03-19 15:07:45,599 Stage-8 map = 0%,  reduce = 0%
2014-03-19 15:07:51,527 Stage-8 map = 2%,  reduce = 0%
2014-03-19 15:07:52,735 Stage-8 map = 6%,  reduce = 0%
2014-03-19 15:07:53,893 Stage-8 map = 10%,  reduce = 0%
2014-03-19 15:07:55,152 Stage-8 map = 14%,  reduce = 0%
2014-03-19 15:07:56,625 Stage-8 map = 18%,  reduce = 0%
2014-03-19 15:07:57,721 Stage-8 map = 24%,  reduce = 0%
2014-03-19 15:07:58,738 Stage-8 map = 28%,  reduce = 0%
2014-03-19 15:08:00,545 Stage-8 map = 31%,  reduce = 0%
2014-03-19 15:08:02,676 Stage-8 map = 38%,  reduce = 0%
2014-03-19 15:08:04,121 Stage-8 map = 42%,  reduce = 0%
2014-03-19 15:08:05,831 Stage-8 map = 45%,  reduce = 0%
2014-03-19 15:08:07,330 Stage-8 map = 48%,  reduce = 0%
2014-03-19 15:08:08,564 Stage-8 map = 52%,  reduce = 0%
2014-03-19 15:08:09,847 Stage-8 map = 54%,  reduce = 0%
2014-03-19 15:08:11,199 Stage-8 map = 58%,  reduce = 0%
2014-03-19 15:08:12,548 Stage-8 map = 61%,  reduce = 0%
2014-03-19 15:08:14,068 Stage-8 map = 66%,  reduce = 0%
2014-03-19 15:08:15,640 Stage-8 map = 71%,  reduce = 0%
2014-03-19 15:08:17,571 Stage-8 map = 75%,  reduce = 0%
2014-03-19 15:08:19,545 Stage-8 map = 82%,  reduce = 0%
2014-03-19 15:08:21,365 Stage-8 map = 87%,  reduce = 0%
2014-03-19 15:08:22,951 Stage-8 map = 88%,  reduce = 0%
2014-03-19 15:08:24,221 Stage-8 map = 89%,  reduce = 0%
2014-03-19 15:08:42,111 Stage-8 map = 92%,  reduce = 0%
2014-03-19 15:08:47,479 Stage-8 map = 93%,  reduce = 0%
2014-03-19 15:08:56,089 Stage-8 map = 94%,  reduce = 0%
2014-03-19 15:08:58,319 Stage-8 map = 95%,  reduce = 0%
2014-03-19 15:08:59,470 Stage-8 map = 96%,  reduce = 0%
2014-03-19 15:09:37,321 Stage-8 map = 98%,  reduce = 32%
2014-03-19 15:09:38,641 Stage-8 map = 99%,  reduce = 32%
2014-03-19 15:09:39,778 Stage-8 map = 100%,  reduce = 33%
2014-03-19 15:09:52,732 Stage-8 map = 100%,  reduce = 100%
2014-03-19 15:11:58,369 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710614
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710666, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710666
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710666
2014-03-19 15:12:14,067 Stage-3 map = 0%,  reduce = 0%
2014-03-19 15:12:25,734 Stage-3 map = 100%,  reduce = 0%
2014-03-19 15:12:55,078 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710666
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0318
12 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-19_14-40-03_469_3872921730377657552/-ext-10000
OK
Time taken: 1986.293 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0318 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-18' and dd>='2014-03-17' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-18' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-18' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403191513_1683562380.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710695, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710695
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710695
2014-03-19 15:14:52,583 Stage-1 map = 0%,  reduce = 0%
2014-03-19 15:14:57,453 Stage-1 map = 1%,  reduce = 0%
2014-03-19 15:15:00,793 Stage-1 map = 2%,  reduce = 0%
2014-03-19 15:15:02,824 Stage-1 map = 6%,  reduce = 0%
2014-03-19 15:15:04,171 Stage-1 map = 13%,  reduce = 0%
2014-03-19 15:15:05,310 Stage-1 map = 26%,  reduce = 0%
2014-03-19 15:15:08,379 Stage-1 map = 35%,  reduce = 0%
2014-03-19 15:15:10,817 Stage-1 map = 74%,  reduce = 0%
2014-03-19 15:15:13,058 Stage-1 map = 87%,  reduce = 0%
2014-03-19 15:15:14,559 Stage-1 map = 88%,  reduce = 0%
2014-03-19 15:15:15,722 Stage-1 map = 92%,  reduce = 0%
2014-03-19 15:15:16,776 Stage-1 map = 93%,  reduce = 0%
2014-03-19 15:15:19,820 Stage-1 map = 94%,  reduce = 0%
2014-03-19 15:15:49,084 Stage-1 map = 100%,  reduce = 32%
2014-03-19 15:15:58,948 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710695
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 4
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710793, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710793
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710793
2014-03-19 15:17:11,208 Stage-3 map = 0%,  reduce = 0%
2014-03-19 15:17:12,825 Stage-3 map = 2%,  reduce = 0%
2014-03-19 15:17:15,740 Stage-3 map = 5%,  reduce = 0%
2014-03-19 15:17:17,649 Stage-3 map = 15%,  reduce = 0%
2014-03-19 15:17:28,746 Stage-3 map = 20%,  reduce = 0%
2014-03-19 15:17:32,828 Stage-3 map = 71%,  reduce = 0%
2014-03-19 15:17:34,729 Stage-3 map = 74%,  reduce = 0%
2014-03-19 15:17:37,164 Stage-3 map = 83%,  reduce = 0%
2014-03-19 15:17:39,932 Stage-3 map = 95%,  reduce = 0%
2014-03-19 15:17:42,140 Stage-3 map = 98%,  reduce = 0%
2014-03-19 15:17:51,101 Stage-3 map = 98%,  reduce = 20%
2014-03-19 15:17:54,440 Stage-3 map = 98%,  reduce = 33%
2014-03-19 15:18:17,595 Stage-3 map = 99%,  reduce = 33%
2014-03-19 15:18:21,319 Stage-3 map = 100%,  reduce = 33%
2014-03-19 15:18:30,098 Stage-3 map = 100%,  reduce = 66%
2014-03-19 15:18:32,723 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710793
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_710925, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_710925
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_710925
2014-03-19 15:18:47,904 Stage-4 map = 0%,  reduce = 0%
2014-03-19 15:18:52,563 Stage-4 map = 3%,  reduce = 0%
2014-03-19 15:18:53,849 Stage-4 map = 15%,  reduce = 0%
2014-03-19 15:18:55,035 Stage-4 map = 18%,  reduce = 0%
2014-03-19 15:18:56,117 Stage-4 map = 24%,  reduce = 0%
2014-03-19 15:18:58,175 Stage-4 map = 30%,  reduce = 0%
2014-03-19 15:18:59,811 Stage-4 map = 37%,  reduce = 0%
2014-03-19 15:19:01,479 Stage-4 map = 44%,  reduce = 0%
2014-03-19 15:19:02,891 Stage-4 map = 48%,  reduce = 0%
2014-03-19 15:19:03,961 Stage-4 map = 54%,  reduce = 0%
2014-03-19 15:19:05,342 Stage-4 map = 58%,  reduce = 0%
2014-03-19 15:19:07,264 Stage-4 map = 62%,  reduce = 0%
2014-03-19 15:19:08,874 Stage-4 map = 71%,  reduce = 0%
2014-03-19 15:19:10,235 Stage-4 map = 76%,  reduce = 0%
2014-03-19 15:19:11,480 Stage-4 map = 81%,  reduce = 0%
2014-03-19 15:19:12,722 Stage-4 map = 82%,  reduce = 0%
2014-03-19 15:19:13,931 Stage-4 map = 83%,  reduce = 0%
2014-03-19 15:19:18,333 Stage-4 map = 85%,  reduce = 0%
2014-03-19 15:19:19,898 Stage-4 map = 88%,  reduce = 0%
2014-03-19 15:19:37,760 Stage-4 map = 91%,  reduce = 0%
2014-03-19 15:19:39,553 Stage-4 map = 96%,  reduce = 0%
2014-03-19 15:19:41,346 Stage-4 map = 98%,  reduce = 0%
2014-03-19 15:20:04,566 Stage-4 map = 99%,  reduce = 0%
2014-03-19 15:20:42,948 Stage-4 map = 100%,  reduce = 0%
2014-03-19 15:21:01,319 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_710925
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711016, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711016
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711016
2014-03-19 15:21:15,500 Stage-2 map = 0%,  reduce = 0%
2014-03-19 15:22:27,008 Stage-2 map = 100%,  reduce = 0%
2014-03-19 15:22:43,591 Stage-2 map = 100%,  reduce = 33%
2014-03-19 15:22:47,025 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711016
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0318
OK
Time taken: 576.486 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0318 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0318 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403191522_1100171020.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711048, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711048
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711048
2014-03-19 15:24:33,187 Stage-3 map = 0%,  reduce = 0%
2014-03-19 15:24:48,506 Stage-3 map = 100%,  reduce = 0%
2014-03-19 15:24:57,714 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711048
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711094, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711094
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711094
2014-03-19 15:25:19,237 Stage-1 map = 0%,  reduce = 0%
2014-03-19 15:25:22,868 Stage-1 map = 33%,  reduce = 0%
2014-03-19 15:26:11,240 Stage-1 map = 100%,  reduce = 0%
2014-03-19 15:26:22,298 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711094
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711131, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711131
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711131
2014-03-19 15:26:43,423 Stage-2 map = 0%,  reduce = 0%
2014-03-19 15:26:49,626 Stage-2 map = 100%,  reduce = 0%
2014-03-19 15:27:02,278 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711131
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18
12 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18
OK
Time taken: 254.23 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18/* > /home/group_dataanalysis/yhd_Report/res-2014-03-18/output-1
14/03/19 15:27:06 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/19 15:27:06 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0318 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403191527_423860992.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711177, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711177
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711177
2014-03-19 15:27:18,552 Stage-1 map = 0%,  reduce = 0%
2014-03-19 15:27:22,943 Stage-1 map = 50%,  reduce = 0%
2014-03-19 15:27:25,484 Stage-1 map = 100%,  reduce = 0%
2014-03-19 15:28:25,789 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711177
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711202, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711202
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711202
2014-03-19 15:28:36,531 Stage-2 map = 0%,  reduce = 0%
2014-03-19 15:28:41,314 Stage-2 map = 50%,  reduce = 0%
2014-03-19 15:28:43,846 Stage-2 map = 100%,  reduce = 0%
2014-03-19 15:28:53,545 Stage-2 map = 100%,  reduce = 33%
2014-03-19 15:28:55,626 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711202
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_711227, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_711227
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_711227
2014-03-19 15:29:06,146 Stage-3 map = 0%,  reduce = 0%
2014-03-19 15:29:13,361 Stage-3 map = 100%,  reduce = 0%
2014-03-19 15:29:21,735 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_711227
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18
OK
Time taken: 137.902 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-18/* > /home/group_dataanalysis/yhd_Report/res-2014-03-18/output-2
14/03/19 15:29:28 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/19 15:29:28 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395214168.54
total time is :  0.0798771381378 seconds
size is       :  91 KB
speed is      :  0.00113924962914 MB/s
#######################SAVE DATE TO FILE#########################
20140319_152928
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0319 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-19' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-19' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-19' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403201440_2089795484.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_734603, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_734603
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_734603
2014-03-20 14:41:23,193 Stage-1 map = 0%,  reduce = 0%
2014-03-20 14:41:28,413 Stage-1 map = 34%,  reduce = 0%
2014-03-20 14:41:29,434 Stage-1 map = 44%,  reduce = 0%
2014-03-20 14:41:30,465 Stage-1 map = 49%,  reduce = 0%
2014-03-20 14:41:31,520 Stage-1 map = 51%,  reduce = 0%
2014-03-20 14:41:33,655 Stage-1 map = 58%,  reduce = 0%
2014-03-20 14:41:34,692 Stage-1 map = 75%,  reduce = 0%
2014-03-20 14:41:35,719 Stage-1 map = 86%,  reduce = 0%
2014-03-20 14:43:04,860 Stage-1 map = 96%,  reduce = 0%
2014-03-20 14:43:06,313 Stage-1 map = 100%,  reduce = 0%
2014-03-20 14:43:28,970 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_734603
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_734633, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_734633
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_734633
2014-03-20 14:44:04,250 Stage-5 map = 0%,  reduce = 0%
2014-03-20 14:44:13,945 Stage-5 map = 13%,  reduce = 0%
2014-03-20 14:44:15,928 Stage-5 map = 33%,  reduce = 0%
2014-03-20 14:44:28,397 Stage-5 map = 45%,  reduce = 0%
2014-03-20 14:44:31,848 Stage-5 map = 56%,  reduce = 0%
2014-03-20 14:44:33,643 Stage-5 map = 69%,  reduce = 0%
2014-03-20 14:44:35,287 Stage-5 map = 75%,  reduce = 0%
2014-03-20 14:44:37,220 Stage-5 map = 82%,  reduce = 0%
2014-03-20 14:44:38,466 Stage-5 map = 84%,  reduce = 0%
2014-03-20 14:44:40,143 Stage-5 map = 87%,  reduce = 0%
2014-03-20 14:44:41,578 Stage-5 map = 88%,  reduce = 0%
2014-03-20 14:44:42,666 Stage-5 map = 91%,  reduce = 0%
2014-03-20 14:44:45,114 Stage-5 map = 93%,  reduce = 0%
2014-03-20 14:44:46,468 Stage-5 map = 96%,  reduce = 0%
2014-03-20 14:44:47,879 Stage-5 map = 97%,  reduce = 0%
2014-03-20 14:44:51,481 Stage-5 map = 98%,  reduce = 0%
2014-03-20 14:44:53,330 Stage-5 map = 99%,  reduce = 0%
2014-03-20 14:44:56,246 Stage-5 map = 100%,  reduce = 0%
2014-03-20 14:45:03,130 Stage-5 map = 100%,  reduce = 11%
2014-03-20 14:45:05,884 Stage-5 map = 100%,  reduce = 14%
2014-03-20 14:45:08,764 Stage-5 map = 100%,  reduce = 32%
2014-03-20 14:45:10,351 Stage-5 map = 100%,  reduce = 72%
2014-03-20 14:45:11,868 Stage-5 map = 100%,  reduce = 81%
2014-03-20 14:45:13,394 Stage-5 map = 100%,  reduce = 85%
2014-03-20 14:45:15,016 Stage-5 map = 100%,  reduce = 93%
2014-03-20 14:45:19,292 Stage-5 map = 100%,  reduce = 98%
2014-03-20 14:45:20,623 Stage-5 map = 100%,  reduce = 99%
2014-03-20 14:45:25,878 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_734633
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 486
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_734730, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_734730
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_734730
2014-03-20 14:47:06,149 Stage-7 map = 0%,  reduce = 0%
2014-03-20 14:48:07,975 Stage-7 map = 5%,  reduce = 0%
2014-03-20 14:48:11,399 Stage-7 map = 8%,  reduce = 0%
2014-03-20 14:48:12,915 Stage-7 map = 9%,  reduce = 0%
2014-03-20 14:48:28,873 Stage-7 map = 11%,  reduce = 0%
2014-03-20 14:48:30,253 Stage-7 map = 12%,  reduce = 0%
2014-03-20 14:48:32,441 Stage-7 map = 13%,  reduce = 0%
2014-03-20 14:48:34,859 Stage-7 map = 14%,  reduce = 0%
2014-03-20 14:48:37,955 Stage-7 map = 15%,  reduce = 0%
2014-03-20 14:48:42,291 Stage-7 map = 16%,  reduce = 0%
2014-03-20 14:48:43,469 Stage-7 map = 17%,  reduce = 0%
2014-03-20 14:48:44,517 Stage-7 map = 18%,  reduce = 0%
2014-03-20 14:48:46,777 Stage-7 map = 19%,  reduce = 0%
2014-03-20 14:48:49,881 Stage-7 map = 20%,  reduce = 0%
2014-03-20 14:48:50,926 Stage-7 map = 21%,  reduce = 0%
2014-03-20 14:48:52,939 Stage-7 map = 22%,  reduce = 0%
2014-03-20 14:48:55,093 Stage-7 map = 23%,  reduce = 0%
2014-03-20 14:48:57,242 Stage-7 map = 24%,  reduce = 0%
2014-03-20 14:48:59,293 Stage-7 map = 25%,  reduce = 0%
2014-03-20 14:49:01,322 Stage-7 map = 26%,  reduce = 0%
2014-03-20 14:49:02,342 Stage-7 map = 27%,  reduce = 0%
2014-03-20 14:49:05,385 Stage-7 map = 28%,  reduce = 0%
2014-03-20 14:49:07,415 Stage-7 map = 29%,  reduce = 0%
2014-03-20 14:49:08,422 Stage-7 map = 30%,  reduce = 0%
2014-03-20 14:49:10,442 Stage-7 map = 31%,  reduce = 0%
2014-03-20 14:49:13,472 Stage-7 map = 32%,  reduce = 0%
2014-03-20 14:49:14,489 Stage-7 map = 33%,  reduce = 0%
2014-03-20 14:49:17,057 Stage-7 map = 34%,  reduce = 0%
2014-03-20 14:49:19,186 Stage-7 map = 35%,  reduce = 0%
2014-03-20 14:49:21,364 Stage-7 map = 36%,  reduce = 0%
2014-03-20 14:49:23,651 Stage-7 map = 37%,  reduce = 0%
2014-03-20 14:49:26,258 Stage-7 map = 38%,  reduce = 0%
2014-03-20 14:49:29,298 Stage-7 map = 39%,  reduce = 0%
2014-03-20 14:49:32,395 Stage-7 map = 40%,  reduce = 0%
2014-03-20 14:49:35,788 Stage-7 map = 41%,  reduce = 0%
2014-03-20 14:49:40,162 Stage-7 map = 42%,  reduce = 0%
2014-03-20 14:49:42,208 Stage-7 map = 43%,  reduce = 0%
2014-03-20 14:49:52,593 Stage-7 map = 44%,  reduce = 0%
2014-03-20 14:49:54,398 Stage-7 map = 48%,  reduce = 0%
2014-03-20 14:49:55,551 Stage-7 map = 52%,  reduce = 0%
2014-03-20 14:49:56,622 Stage-7 map = 53%,  reduce = 0%
2014-03-20 14:49:57,875 Stage-7 map = 54%,  reduce = 0%
2014-03-20 14:49:59,046 Stage-7 map = 55%,  reduce = 0%
2014-03-20 14:50:00,055 Stage-7 map = 56%,  reduce = 0%
2014-03-20 14:50:01,066 Stage-7 map = 57%,  reduce = 0%
2014-03-20 14:50:02,079 Stage-7 map = 58%,  reduce = 0%
2014-03-20 14:50:24,876 Stage-7 map = 59%,  reduce = 0%
2014-03-20 14:50:28,427 Stage-7 map = 68%,  reduce = 0%
2014-03-20 14:50:29,521 Stage-7 map = 73%,  reduce = 0%
2014-03-20 14:50:33,842 Stage-7 map = 74%,  reduce = 0%
2014-03-20 14:50:36,725 Stage-7 map = 76%,  reduce = 0%
2014-03-20 14:50:37,799 Stage-7 map = 77%,  reduce = 0%
2014-03-20 14:50:38,846 Stage-7 map = 78%,  reduce = 0%
2014-03-20 14:50:39,857 Stage-7 map = 79%,  reduce = 0%
2014-03-20 14:50:41,879 Stage-7 map = 81%,  reduce = 0%
2014-03-20 14:50:42,891 Stage-7 map = 82%,  reduce = 0%
2014-03-20 14:50:43,904 Stage-7 map = 83%,  reduce = 0%
2014-03-20 14:50:45,274 Stage-7 map = 85%,  reduce = 0%
2014-03-20 14:50:46,309 Stage-7 map = 86%,  reduce = 0%
2014-03-20 14:50:47,337 Stage-7 map = 87%,  reduce = 0%
2014-03-20 14:50:48,380 Stage-7 map = 88%,  reduce = 0%
2014-03-20 14:51:08,500 Stage-7 map = 90%,  reduce = 0%
2014-03-20 14:51:11,874 Stage-7 map = 94%,  reduce = 0%
2014-03-20 14:51:13,348 Stage-7 map = 97%,  reduce = 0%
2014-03-20 14:51:15,723 Stage-7 map = 98%,  reduce = 0%
2014-03-20 14:51:20,173 Stage-7 map = 99%,  reduce = 0%
2014-03-20 14:51:31,884 Stage-7 map = 100%,  reduce = 0%
2014-03-20 14:51:38,679 Stage-7 map = 100%,  reduce = 1%
2014-03-20 14:51:39,904 Stage-7 map = 100%,  reduce = 2%
2014-03-20 14:51:40,949 Stage-7 map = 100%,  reduce = 4%
2014-03-20 14:51:41,991 Stage-7 map = 100%,  reduce = 5%
2014-03-20 14:51:43,024 Stage-7 map = 100%,  reduce = 8%
2014-03-20 14:51:44,287 Stage-7 map = 100%,  reduce = 11%
2014-03-20 14:51:45,308 Stage-7 map = 100%,  reduce = 13%
2014-03-20 14:51:46,374 Stage-7 map = 100%,  reduce = 15%
2014-03-20 14:51:47,557 Stage-7 map = 100%,  reduce = 16%
2014-03-20 14:51:52,596 Stage-7 map = 100%,  reduce = 17%
2014-03-20 14:51:54,913 Stage-7 map = 100%,  reduce = 18%
2014-03-20 14:51:59,182 Stage-7 map = 100%,  reduce = 19%
2014-03-20 14:52:00,816 Stage-7 map = 100%,  reduce = 20%
2014-03-20 14:52:01,992 Stage-7 map = 100%,  reduce = 23%
2014-03-20 14:52:03,132 Stage-7 map = 100%,  reduce = 25%
2014-03-20 14:52:04,191 Stage-7 map = 100%,  reduce = 35%
2014-03-20 14:52:05,246 Stage-7 map = 100%,  reduce = 47%
2014-03-20 14:52:07,143 Stage-7 map = 100%,  reduce = 54%
2014-03-20 14:52:08,432 Stage-7 map = 100%,  reduce = 58%
2014-03-20 14:52:09,627 Stage-7 map = 100%,  reduce = 59%
2014-03-20 14:52:11,806 Stage-7 map = 100%,  reduce = 61%
2014-03-20 14:52:12,846 Stage-7 map = 100%,  reduce = 64%
2014-03-20 14:52:14,381 Stage-7 map = 100%,  reduce = 69%
2014-03-20 14:52:15,562 Stage-7 map = 100%,  reduce = 83%
2014-03-20 14:52:16,687 Stage-7 map = 100%,  reduce = 94%
2014-03-20 14:52:17,777 Stage-7 map = 100%,  reduce = 98%
2014-03-20 14:52:18,815 Stage-7 map = 100%,  reduce = 99%
2014-03-20 14:52:19,833 Stage-7 map = 100%,  reduce = 100%
2014-03-20 14:53:19,946 Stage-7 map = 100%,  reduce = 100%
2014-03-20 14:54:20,148 Stage-7 map = 100%,  reduce = 100%
2014-03-20 14:55:20,906 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_734730
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_734996, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_734996
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_734996
2014-03-20 14:57:06,085 Stage-6 map = 0%,  reduce = 0%
2014-03-20 14:57:22,356 Stage-6 map = 47%,  reduce = 0%
2014-03-20 14:57:23,363 Stage-6 map = 58%,  reduce = 0%
2014-03-20 14:57:24,378 Stage-6 map = 74%,  reduce = 0%
2014-03-20 14:59:06,605 Stage-6 map = 74%,  reduce = 0%
2014-03-20 14:59:09,431 Stage-6 map = 89%,  reduce = 0%
2014-03-20 14:59:10,478 Stage-6 map = 100%,  reduce = 0%
2014-03-20 14:59:37,938 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_734996
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735029, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735029
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735029
2014-03-20 14:59:44,278 Stage-2 map = 0%,  reduce = 0%
2014-03-20 14:59:57,842 Stage-2 map = 100%,  reduce = 0%
2014-03-20 15:00:07,170 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735029
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735036, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735036
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735036
2014-03-20 15:02:37,477 Stage-8 map = 0%,  reduce = 0%
2014-03-20 15:02:40,901 Stage-8 map = 1%,  reduce = 0%
2014-03-20 15:02:43,074 Stage-8 map = 3%,  reduce = 0%
2014-03-20 15:02:44,098 Stage-8 map = 4%,  reduce = 0%
2014-03-20 15:02:45,138 Stage-8 map = 6%,  reduce = 0%
2014-03-20 15:02:46,144 Stage-8 map = 9%,  reduce = 0%
2014-03-20 15:02:47,266 Stage-8 map = 11%,  reduce = 0%
2014-03-20 15:02:48,587 Stage-8 map = 13%,  reduce = 0%
2014-03-20 15:02:49,738 Stage-8 map = 14%,  reduce = 0%
2014-03-20 15:02:50,863 Stage-8 map = 16%,  reduce = 0%
2014-03-20 15:02:52,032 Stage-8 map = 17%,  reduce = 0%
2014-03-20 15:02:53,230 Stage-8 map = 20%,  reduce = 0%
2014-03-20 15:02:55,881 Stage-8 map = 22%,  reduce = 0%
2014-03-20 15:02:57,278 Stage-8 map = 23%,  reduce = 0%
2014-03-20 15:02:58,362 Stage-8 map = 25%,  reduce = 0%
2014-03-20 15:03:00,384 Stage-8 map = 26%,  reduce = 0%
2014-03-20 15:03:01,432 Stage-8 map = 27%,  reduce = 0%
2014-03-20 15:03:02,438 Stage-8 map = 28%,  reduce = 0%
2014-03-20 15:03:03,446 Stage-8 map = 29%,  reduce = 0%
2014-03-20 15:03:05,458 Stage-8 map = 30%,  reduce = 0%
2014-03-20 15:03:06,474 Stage-8 map = 31%,  reduce = 0%
2014-03-20 15:03:07,488 Stage-8 map = 32%,  reduce = 0%
2014-03-20 15:03:11,587 Stage-8 map = 33%,  reduce = 0%
2014-03-20 15:03:13,638 Stage-8 map = 34%,  reduce = 0%
2014-03-20 15:03:16,693 Stage-8 map = 35%,  reduce = 0%
2014-03-20 15:03:18,742 Stage-8 map = 36%,  reduce = 0%
2014-03-20 15:03:22,772 Stage-8 map = 37%,  reduce = 0%
2014-03-20 15:03:33,974 Stage-8 map = 38%,  reduce = 0%
2014-03-20 15:03:42,153 Stage-8 map = 39%,  reduce = 0%
2014-03-20 15:03:43,213 Stage-8 map = 40%,  reduce = 0%
2014-03-20 15:03:45,307 Stage-8 map = 42%,  reduce = 0%
2014-03-20 15:03:46,338 Stage-8 map = 44%,  reduce = 0%
2014-03-20 15:03:47,350 Stage-8 map = 46%,  reduce = 0%
2014-03-20 15:03:48,405 Stage-8 map = 47%,  reduce = 0%
2014-03-20 15:03:49,550 Stage-8 map = 49%,  reduce = 0%
2014-03-20 15:03:50,723 Stage-8 map = 51%,  reduce = 0%
2014-03-20 15:03:51,862 Stage-8 map = 53%,  reduce = 0%
2014-03-20 15:03:52,896 Stage-8 map = 55%,  reduce = 0%
2014-03-20 15:03:53,926 Stage-8 map = 56%,  reduce = 0%
2014-03-20 15:03:54,975 Stage-8 map = 59%,  reduce = 0%
2014-03-20 15:03:56,175 Stage-8 map = 60%,  reduce = 0%
2014-03-20 15:03:57,400 Stage-8 map = 63%,  reduce = 0%
2014-03-20 15:03:58,450 Stage-8 map = 64%,  reduce = 0%
2014-03-20 15:03:59,488 Stage-8 map = 65%,  reduce = 0%
2014-03-20 15:04:00,512 Stage-8 map = 67%,  reduce = 0%
2014-03-20 15:05:11,947 Stage-8 map = 68%,  reduce = 0%
2014-03-20 15:07:32,131 Stage-8 map = 89%,  reduce = 0%
2014-03-20 15:07:35,150 Stage-8 map = 96%,  reduce = 0%
2014-03-20 15:07:36,664 Stage-8 map = 100%,  reduce = 0%
2014-03-20 15:08:47,905 Stage-8 map = 100%,  reduce = 0%
2014-03-20 15:08:51,207 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735036
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735098, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735098
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735098
2014-03-20 15:10:48,722 Stage-3 map = 0%,  reduce = 0%
2014-03-20 15:10:56,327 Stage-3 map = 33%,  reduce = 0%
2014-03-20 15:10:57,509 Stage-3 map = 67%,  reduce = 0%
2014-03-20 15:10:58,587 Stage-3 map = 100%,  reduce = 0%
2014-03-20 15:11:07,152 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735098
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0319
5 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-20_14-40-03_517_559966610891580915/-ext-10000
OK
Time taken: 1864.911 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0319 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-19' and dd>='2014-03-18' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-19' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-19' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403201511_967142448.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735137, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735137
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735137
2014-03-20 15:15:11,660 Stage-1 map = 0%,  reduce = 0%
2014-03-20 15:15:16,076 Stage-1 map = 3%,  reduce = 0%
2014-03-20 15:15:19,258 Stage-1 map = 52%,  reduce = 0%
2014-03-20 15:15:20,750 Stage-1 map = 78%,  reduce = 0%
2014-03-20 15:15:21,884 Stage-1 map = 85%,  reduce = 0%
2014-03-20 15:15:23,608 Stage-1 map = 86%,  reduce = 0%
2014-03-20 15:15:24,937 Stage-1 map = 89%,  reduce = 0%
2014-03-20 15:16:28,460 Stage-1 map = 89%,  reduce = 0%
2014-03-20 15:16:42,459 Stage-1 map = 100%,  reduce = 0%
2014-03-20 15:16:54,542 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735137
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735201, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735201
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735201
2014-03-20 15:17:15,577 Stage-3 map = 0%,  reduce = 0%
2014-03-20 15:17:24,642 Stage-3 map = 6%,  reduce = 0%
2014-03-20 15:17:27,743 Stage-3 map = 12%,  reduce = 0%
2014-03-20 15:17:29,050 Stage-3 map = 16%,  reduce = 0%
2014-03-20 15:17:30,203 Stage-3 map = 18%,  reduce = 0%
2014-03-20 15:17:31,313 Stage-3 map = 21%,  reduce = 0%
2014-03-20 15:17:33,609 Stage-3 map = 23%,  reduce = 0%
2014-03-20 15:17:35,981 Stage-3 map = 28%,  reduce = 0%
2014-03-20 15:17:41,091 Stage-3 map = 31%,  reduce = 0%
2014-03-20 15:17:44,686 Stage-3 map = 46%,  reduce = 0%
2014-03-20 15:17:46,082 Stage-3 map = 51%,  reduce = 0%
2014-03-20 15:17:47,196 Stage-3 map = 52%,  reduce = 0%
2014-03-20 15:17:48,449 Stage-3 map = 57%,  reduce = 0%
2014-03-20 15:17:50,383 Stage-3 map = 62%,  reduce = 0%
2014-03-20 15:17:52,395 Stage-3 map = 69%,  reduce = 0%
2014-03-20 15:17:55,264 Stage-3 map = 78%,  reduce = 0%
2014-03-20 15:17:57,886 Stage-3 map = 86%,  reduce = 0%
2014-03-20 15:17:59,401 Stage-3 map = 91%,  reduce = 0%
2014-03-20 15:18:00,821 Stage-3 map = 94%,  reduce = 0%
2014-03-20 15:18:03,754 Stage-3 map = 96%,  reduce = 0%
2014-03-20 15:18:12,016 Stage-3 map = 96%,  reduce = 32%
2014-03-20 15:18:18,186 Stage-3 map = 97%,  reduce = 32%
2014-03-20 15:18:21,144 Stage-3 map = 98%,  reduce = 32%
2014-03-20 15:18:23,666 Stage-3 map = 99%,  reduce = 32%
2014-03-20 15:18:26,071 Stage-3 map = 100%,  reduce = 33%
2014-03-20 15:18:31,671 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735201
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735282, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735282
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735282
2014-03-20 15:18:39,905 Stage-4 map = 0%,  reduce = 0%
2014-03-20 15:18:43,773 Stage-4 map = 3%,  reduce = 0%
2014-03-20 15:18:44,947 Stage-4 map = 15%,  reduce = 0%
2014-03-20 15:18:45,963 Stage-4 map = 18%,  reduce = 0%
2014-03-20 15:18:46,997 Stage-4 map = 23%,  reduce = 0%
2014-03-20 15:18:48,159 Stage-4 map = 30%,  reduce = 0%
2014-03-20 15:18:49,191 Stage-4 map = 32%,  reduce = 0%
2014-03-20 15:18:50,204 Stage-4 map = 34%,  reduce = 0%
2014-03-20 15:18:55,292 Stage-4 map = 36%,  reduce = 0%
2014-03-20 15:18:57,811 Stage-4 map = 40%,  reduce = 0%
2014-03-20 15:18:59,068 Stage-4 map = 45%,  reduce = 0%
2014-03-20 15:19:00,292 Stage-4 map = 50%,  reduce = 0%
2014-03-20 15:19:01,535 Stage-4 map = 59%,  reduce = 0%
2014-03-20 15:19:02,645 Stage-4 map = 68%,  reduce = 0%
2014-03-20 15:19:03,734 Stage-4 map = 73%,  reduce = 0%
2014-03-20 15:19:04,908 Stage-4 map = 80%,  reduce = 0%
2014-03-20 15:19:06,320 Stage-4 map = 87%,  reduce = 0%
2014-03-20 15:19:07,544 Stage-4 map = 90%,  reduce = 0%
2014-03-20 15:19:08,815 Stage-4 map = 92%,  reduce = 0%
2014-03-20 15:19:26,092 Stage-4 map = 94%,  reduce = 0%
2014-03-20 15:19:29,027 Stage-4 map = 97%,  reduce = 0%
2014-03-20 15:19:30,319 Stage-4 map = 99%,  reduce = 0%
2014-03-20 15:20:40,836 Stage-4 map = 99%,  reduce = 0%
2014-03-20 15:20:42,695 Stage-4 map = 99%,  reduce = 33%
2014-03-20 15:20:48,757 Stage-4 map = 100%,  reduce = 33%
2014-03-20 15:22:15,523 Stage-4 map = 100%,  reduce = 33%
2014-03-20 15:22:18,970 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735282
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735346, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735346
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735346
2014-03-20 15:22:34,998 Stage-2 map = 0%,  reduce = 0%
2014-03-20 15:22:39,067 Stage-2 map = 50%,  reduce = 0%
2014-03-20 15:23:18,636 Stage-2 map = 100%,  reduce = 0%
2014-03-20 15:24:09,687 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735346
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0319
OK
Time taken: 789.431 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0319 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0319 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403201524_1795260137.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735378, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735378
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735378
2014-03-20 15:25:52,848 Stage-1 map = 0%,  reduce = 0%
2014-03-20 15:25:58,774 Stage-1 map = 100%,  reduce = 0%
2014-03-20 15:26:07,189 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735378
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735414, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735414
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735414
2014-03-20 15:26:19,797 Stage-2 map = 0%,  reduce = 0%
2014-03-20 15:26:27,984 Stage-2 map = 100%,  reduce = 0%
2014-03-20 15:26:41,587 Stage-2 map = 100%,  reduce = 100%
2014-03-20 15:27:41,961 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735414
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735443, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735443
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735443
2014-03-20 15:27:48,569 Stage-3 map = 0%,  reduce = 0%
2014-03-20 15:27:52,210 Stage-3 map = 100%,  reduce = 0%
2014-03-20 15:29:02,247 Stage-3 map = 100%,  reduce = 0%
2014-03-20 15:29:05,620 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735443
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19
5 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19
OK
Time taken: 286.153 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19/* > /home/group_dataanalysis/yhd_Report/res-2014-03-19/output-1
14/03/20 15:29:08 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/20 15:29:08 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0319 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403201529_102948339.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735458, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735458
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735458
2014-03-20 15:32:12,151 Stage-1 map = 0%,  reduce = 0%
2014-03-20 15:32:50,573 Stage-1 map = 100%,  reduce = 0%
2014-03-20 15:33:41,827 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735458
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735492, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735492
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735492
2014-03-20 15:33:51,652 Stage-2 map = 0%,  reduce = 0%
2014-03-20 15:33:55,978 Stage-2 map = 100%,  reduce = 0%
2014-03-20 15:34:04,817 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735492
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_735518, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_735518
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_735518
2014-03-20 15:34:15,450 Stage-3 map = 0%,  reduce = 0%
2014-03-20 15:34:20,307 Stage-3 map = 100%,  reduce = 0%
2014-03-20 15:37:24,899 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_735518
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19
OK
Time taken: 507.655 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-19/* > /home/group_dataanalysis/yhd_Report/res-2014-03-19/output-2
14/03/20 15:37:39 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/20 15:37:39 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395301059.99
total time is :  0.0550601482391 seconds
size is       :  6794 KB
speed is      :  0.123392330338 MB/s
140319FEVFFC 20
140319BEPPMS 20
140319TEPHCM 20
140319CERQ48 20
140319FEVHBB 20
140319FEVG3G 20
140319MES0T7 34
140319EEU1MF 20
140319REVGSK 38
140319VEQ5C9 20
140319VEQAN1 20
140319VEQCYC 3
140319RESXA2 20
140319XERQA0 34
140319FEVBRC 34
140319TEUS90 3
140319NF0GM4 38
140319KEWGWG 38
#######################SAVE DATE TO FILE#########################
#######################MAIL FILE#########################
<logging.Logger instance at 0x2abb5814ffc8>
{'loglevel': 'debug', 'debug': False, 'logfile': None}
['res-2014-03-19', '0319', '2014-03-19']
res-2014-03-19/yhd_dsp_report_0319.xls
mail_exchange -s '[2014-03-19]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-19/yhd_dsp_report_0319.xls' -u 'wentao_wang' -p '2238681Xwww' 
Main       2014-03-20 15:37:42,084 INFO  executing: mail_exchange -s '[2014-03-19]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-19/yhd_dsp_report_0319.xls' -u 'wentao_wang' -p '2238681Xwww' 
20140320_153744
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0320 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-20' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-20' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-20' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403211440_1664790383.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_759964, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_759964
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_759964
2014-03-21 14:41:48,225 Stage-1 map = 0%,  reduce = 0%
2014-03-21 14:41:50,805 Stage-1 map = 2%,  reduce = 0%
2014-03-21 14:43:03,202 Stage-1 map = 7%,  reduce = 0%
2014-03-21 14:44:12,179 Stage-1 map = 73%,  reduce = 0%
2014-03-21 14:44:13,526 Stage-1 map = 98%,  reduce = 0%
2014-03-21 14:44:14,594 Stage-1 map = 100%,  reduce = 0%
2014-03-21 14:44:26,226 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_759964
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 21
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760000, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760000
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760000
2014-03-21 14:44:36,996 Stage-5 map = 0%,  reduce = 0%
2014-03-21 14:45:47,683 Stage-5 map = 0%,  reduce = 0%
2014-03-21 14:45:50,850 Stage-5 map = 53%,  reduce = 0%
2014-03-21 14:45:52,387 Stage-5 map = 74%,  reduce = 0%
2014-03-21 14:45:53,437 Stage-5 map = 75%,  reduce = 0%
2014-03-21 14:45:56,680 Stage-5 map = 76%,  reduce = 0%
2014-03-21 14:45:57,692 Stage-5 map = 77%,  reduce = 0%
2014-03-21 14:46:06,238 Stage-5 map = 78%,  reduce = 0%
2014-03-21 14:46:09,304 Stage-5 map = 79%,  reduce = 0%
2014-03-21 14:46:10,313 Stage-5 map = 80%,  reduce = 0%
2014-03-21 14:46:11,321 Stage-5 map = 81%,  reduce = 0%
2014-03-21 14:46:13,893 Stage-5 map = 84%,  reduce = 0%
2014-03-21 14:46:16,977 Stage-5 map = 86%,  reduce = 0%
2014-03-21 14:46:17,992 Stage-5 map = 87%,  reduce = 0%
2014-03-21 14:46:19,005 Stage-5 map = 89%,  reduce = 0%
2014-03-21 14:46:20,020 Stage-5 map = 90%,  reduce = 0%
2014-03-21 14:46:22,130 Stage-5 map = 91%,  reduce = 0%
2014-03-21 14:46:23,509 Stage-5 map = 92%,  reduce = 0%
2014-03-21 14:46:24,566 Stage-5 map = 93%,  reduce = 0%
2014-03-21 14:46:25,746 Stage-5 map = 94%,  reduce = 0%
2014-03-21 14:46:27,889 Stage-5 map = 95%,  reduce = 0%
2014-03-21 14:46:31,112 Stage-5 map = 96%,  reduce = 0%
2014-03-21 14:46:58,109 Stage-5 map = 97%,  reduce = 0%
2014-03-21 14:46:59,363 Stage-5 map = 100%,  reduce = 0%
2014-03-21 14:47:05,220 Stage-5 map = 100%,  reduce = 6%
2014-03-21 14:47:06,294 Stage-5 map = 100%,  reduce = 17%
2014-03-21 14:47:07,313 Stage-5 map = 100%,  reduce = 22%
2014-03-21 14:47:09,585 Stage-5 map = 100%,  reduce = 56%
2014-03-21 14:47:10,593 Stage-5 map = 100%,  reduce = 68%
2014-03-21 14:47:11,609 Stage-5 map = 100%,  reduce = 79%
2014-03-21 14:47:12,623 Stage-5 map = 100%,  reduce = 88%
2014-03-21 14:47:13,631 Stage-5 map = 100%,  reduce = 94%
2014-03-21 14:47:14,656 Stage-5 map = 100%,  reduce = 96%
2014-03-21 14:47:15,665 Stage-5 map = 100%,  reduce = 97%
2014-03-21 14:47:16,684 Stage-5 map = 100%,  reduce = 98%
2014-03-21 14:47:21,825 Stage-5 map = 100%,  reduce = 99%
2014-03-21 14:47:28,647 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760000
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 395
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760042, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760042
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760042
2014-03-21 14:49:34,566 Stage-7 map = 0%,  reduce = 0%
2014-03-21 14:49:55,186 Stage-7 map = 11%,  reduce = 0%
2014-03-21 14:49:56,198 Stage-7 map = 12%,  reduce = 0%
2014-03-21 14:49:57,214 Stage-7 map = 13%,  reduce = 0%
2014-03-21 14:49:58,248 Stage-7 map = 14%,  reduce = 0%
2014-03-21 14:49:59,276 Stage-7 map = 15%,  reduce = 0%
2014-03-21 14:50:00,288 Stage-7 map = 16%,  reduce = 0%
2014-03-21 14:50:02,309 Stage-7 map = 17%,  reduce = 0%
2014-03-21 14:50:03,324 Stage-7 map = 19%,  reduce = 0%
2014-03-21 14:50:04,335 Stage-7 map = 20%,  reduce = 0%
2014-03-21 14:50:05,391 Stage-7 map = 21%,  reduce = 0%
2014-03-21 14:50:31,351 Stage-7 map = 22%,  reduce = 0%
2014-03-21 14:50:33,584 Stage-7 map = 37%,  reduce = 0%
2014-03-21 14:50:34,681 Stage-7 map = 58%,  reduce = 0%
2014-03-21 14:50:38,429 Stage-7 map = 59%,  reduce = 0%
2014-03-21 14:50:40,247 Stage-7 map = 63%,  reduce = 0%
2014-03-21 14:50:41,268 Stage-7 map = 65%,  reduce = 0%
2014-03-21 14:50:42,294 Stage-7 map = 66%,  reduce = 0%
2014-03-21 14:50:43,319 Stage-7 map = 67%,  reduce = 0%
2014-03-21 14:50:45,341 Stage-7 map = 68%,  reduce = 0%
2014-03-21 14:50:46,870 Stage-7 map = 69%,  reduce = 0%
2014-03-21 14:50:47,921 Stage-7 map = 70%,  reduce = 0%
2014-03-21 14:50:48,943 Stage-7 map = 71%,  reduce = 0%
2014-03-21 14:50:51,299 Stage-7 map = 72%,  reduce = 0%
2014-03-21 14:50:52,542 Stage-7 map = 73%,  reduce = 0%
2014-03-21 14:50:53,656 Stage-7 map = 74%,  reduce = 0%
2014-03-21 14:50:54,896 Stage-7 map = 75%,  reduce = 0%
2014-03-21 14:50:57,099 Stage-7 map = 76%,  reduce = 0%
2014-03-21 14:50:59,243 Stage-7 map = 77%,  reduce = 0%
2014-03-21 14:51:00,264 Stage-7 map = 78%,  reduce = 0%
2014-03-21 14:51:02,774 Stage-7 map = 79%,  reduce = 0%
2014-03-21 14:51:05,375 Stage-7 map = 80%,  reduce = 0%
2014-03-21 14:51:07,843 Stage-7 map = 81%,  reduce = 0%
2014-03-21 14:51:10,101 Stage-7 map = 82%,  reduce = 0%
2014-03-21 14:51:11,197 Stage-7 map = 83%,  reduce = 0%
2014-03-21 14:51:14,753 Stage-7 map = 84%,  reduce = 0%
2014-03-21 14:51:16,922 Stage-7 map = 85%,  reduce = 0%
2014-03-21 14:51:17,955 Stage-7 map = 86%,  reduce = 0%
2014-03-21 14:51:20,005 Stage-7 map = 87%,  reduce = 0%
2014-03-21 14:51:21,018 Stage-7 map = 88%,  reduce = 0%
2014-03-21 14:51:22,059 Stage-7 map = 89%,  reduce = 0%
2014-03-21 14:51:23,097 Stage-7 map = 90%,  reduce = 0%
2014-03-21 14:51:25,174 Stage-7 map = 91%,  reduce = 0%
2014-03-21 14:51:26,188 Stage-7 map = 92%,  reduce = 0%
2014-03-21 14:51:27,202 Stage-7 map = 93%,  reduce = 0%
2014-03-21 14:51:29,498 Stage-7 map = 94%,  reduce = 0%
2014-03-21 14:51:30,635 Stage-7 map = 95%,  reduce = 0%
2014-03-21 14:51:31,690 Stage-7 map = 96%,  reduce = 0%
2014-03-21 14:51:34,729 Stage-7 map = 97%,  reduce = 0%
2014-03-21 14:51:41,953 Stage-7 map = 98%,  reduce = 0%
2014-03-21 14:52:02,835 Stage-7 map = 98%,  reduce = 3%
2014-03-21 14:52:03,889 Stage-7 map = 98%,  reduce = 5%
2014-03-21 14:52:05,001 Stage-7 map = 99%,  reduce = 5%
2014-03-21 14:52:06,094 Stage-7 map = 99%,  reduce = 15%
2014-03-21 14:52:07,108 Stage-7 map = 99%,  reduce = 16%
2014-03-21 14:52:08,127 Stage-7 map = 99%,  reduce = 19%
2014-03-21 14:52:09,140 Stage-7 map = 99%,  reduce = 28%
2014-03-21 14:52:59,126 Stage-7 map = 99%,  reduce = 29%
2014-03-21 14:53:01,619 Stage-7 map = 100%,  reduce = 32%
2014-03-21 14:53:08,827 Stage-7 map = 100%,  reduce = 33%
2014-03-21 14:53:50,532 Stage-7 map = 100%,  reduce = 35%
2014-03-21 14:53:51,557 Stage-7 map = 100%,  reduce = 41%
2014-03-21 14:53:52,588 Stage-7 map = 100%,  reduce = 44%
2014-03-21 14:53:54,531 Stage-7 map = 100%,  reduce = 57%
2014-03-21 14:53:55,691 Stage-7 map = 100%,  reduce = 90%
2014-03-21 14:53:56,720 Stage-7 map = 100%,  reduce = 98%
2014-03-21 14:53:57,774 Stage-7 map = 100%,  reduce = 99%
2014-03-21 14:54:01,740 Stage-7 map = 100%,  reduce = 100%
2014-03-21 14:55:01,764 Stage-7 map = 100%,  reduce = 100%
2014-03-21 14:56:02,127 Stage-7 map = 100%,  reduce = 100%
2014-03-21 14:57:39,299 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760042
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760153, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760153
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760153
2014-03-21 14:58:37,752 Stage-8 map = 0%,  reduce = 0%
2014-03-21 14:58:39,878 Stage-8 map = 3%,  reduce = 0%
2014-03-21 14:58:40,944 Stage-8 map = 18%,  reduce = 0%
2014-03-21 14:58:42,110 Stage-8 map = 78%,  reduce = 0%
2014-03-21 14:58:43,159 Stage-8 map = 87%,  reduce = 0%
2014-03-21 14:58:46,188 Stage-8 map = 94%,  reduce = 0%
2014-03-21 14:58:47,233 Stage-8 map = 98%,  reduce = 0%
2014-03-21 14:58:51,332 Stage-8 map = 100%,  reduce = 0%
2014-03-21 14:58:56,444 Stage-8 map = 100%,  reduce = 33%
2014-03-21 14:58:57,469 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760153
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760170, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760170
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760170
2014-03-21 14:59:40,692 Stage-6 map = 0%,  reduce = 0%
2014-03-21 14:59:45,815 Stage-6 map = 10%,  reduce = 0%
2014-03-21 14:59:47,028 Stage-6 map = 14%,  reduce = 0%
2014-03-21 14:59:51,179 Stage-6 map = 24%,  reduce = 0%
2014-03-21 14:59:52,997 Stage-6 map = 33%,  reduce = 0%
2014-03-21 14:59:54,261 Stage-6 map = 67%,  reduce = 0%
2014-03-21 14:59:55,469 Stage-6 map = 81%,  reduce = 0%
2014-03-21 15:00:08,932 Stage-6 map = 90%,  reduce = 0%
2014-03-21 15:00:10,007 Stage-6 map = 95%,  reduce = 0%
2014-03-21 15:00:11,099 Stage-6 map = 100%,  reduce = 0%
2014-03-21 15:00:19,426 Stage-6 map = 100%,  reduce = 32%
2014-03-21 15:00:20,447 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760170
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760196, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760196
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760196
2014-03-21 15:01:14,651 Stage-2 map = 0%,  reduce = 0%
2014-03-21 15:01:23,027 Stage-2 map = 100%,  reduce = 0%
2014-03-21 15:01:31,372 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760196
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760228, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760228
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760228
2014-03-21 15:02:22,830 Stage-3 map = 0%,  reduce = 0%
2014-03-21 15:02:25,163 Stage-3 map = 33%,  reduce = 0%
2014-03-21 15:02:26,420 Stage-3 map = 67%,  reduce = 0%
2014-03-21 15:02:27,454 Stage-3 map = 100%,  reduce = 0%
2014-03-21 15:03:43,894 Stage-3 map = 100%,  reduce = 0%
2014-03-21 15:03:46,762 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760228
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0320
12 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-21_14-40-03_861_4667956815131200829/-ext-10000
OK
Time taken: 1424.294 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0320 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-20' and dd>='2014-03-19' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-20' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-20' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403211503_541594540.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760256, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760256
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760256
2014-03-21 15:06:39,298 Stage-1 map = 0%,  reduce = 0%
2014-03-21 15:08:59,307 Stage-1 map = 0%,  reduce = 0%
2014-03-21 15:10:07,224 Stage-1 map = 34%,  reduce = 0%
2014-03-21 15:10:10,054 Stage-1 map = 76%,  reduce = 0%
2014-03-21 15:10:11,394 Stage-1 map = 94%,  reduce = 0%
2014-03-21 15:11:55,501 Stage-1 map = 94%,  reduce = 0%
2014-03-21 15:11:58,772 Stage-1 map = 100%,  reduce = 0%
2014-03-21 15:12:04,982 Stage-1 map = 100%,  reduce = 33%
2014-03-21 15:12:06,026 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760256
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760300, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760300
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760300
2014-03-21 15:15:46,778 Stage-3 map = 0%,  reduce = 0%
2014-03-21 15:16:54,363 Stage-3 map = 0%,  reduce = 0%
2014-03-21 15:18:06,695 Stage-3 map = 0%,  reduce = 0%
2014-03-21 15:18:36,413 Stage-3 map = 45%,  reduce = 0%
2014-03-21 15:18:38,279 Stage-3 map = 78%,  reduce = 0%
2014-03-21 15:18:39,348 Stage-3 map = 81%,  reduce = 0%
2014-03-21 15:18:40,711 Stage-3 map = 85%,  reduce = 0%
2014-03-21 15:18:41,898 Stage-3 map = 91%,  reduce = 0%
2014-03-21 15:18:43,069 Stage-3 map = 94%,  reduce = 0%
2014-03-21 15:18:48,584 Stage-3 map = 96%,  reduce = 0%
2014-03-21 15:18:51,616 Stage-3 map = 97%,  reduce = 0%
2014-03-21 15:18:52,772 Stage-3 map = 99%,  reduce = 0%
2014-03-21 15:18:54,991 Stage-3 map = 99%,  reduce = 31%
2014-03-21 15:18:58,055 Stage-3 map = 100%,  reduce = 31%
2014-03-21 15:18:59,063 Stage-3 map = 100%,  reduce = 33%
2014-03-21 15:19:07,044 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760300
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760372, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760372
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760372
2014-03-21 15:21:04,243 Stage-4 map = 0%,  reduce = 0%
2014-03-21 15:21:07,767 Stage-4 map = 1%,  reduce = 0%
2014-03-21 15:21:08,797 Stage-4 map = 8%,  reduce = 0%
2014-03-21 15:21:13,848 Stage-4 map = 22%,  reduce = 0%
2014-03-21 15:21:15,682 Stage-4 map = 33%,  reduce = 0%
2014-03-21 15:21:17,073 Stage-4 map = 37%,  reduce = 0%
2014-03-21 15:21:18,268 Stage-4 map = 41%,  reduce = 0%
2014-03-21 15:21:19,379 Stage-4 map = 51%,  reduce = 0%
2014-03-21 15:21:20,522 Stage-4 map = 55%,  reduce = 0%
2014-03-21 15:21:21,702 Stage-4 map = 60%,  reduce = 0%
2014-03-21 15:21:22,757 Stage-4 map = 65%,  reduce = 0%
2014-03-21 15:21:23,781 Stage-4 map = 71%,  reduce = 0%
2014-03-21 15:21:24,881 Stage-4 map = 74%,  reduce = 0%
2014-03-21 15:21:25,979 Stage-4 map = 78%,  reduce = 0%
2014-03-21 15:21:27,013 Stage-4 map = 80%,  reduce = 0%
2014-03-21 15:21:28,074 Stage-4 map = 82%,  reduce = 0%
2014-03-21 15:21:31,130 Stage-4 map = 84%,  reduce = 0%
2014-03-21 15:22:48,663 Stage-4 map = 85%,  reduce = 0%
2014-03-21 15:25:03,971 Stage-4 map = 96%,  reduce = 0%
2014-03-21 15:25:10,965 Stage-4 map = 100%,  reduce = 0%
2014-03-21 15:25:38,019 Stage-4 map = 100%,  reduce = 33%
2014-03-21 15:25:41,601 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760372
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760457, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760457
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760457
2014-03-21 15:26:28,521 Stage-2 map = 0%,  reduce = 0%
2014-03-21 15:26:34,898 Stage-2 map = 50%,  reduce = 0%
2014-03-21 15:26:37,622 Stage-2 map = 100%,  reduce = 0%
2014-03-21 15:26:44,870 Stage-2 map = 100%,  reduce = 33%
2014-03-21 15:26:47,734 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760457
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0320
OK
Time taken: 1379.985 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0320 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0320 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403211526_34579910.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760504, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760504
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760504
2014-03-21 15:27:01,076 Stage-3 map = 0%,  reduce = 0%
2014-03-21 15:27:04,870 Stage-3 map = 100%,  reduce = 0%
2014-03-21 15:27:12,164 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760504
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760535, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760535
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760535
2014-03-21 15:27:23,367 Stage-1 map = 0%,  reduce = 0%
2014-03-21 15:27:28,214 Stage-1 map = 33%,  reduce = 0%
2014-03-21 15:27:30,716 Stage-1 map = 100%,  reduce = 0%
2014-03-21 15:27:38,329 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760535
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760583, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760583
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760583
2014-03-21 15:28:22,802 Stage-2 map = 0%,  reduce = 0%
2014-03-21 15:28:33,044 Stage-2 map = 100%,  reduce = 0%
2014-03-21 15:28:42,334 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760583
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20
12 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20
OK
Time taken: 111.662 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20/* > /home/group_dataanalysis/yhd_Report/res-2014-03-20/output-1
14/03/21 15:28:44 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/21 15:28:44 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0320 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403211528_576580288.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760624, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760624
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760624
2014-03-21 15:28:55,737 Stage-1 map = 0%,  reduce = 0%
2014-03-21 15:29:00,936 Stage-1 map = 50%,  reduce = 0%
2014-03-21 15:29:02,092 Stage-1 map = 100%,  reduce = 0%
2014-03-21 15:29:10,973 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760624
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760663, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760663
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760663
2014-03-21 15:29:17,882 Stage-2 map = 0%,  reduce = 0%
2014-03-21 15:29:27,332 Stage-2 map = 100%,  reduce = 0%
2014-03-21 15:29:37,535 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760663
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_760713, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_760713
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_760713
2014-03-21 15:29:53,143 Stage-3 map = 0%,  reduce = 0%
2014-03-21 15:29:57,942 Stage-3 map = 100%,  reduce = 0%
2014-03-21 15:30:13,517 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_760713
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20
OK
Time taken: 91.231 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-20/* > /home/group_dataanalysis/yhd_Report/res-2014-03-20/output-2
14/03/21 15:30:18 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/21 15:30:18 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395387019.12
total time is :  0.0672559738159 seconds
size is       :  10328 KB
speed is      :  0.153562567219 MB/s
140320MF5RUF 20
140320BF31GG 20
140320EF3BGK 4
140320MF40KU 20
140320CF9EHL 20
140320XF804B 20
140320CF41JA 20
140320XF520G 3
140320XF7U4H 34
140320CF3L3M 20
140320BF85SN 20
140320WFEGBU 38
140320KF39AB 24
140320AF27J6 24
140320GFE4TV 38
140320HFB536 20
140320MF6353 20
140320GFCUP1 3
140320XF4YLA 20
140320XF5JRL 20
140320GF3KA8 20
140320JF6L2W 3
140320RF7LLM 3
140320AF543J 20
140320SF3A4S 24
140320XF6QHX 34
140320XF6LUX 34
140320FF7359 20
140320GF9QW9 20
#######################SAVE DATE TO FILE#########################
20140321_153019
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0321 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-21' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-21' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-21' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403221440_1747472604.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784243, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784243
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784243
2014-03-22 14:41:39,985 Stage-1 map = 0%,  reduce = 0%
2014-03-22 14:42:46,319 Stage-1 map = 0%,  reduce = 0%
2014-03-22 14:42:47,623 Stage-1 map = 78%,  reduce = 0%
2014-03-22 14:42:56,570 Stage-1 map = 79%,  reduce = 0%
2014-03-22 14:42:57,743 Stage-1 map = 82%,  reduce = 0%
2014-03-22 14:42:58,950 Stage-1 map = 96%,  reduce = 0%
2014-03-22 14:43:00,758 Stage-1 map = 99%,  reduce = 0%
2014-03-22 14:43:03,011 Stage-1 map = 100%,  reduce = 0%
2014-03-22 14:44:12,401 Stage-1 map = 100%,  reduce = 24%
2014-03-22 14:44:15,280 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784243
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 18
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784257, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784257
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784257
2014-03-22 14:44:33,140 Stage-5 map = 0%,  reduce = 0%
2014-03-22 14:44:37,676 Stage-5 map = 3%,  reduce = 0%
2014-03-22 14:44:38,689 Stage-5 map = 9%,  reduce = 0%
2014-03-22 14:44:39,820 Stage-5 map = 17%,  reduce = 0%
2014-03-22 14:44:40,932 Stage-5 map = 22%,  reduce = 0%
2014-03-22 14:44:42,027 Stage-5 map = 39%,  reduce = 0%
2014-03-22 14:44:43,064 Stage-5 map = 50%,  reduce = 0%
2014-03-22 14:44:44,080 Stage-5 map = 59%,  reduce = 0%
2014-03-22 14:44:45,088 Stage-5 map = 67%,  reduce = 0%
2014-03-22 14:44:46,101 Stage-5 map = 74%,  reduce = 0%
2014-03-22 14:44:47,141 Stage-5 map = 78%,  reduce = 0%
2014-03-22 14:44:48,412 Stage-5 map = 80%,  reduce = 0%
2014-03-22 14:44:49,542 Stage-5 map = 84%,  reduce = 0%
2014-03-22 14:44:50,618 Stage-5 map = 86%,  reduce = 0%
2014-03-22 14:44:51,707 Stage-5 map = 88%,  reduce = 0%
2014-03-22 14:44:52,737 Stage-5 map = 90%,  reduce = 0%
2014-03-22 14:44:53,778 Stage-5 map = 91%,  reduce = 0%
2014-03-22 14:44:55,209 Stage-5 map = 92%,  reduce = 0%
2014-03-22 14:44:56,401 Stage-5 map = 93%,  reduce = 0%
2014-03-22 14:44:57,425 Stage-5 map = 94%,  reduce = 0%
2014-03-22 14:45:00,455 Stage-5 map = 95%,  reduce = 0%
2014-03-22 14:45:02,507 Stage-5 map = 96%,  reduce = 0%
2014-03-22 14:45:04,539 Stage-5 map = 97%,  reduce = 0%
2014-03-22 14:45:10,777 Stage-5 map = 98%,  reduce = 0%
2014-03-22 14:45:17,011 Stage-5 map = 98%,  reduce = 27%
2014-03-22 14:45:18,033 Stage-5 map = 98%,  reduce = 30%
2014-03-22 14:45:19,076 Stage-5 map = 98%,  reduce = 32%
2014-03-22 14:47:00,759 Stage-5 map = 98%,  reduce = 32%
2014-03-22 14:47:02,898 Stage-5 map = 100%,  reduce = 32%
2014-03-22 14:47:11,232 Stage-5 map = 100%,  reduce = 85%
2014-03-22 14:47:12,268 Stage-5 map = 100%,  reduce = 92%
2014-03-22 14:47:13,452 Stage-5 map = 100%,  reduce = 93%
2014-03-22 14:47:14,490 Stage-5 map = 100%,  reduce = 95%
2014-03-22 14:47:15,505 Stage-5 map = 100%,  reduce = 97%
2014-03-22 14:47:17,534 Stage-5 map = 100%,  reduce = 99%
2014-03-22 14:47:23,651 Stage-5 map = 100%,  reduce = 100%
2014-03-22 14:48:24,450 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784257
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 426
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784286, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784286
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784286
2014-03-22 14:48:35,399 Stage-7 map = 0%,  reduce = 0%
2014-03-22 14:48:44,727 Stage-7 map = 1%,  reduce = 0%
2014-03-22 14:48:45,739 Stage-7 map = 4%,  reduce = 0%
2014-03-22 14:48:46,750 Stage-7 map = 5%,  reduce = 0%
2014-03-22 14:49:01,471 Stage-7 map = 7%,  reduce = 0%
2014-03-22 14:49:02,479 Stage-7 map = 21%,  reduce = 0%
2014-03-22 14:49:03,491 Stage-7 map = 22%,  reduce = 0%
2014-03-22 14:49:04,502 Stage-7 map = 24%,  reduce = 0%
2014-03-22 14:49:05,512 Stage-7 map = 25%,  reduce = 0%
2014-03-22 14:49:06,522 Stage-7 map = 27%,  reduce = 0%
2014-03-22 14:49:07,548 Stage-7 map = 28%,  reduce = 0%
2014-03-22 14:49:09,576 Stage-7 map = 30%,  reduce = 0%
2014-03-22 14:49:10,590 Stage-7 map = 32%,  reduce = 0%
2014-03-22 14:49:11,600 Stage-7 map = 34%,  reduce = 0%
2014-03-22 14:49:12,610 Stage-7 map = 35%,  reduce = 0%
2014-03-22 14:49:13,620 Stage-7 map = 37%,  reduce = 0%
2014-03-22 14:49:14,630 Stage-7 map = 38%,  reduce = 0%
2014-03-22 14:49:15,640 Stage-7 map = 39%,  reduce = 0%
2014-03-22 14:49:16,650 Stage-7 map = 40%,  reduce = 0%
2014-03-22 14:49:17,660 Stage-7 map = 41%,  reduce = 0%
2014-03-22 14:49:18,670 Stage-7 map = 43%,  reduce = 0%
2014-03-22 14:49:19,680 Stage-7 map = 44%,  reduce = 0%
2014-03-22 14:49:20,690 Stage-7 map = 46%,  reduce = 0%
2014-03-22 14:49:21,700 Stage-7 map = 47%,  reduce = 0%
2014-03-22 14:49:22,720 Stage-7 map = 49%,  reduce = 0%
2014-03-22 14:49:24,740 Stage-7 map = 50%,  reduce = 0%
2014-03-22 14:49:25,752 Stage-7 map = 52%,  reduce = 0%
2014-03-22 14:49:26,763 Stage-7 map = 54%,  reduce = 0%
2014-03-22 14:49:27,774 Stage-7 map = 55%,  reduce = 0%
2014-03-22 14:49:28,785 Stage-7 map = 56%,  reduce = 0%
2014-03-22 14:49:29,796 Stage-7 map = 57%,  reduce = 0%
2014-03-22 14:49:30,823 Stage-7 map = 58%,  reduce = 0%
2014-03-22 14:49:31,901 Stage-7 map = 59%,  reduce = 0%
2014-03-22 14:49:32,927 Stage-7 map = 60%,  reduce = 0%
2014-03-22 14:49:33,950 Stage-7 map = 62%,  reduce = 0%
2014-03-22 14:49:34,961 Stage-7 map = 63%,  reduce = 0%
2014-03-22 14:49:35,976 Stage-7 map = 65%,  reduce = 0%
2014-03-22 14:49:36,990 Stage-7 map = 66%,  reduce = 0%
2014-03-22 14:49:38,157 Stage-7 map = 67%,  reduce = 0%
2014-03-22 14:49:39,255 Stage-7 map = 68%,  reduce = 0%
2014-03-22 14:49:40,356 Stage-7 map = 70%,  reduce = 0%
2014-03-22 14:49:41,392 Stage-7 map = 71%,  reduce = 0%
2014-03-22 14:50:50,684 Stage-7 map = 72%,  reduce = 0%
2014-03-22 14:50:53,406 Stage-7 map = 93%,  reduce = 0%
2014-03-22 14:50:54,799 Stage-7 map = 99%,  reduce = 0%
2014-03-22 14:50:59,666 Stage-7 map = 100%,  reduce = 0%
2014-03-22 14:51:04,079 Stage-7 map = 100%,  reduce = 3%
2014-03-22 14:51:05,185 Stage-7 map = 100%,  reduce = 7%
2014-03-22 14:51:06,506 Stage-7 map = 100%,  reduce = 10%
2014-03-22 14:51:07,646 Stage-7 map = 100%,  reduce = 14%
2014-03-22 14:51:08,672 Stage-7 map = 100%,  reduce = 18%
2014-03-22 14:51:09,859 Stage-7 map = 100%,  reduce = 20%
2014-03-22 14:51:11,169 Stage-7 map = 100%,  reduce = 21%
2014-03-22 14:51:14,715 Stage-7 map = 100%,  reduce = 22%
2014-03-22 14:51:21,480 Stage-7 map = 100%,  reduce = 23%
2014-03-22 14:51:25,562 Stage-7 map = 100%,  reduce = 24%
2014-03-22 14:51:35,187 Stage-7 map = 100%,  reduce = 25%
2014-03-22 14:51:36,214 Stage-7 map = 100%,  reduce = 27%
2014-03-22 14:51:38,316 Stage-7 map = 100%,  reduce = 28%
2014-03-22 14:51:45,833 Stage-7 map = 100%,  reduce = 30%
2014-03-22 14:51:46,867 Stage-7 map = 100%,  reduce = 32%
2014-03-22 14:51:48,366 Stage-7 map = 100%,  reduce = 33%
2014-03-22 14:52:12,689 Stage-7 map = 100%,  reduce = 35%
2014-03-22 14:52:14,473 Stage-7 map = 100%,  reduce = 46%
2014-03-22 14:52:15,750 Stage-7 map = 100%,  reduce = 66%
2014-03-22 14:52:18,235 Stage-7 map = 100%,  reduce = 87%
2014-03-22 14:52:19,847 Stage-7 map = 100%,  reduce = 97%
2014-03-22 14:52:20,865 Stage-7 map = 100%,  reduce = 100%
2014-03-22 14:53:21,138 Stage-7 map = 100%,  reduce = 100%
2014-03-22 14:54:22,099 Stage-7 map = 100%,  reduce = 100%
2014-03-22 14:55:22,819 Stage-7 map = 100%,  reduce = 100%
2014-03-22 14:56:22,875 Stage-7 map = 100%,  reduce = 100%
2014-03-22 14:57:23,467 Stage-7 map = 100%,  reduce = 100%
2014-03-22 15:01:03,678 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784286
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784374, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784374
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784374
2014-03-22 15:01:22,883 Stage-6 map = 0%,  reduce = 0%
2014-03-22 15:01:27,783 Stage-6 map = 11%,  reduce = 0%
2014-03-22 15:01:28,934 Stage-6 map = 39%,  reduce = 0%
2014-03-22 15:01:31,343 Stage-6 map = 61%,  reduce = 0%
2014-03-22 15:01:32,599 Stage-6 map = 83%,  reduce = 0%
2014-03-22 15:01:35,906 Stage-6 map = 94%,  reduce = 0%
2014-03-22 15:01:39,027 Stage-6 map = 100%,  reduce = 0%
2014-03-22 15:01:46,598 Stage-6 map = 100%,  reduce = 31%
2014-03-22 15:01:50,013 Stage-6 map = 100%,  reduce = 100%
2014-03-22 15:03:00,839 Stage-6 map = 100%,  reduce = 100%
2014-03-22 15:04:07,703 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784374
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784404, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784404
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784404
2014-03-22 15:06:31,702 Stage-8 map = 0%,  reduce = 0%
2014-03-22 15:07:41,166 Stage-8 map = 0%,  reduce = 0%
2014-03-22 15:07:44,219 Stage-8 map = 11%,  reduce = 0%
2014-03-22 15:07:46,245 Stage-8 map = 23%,  reduce = 0%
2014-03-22 15:07:47,369 Stage-8 map = 69%,  reduce = 0%
2014-03-22 15:07:48,532 Stage-8 map = 83%,  reduce = 0%
2014-03-22 15:07:49,562 Stage-8 map = 88%,  reduce = 0%
2014-03-22 15:07:50,570 Stage-8 map = 91%,  reduce = 0%
2014-03-22 15:07:51,581 Stage-8 map = 92%,  reduce = 0%
2014-03-22 15:07:52,605 Stage-8 map = 94%,  reduce = 0%
2014-03-22 15:07:55,638 Stage-8 map = 95%,  reduce = 0%
2014-03-22 15:08:00,701 Stage-8 map = 96%,  reduce = 0%
2014-03-22 15:08:01,711 Stage-8 map = 97%,  reduce = 0%
2014-03-22 15:08:03,743 Stage-8 map = 98%,  reduce = 0%
2014-03-22 15:08:15,879 Stage-8 map = 99%,  reduce = 0%
2014-03-22 15:08:21,930 Stage-8 map = 100%,  reduce = 0%
2014-03-22 15:09:30,766 Stage-8 map = 100%,  reduce = 0%
2014-03-22 15:11:30,420 Stage-8 map = 100%,  reduce = 0%
2014-03-22 15:12:57,972 Stage-8 map = 100%,  reduce = 0%
2014-03-22 15:14:09,449 Stage-8 map = 100%,  reduce = 0%
2014-03-22 15:15:09,745 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784404
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784520, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784520
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784520
2014-03-22 15:15:14,024 Stage-2 map = 0%,  reduce = 0%
2014-03-22 15:15:19,196 Stage-2 map = 100%,  reduce = 0%
2014-03-22 15:15:38,992 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784520
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784555, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784555
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784555
2014-03-22 15:15:46,963 Stage-3 map = 0%,  reduce = 0%
2014-03-22 15:15:51,589 Stage-3 map = 100%,  reduce = 0%
2014-03-22 15:16:47,051 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784555
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0321
6 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-22_14-40-03_379_9080365162989025550/-ext-10000
OK
Time taken: 2211.73 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0321 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-21' and dd>='2014-03-20' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-21' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-21' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403221516_1319930746.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784585, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784585
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784585
2014-03-22 15:17:14,493 Stage-1 map = 0%,  reduce = 0%
2014-03-22 15:17:19,795 Stage-1 map = 5%,  reduce = 0%
2014-03-22 15:17:20,804 Stage-1 map = 32%,  reduce = 0%
2014-03-22 15:17:22,736 Stage-1 map = 42%,  reduce = 0%
2014-03-22 15:17:23,979 Stage-1 map = 65%,  reduce = 0%
2014-03-22 15:17:25,065 Stage-1 map = 84%,  reduce = 0%
2014-03-22 15:17:26,107 Stage-1 map = 89%,  reduce = 0%
2014-03-22 15:17:27,449 Stage-1 map = 93%,  reduce = 0%
2014-03-22 15:17:28,871 Stage-1 map = 94%,  reduce = 0%
2014-03-22 15:17:30,297 Stage-1 map = 95%,  reduce = 0%
2014-03-22 15:17:34,067 Stage-1 map = 96%,  reduce = 0%
2014-03-22 15:17:37,218 Stage-1 map = 97%,  reduce = 0%
2014-03-22 15:17:42,638 Stage-1 map = 97%,  reduce = 32%
2014-03-22 15:18:23,447 Stage-1 map = 98%,  reduce = 32%
2014-03-22 15:18:25,288 Stage-1 map = 99%,  reduce = 32%
2014-03-22 15:18:26,565 Stage-1 map = 100%,  reduce = 33%
2014-03-22 15:18:36,496 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784585
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784798, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784798
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784798
2014-03-22 15:19:41,901 Stage-3 map = 0%,  reduce = 0%
2014-03-22 15:20:11,703 Stage-3 map = 43%,  reduce = 0%
2014-03-22 15:21:16,246 Stage-3 map = 74%,  reduce = 0%
2014-03-22 15:21:18,920 Stage-3 map = 89%,  reduce = 0%
2014-03-22 15:21:20,792 Stage-3 map = 94%,  reduce = 0%
2014-03-22 15:21:21,911 Stage-3 map = 95%,  reduce = 0%
2014-03-22 15:21:22,950 Stage-3 map = 96%,  reduce = 0%
2014-03-22 15:21:24,979 Stage-3 map = 97%,  reduce = 0%
2014-03-22 15:21:26,007 Stage-3 map = 98%,  reduce = 0%
2014-03-22 15:21:27,026 Stage-3 map = 99%,  reduce = 0%
2014-03-22 15:22:10,773 Stage-3 map = 100%,  reduce = 11%
2014-03-22 15:22:30,127 Stage-3 map = 100%,  reduce = 100%
2014-03-22 15:23:33,679 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784798
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784860, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784860
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784860
2014-03-22 15:24:28,019 Stage-4 map = 0%,  reduce = 0%
2014-03-22 15:24:36,786 Stage-4 map = 10%,  reduce = 0%
2014-03-22 15:24:44,256 Stage-4 map = 32%,  reduce = 0%
2014-03-22 15:24:54,427 Stage-4 map = 48%,  reduce = 0%
2014-03-22 15:24:56,756 Stage-4 map = 75%,  reduce = 0%
2014-03-22 15:24:58,371 Stage-4 map = 80%,  reduce = 0%
2014-03-22 15:24:59,512 Stage-4 map = 88%,  reduce = 0%
2014-03-22 15:25:00,679 Stage-4 map = 95%,  reduce = 0%
2014-03-22 15:25:01,696 Stage-4 map = 99%,  reduce = 0%
2014-03-22 15:25:03,787 Stage-4 map = 100%,  reduce = 0%
2014-03-22 15:25:13,414 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784860
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784914, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784914
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784914
2014-03-22 15:26:11,755 Stage-2 map = 0%,  reduce = 0%
2014-03-22 15:26:47,573 Stage-2 map = 100%,  reduce = 0%
2014-03-22 15:26:56,176 Stage-2 map = 100%,  reduce = 33%
2014-03-22 15:27:12,081 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784914
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0321
29 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-22_15-16-56_913_3714579896853901220/-ext-10000
OK
Time taken: 617.548 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0321 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0321 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403221527_294813070.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784959, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784959
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784959
2014-03-22 15:27:27,602 Stage-3 map = 0%,  reduce = 0%
2014-03-22 15:28:17,692 Stage-3 map = 100%,  reduce = 0%
2014-03-22 15:28:32,297 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784959
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_784995, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_784995
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_784995
2014-03-22 15:28:41,387 Stage-1 map = 0%,  reduce = 0%
2014-03-22 15:28:46,424 Stage-1 map = 33%,  reduce = 0%
2014-03-22 15:28:48,900 Stage-1 map = 67%,  reduce = 0%
2014-03-22 15:28:51,145 Stage-1 map = 100%,  reduce = 0%
2014-03-22 15:28:59,988 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_784995
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_785020, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_785020
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_785020
2014-03-22 15:29:06,473 Stage-2 map = 0%,  reduce = 0%
2014-03-22 15:29:11,746 Stage-2 map = 100%,  reduce = 0%
2014-03-22 15:29:19,139 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_785020
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21
6 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21
OK
Time taken: 125.95 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21/* > /home/group_dataanalysis/yhd_Report/res-2014-03-21/output-1
14/03/22 15:29:23 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/22 15:29:23 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0321 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403221529_1762525499.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_785031, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_785031
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_785031
2014-03-22 15:29:34,022 Stage-1 map = 0%,  reduce = 0%
2014-03-22 15:29:46,320 Stage-1 map = 50%,  reduce = 0%
2014-03-22 15:29:47,402 Stage-1 map = 100%,  reduce = 0%
2014-03-22 15:29:55,859 Stage-1 map = 100%,  reduce = 33%
2014-03-22 15:29:57,486 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_785031
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_785050, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_785050
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_785050
2014-03-22 15:30:04,310 Stage-2 map = 0%,  reduce = 0%
2014-03-22 15:30:08,396 Stage-2 map = 50%,  reduce = 0%
2014-03-22 15:30:09,457 Stage-2 map = 100%,  reduce = 0%
2014-03-22 15:30:18,160 Stage-2 map = 100%,  reduce = 33%
2014-03-22 15:30:19,195 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_785050
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_785061, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_785061
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_785061
2014-03-22 15:30:26,508 Stage-3 map = 0%,  reduce = 0%
2014-03-22 15:30:32,684 Stage-3 map = 100%,  reduce = 0%
2014-03-22 15:30:39,124 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_785061
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21
OK
Time taken: 76.711 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-21/* > /home/group_dataanalysis/yhd_Report/res-2014-03-21/output-2
14/03/22 15:30:43 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/22 15:30:43 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395473443.79
total time is :  0.0649240016937 seconds
size is       :  10676 KB
speed is      :  0.164438416017 MB/s
140321TFLA8H 20
140321SFMLX8 20
140321LFL3GN 3
140321KFJ6PC 20
140321LFL960 38
140321YFNK2S 20
140321KFN9G3 38
140321VFP0XE 38
140321HFG6RJ 20
140321UFJB5X 20
140321EFJGNM 20
140321WFLQAG 20
140321LFK95R 3
140321PFHHC5 20
140321PFF4YE 20
140321CFKXEL 20
140321VFK4VL 20
140321BFG65A 20
140321EFLXNM 20
140321EFJ57W 20
140321AFHC22 3
140321QFHUKE 20
140321LFJGFB 20
140321FFL1FG 20
140321GFMU0J 20
140321KFQKYU 4
140321BFQ1JP 3
140321MFPLS2 38
140321PFG6EN 20
140321JFH2B9 20
#######################SAVE DATE TO FILE#########################
#######################MAIL FILE#########################
<logging.Logger instance at 0x2b8ab335efc8>
{'loglevel': 'debug', 'debug': False, 'logfile': None}
['res-2014-03-21', '0321', '2014-03-21']
res-2014-03-21/yhd_dsp_report_0321.xls
mail_exchange -s '[2014-03-21]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-21/yhd_dsp_report_0321.xls' -u 'wentao_wang' -p '2238681Xwww' 
Main       2014-03-22 15:30:45,291 INFO  executing: mail_exchange -s '[2014-03-21]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-21/yhd_dsp_report_0321.xls' -u 'wentao_wang' -p '2238681Xwww' 
20140322_153048
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0322 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-22' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-22' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-22' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403231440_264778126.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808096, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808096
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808096
2014-03-23 14:45:32,712 Stage-1 map = 0%,  reduce = 0%
2014-03-23 14:45:37,019 Stage-1 map = 14%,  reduce = 0%
2014-03-23 14:45:38,062 Stage-1 map = 47%,  reduce = 0%
2014-03-23 14:46:18,584 Stage-1 map = 73%,  reduce = 0%
2014-03-23 14:46:58,472 Stage-1 map = 94%,  reduce = 0%
2014-03-23 14:46:59,647 Stage-1 map = 100%,  reduce = 0%
2014-03-23 14:47:47,708 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808096
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 17
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808128, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808128
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808128
2014-03-23 14:48:21,089 Stage-5 map = 0%,  reduce = 0%
2014-03-23 14:49:27,124 Stage-5 map = 0%,  reduce = 0%
2014-03-23 14:49:29,360 Stage-5 map = 19%,  reduce = 0%
2014-03-23 14:49:30,410 Stage-5 map = 25%,  reduce = 0%
2014-03-23 14:49:32,462 Stage-5 map = 27%,  reduce = 0%
2014-03-23 14:50:39,206 Stage-5 map = 30%,  reduce = 0%
2014-03-23 14:50:42,157 Stage-5 map = 58%,  reduce = 0%
2014-03-23 14:50:43,550 Stage-5 map = 68%,  reduce = 0%
2014-03-23 14:50:44,646 Stage-5 map = 70%,  reduce = 0%
2014-03-23 14:50:48,197 Stage-5 map = 71%,  reduce = 0%
2014-03-23 14:50:49,229 Stage-5 map = 72%,  reduce = 0%
2014-03-23 14:50:52,300 Stage-5 map = 73%,  reduce = 0%
2014-03-23 14:50:54,360 Stage-5 map = 75%,  reduce = 0%
2014-03-23 14:50:56,422 Stage-5 map = 76%,  reduce = 0%
2014-03-23 14:50:58,459 Stage-5 map = 77%,  reduce = 0%
2014-03-23 14:51:00,481 Stage-5 map = 78%,  reduce = 0%
2014-03-23 14:51:03,532 Stage-5 map = 80%,  reduce = 0%
2014-03-23 14:51:06,631 Stage-5 map = 81%,  reduce = 0%
2014-03-23 14:51:07,677 Stage-5 map = 82%,  reduce = 0%
2014-03-23 14:51:08,688 Stage-5 map = 83%,  reduce = 0%
2014-03-23 14:51:09,705 Stage-5 map = 84%,  reduce = 0%
2014-03-23 14:51:11,770 Stage-5 map = 85%,  reduce = 0%
2014-03-23 14:51:12,828 Stage-5 map = 87%,  reduce = 0%
2014-03-23 14:51:15,002 Stage-5 map = 88%,  reduce = 0%
2014-03-23 14:51:16,038 Stage-5 map = 89%,  reduce = 0%
2014-03-23 14:51:18,152 Stage-5 map = 91%,  reduce = 0%
2014-03-23 14:51:21,181 Stage-5 map = 93%,  reduce = 0%
2014-03-23 14:51:22,192 Stage-5 map = 94%,  reduce = 0%
2014-03-23 14:51:24,258 Stage-5 map = 95%,  reduce = 0%
2014-03-23 14:51:26,278 Stage-5 map = 97%,  reduce = 0%
2014-03-23 14:51:28,321 Stage-5 map = 98%,  reduce = 0%
2014-03-23 14:51:32,705 Stage-5 map = 99%,  reduce = 0%
2014-03-23 14:51:34,775 Stage-5 map = 99%,  reduce = 1%
2014-03-23 14:51:35,801 Stage-5 map = 99%,  reduce = 11%
2014-03-23 14:51:36,810 Stage-5 map = 99%,  reduce = 20%
2014-03-23 14:51:37,838 Stage-5 map = 99%,  reduce = 25%
2014-03-23 14:51:38,883 Stage-5 map = 99%,  reduce = 27%
2014-03-23 14:51:39,899 Stage-5 map = 100%,  reduce = 27%
2014-03-23 14:51:40,915 Stage-5 map = 100%,  reduce = 33%
2014-03-23 14:51:50,214 Stage-5 map = 100%,  reduce = 44%
2014-03-23 14:51:51,227 Stage-5 map = 100%,  reduce = 53%
2014-03-23 14:51:52,239 Stage-5 map = 100%,  reduce = 62%
2014-03-23 14:51:53,252 Stage-5 map = 100%,  reduce = 77%
2014-03-23 14:51:54,574 Stage-5 map = 100%,  reduce = 85%
2014-03-23 14:51:55,823 Stage-5 map = 100%,  reduce = 98%
2014-03-23 14:51:56,837 Stage-5 map = 100%,  reduce = 99%
2014-03-23 14:52:09,364 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808128
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 512
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808170, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808170
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808170
2014-03-23 14:52:32,792 Stage-7 map = 0%,  reduce = 0%
2014-03-23 14:53:18,159 Stage-7 map = 5%,  reduce = 0%
2014-03-23 14:53:19,743 Stage-7 map = 6%,  reduce = 0%
2014-03-23 14:53:24,451 Stage-7 map = 7%,  reduce = 0%
2014-03-23 14:53:26,496 Stage-7 map = 8%,  reduce = 0%
2014-03-23 14:53:29,542 Stage-7 map = 9%,  reduce = 0%
2014-03-23 14:53:31,556 Stage-7 map = 10%,  reduce = 0%
2014-03-23 14:53:35,128 Stage-7 map = 11%,  reduce = 0%
2014-03-23 14:53:37,255 Stage-7 map = 12%,  reduce = 0%
2014-03-23 14:53:39,274 Stage-7 map = 13%,  reduce = 0%
2014-03-23 14:53:41,289 Stage-7 map = 14%,  reduce = 0%
2014-03-23 14:53:43,302 Stage-7 map = 15%,  reduce = 0%
2014-03-23 14:53:45,318 Stage-7 map = 16%,  reduce = 0%
2014-03-23 14:53:47,345 Stage-7 map = 17%,  reduce = 0%
2014-03-23 14:53:49,367 Stage-7 map = 18%,  reduce = 0%
2014-03-23 14:53:50,395 Stage-7 map = 19%,  reduce = 0%
2014-03-23 14:53:52,526 Stage-7 map = 20%,  reduce = 0%
2014-03-23 14:53:54,587 Stage-7 map = 21%,  reduce = 0%
2014-03-23 14:53:55,593 Stage-7 map = 22%,  reduce = 0%
2014-03-23 14:53:57,648 Stage-7 map = 23%,  reduce = 0%
2014-03-23 14:53:59,663 Stage-7 map = 24%,  reduce = 0%
2014-03-23 14:54:00,670 Stage-7 map = 25%,  reduce = 0%
2014-03-23 14:54:03,848 Stage-7 map = 26%,  reduce = 0%
2014-03-23 14:54:05,026 Stage-7 map = 27%,  reduce = 0%
2014-03-23 14:54:06,377 Stage-7 map = 28%,  reduce = 0%
2014-03-23 14:54:07,532 Stage-7 map = 29%,  reduce = 0%
2014-03-23 14:54:08,683 Stage-7 map = 30%,  reduce = 0%
2014-03-23 14:54:09,821 Stage-7 map = 31%,  reduce = 0%
2014-03-23 14:54:12,316 Stage-7 map = 32%,  reduce = 0%
2014-03-23 14:54:13,482 Stage-7 map = 33%,  reduce = 0%
2014-03-23 14:54:14,564 Stage-7 map = 34%,  reduce = 0%
2014-03-23 14:54:15,585 Stage-7 map = 35%,  reduce = 0%
2014-03-23 14:54:17,672 Stage-7 map = 36%,  reduce = 0%
2014-03-23 14:54:18,684 Stage-7 map = 37%,  reduce = 0%
2014-03-23 14:54:19,696 Stage-7 map = 38%,  reduce = 0%
2014-03-23 14:54:21,881 Stage-7 map = 39%,  reduce = 0%
2014-03-23 14:54:22,903 Stage-7 map = 40%,  reduce = 0%
2014-03-23 14:54:24,937 Stage-7 map = 41%,  reduce = 0%
2014-03-23 14:54:26,981 Stage-7 map = 42%,  reduce = 0%
2014-03-23 14:54:28,010 Stage-7 map = 43%,  reduce = 0%
2014-03-23 14:54:30,105 Stage-7 map = 44%,  reduce = 0%
2014-03-23 14:54:31,231 Stage-7 map = 45%,  reduce = 0%
2014-03-23 14:54:33,385 Stage-7 map = 46%,  reduce = 0%
2014-03-23 14:54:35,429 Stage-7 map = 47%,  reduce = 0%
2014-03-23 14:54:37,455 Stage-7 map = 48%,  reduce = 0%
2014-03-23 14:54:38,472 Stage-7 map = 49%,  reduce = 0%
2014-03-23 14:54:40,532 Stage-7 map = 50%,  reduce = 0%
2014-03-23 14:54:42,612 Stage-7 map = 51%,  reduce = 0%
2014-03-23 14:54:45,717 Stage-7 map = 52%,  reduce = 0%
2014-03-23 14:54:47,783 Stage-7 map = 53%,  reduce = 0%
2014-03-23 14:54:50,676 Stage-7 map = 54%,  reduce = 0%
2014-03-23 14:54:53,803 Stage-7 map = 55%,  reduce = 0%
2014-03-23 14:54:57,876 Stage-7 map = 56%,  reduce = 0%
2014-03-23 14:55:00,967 Stage-7 map = 57%,  reduce = 0%
2014-03-23 14:55:04,057 Stage-7 map = 58%,  reduce = 0%
2014-03-23 14:55:06,121 Stage-7 map = 59%,  reduce = 0%
2014-03-23 14:55:08,159 Stage-7 map = 60%,  reduce = 0%
2014-03-23 14:55:10,194 Stage-7 map = 61%,  reduce = 0%
2014-03-23 14:55:11,208 Stage-7 map = 62%,  reduce = 0%
2014-03-23 14:55:12,231 Stage-7 map = 63%,  reduce = 0%
2014-03-23 14:55:14,266 Stage-7 map = 65%,  reduce = 0%
2014-03-23 14:55:15,283 Stage-7 map = 66%,  reduce = 0%
2014-03-23 14:55:17,374 Stage-7 map = 68%,  reduce = 0%
2014-03-23 14:55:19,423 Stage-7 map = 69%,  reduce = 0%
2014-03-23 14:55:20,444 Stage-7 map = 70%,  reduce = 0%
2014-03-23 14:55:21,462 Stage-7 map = 71%,  reduce = 0%
2014-03-23 14:55:22,475 Stage-7 map = 72%,  reduce = 0%
2014-03-23 14:55:23,501 Stage-7 map = 74%,  reduce = 0%
2014-03-23 14:55:24,518 Stage-7 map = 75%,  reduce = 0%
2014-03-23 14:55:25,533 Stage-7 map = 76%,  reduce = 0%
2014-03-23 14:55:26,566 Stage-7 map = 77%,  reduce = 0%
2014-03-23 14:55:27,591 Stage-7 map = 78%,  reduce = 0%
2014-03-23 14:55:28,608 Stage-7 map = 79%,  reduce = 0%
2014-03-23 14:55:29,621 Stage-7 map = 80%,  reduce = 0%
2014-03-23 14:55:30,645 Stage-7 map = 82%,  reduce = 0%
2014-03-23 14:55:31,668 Stage-7 map = 83%,  reduce = 0%
2014-03-23 14:55:32,729 Stage-7 map = 84%,  reduce = 0%
2014-03-23 14:55:33,755 Stage-7 map = 86%,  reduce = 0%
2014-03-23 14:55:34,781 Stage-7 map = 87%,  reduce = 0%
2014-03-23 14:55:35,794 Stage-7 map = 88%,  reduce = 0%
2014-03-23 14:55:36,822 Stage-7 map = 89%,  reduce = 0%
2014-03-23 14:55:37,868 Stage-7 map = 90%,  reduce = 0%
2014-03-23 14:55:38,979 Stage-7 map = 91%,  reduce = 0%
2014-03-23 14:55:40,058 Stage-7 map = 92%,  reduce = 0%
2014-03-23 14:55:42,192 Stage-7 map = 93%,  reduce = 0%
2014-03-23 14:55:44,294 Stage-7 map = 94%,  reduce = 0%
2014-03-23 14:55:46,365 Stage-7 map = 95%,  reduce = 0%
2014-03-23 14:55:49,452 Stage-7 map = 96%,  reduce = 0%
2014-03-23 14:55:52,981 Stage-7 map = 97%,  reduce = 0%
2014-03-23 14:55:57,155 Stage-7 map = 98%,  reduce = 0%
2014-03-23 14:56:02,459 Stage-7 map = 99%,  reduce = 0%
2014-03-23 14:56:20,811 Stage-7 map = 100%,  reduce = 0%
2014-03-23 14:56:23,881 Stage-7 map = 100%,  reduce = 4%
2014-03-23 14:56:24,936 Stage-7 map = 100%,  reduce = 7%
2014-03-23 14:56:25,990 Stage-7 map = 100%,  reduce = 8%
2014-03-23 14:56:27,005 Stage-7 map = 100%,  reduce = 12%
2014-03-23 14:56:28,031 Stage-7 map = 100%,  reduce = 17%
2014-03-23 14:56:29,063 Stage-7 map = 100%,  reduce = 18%
2014-03-23 14:56:30,078 Stage-7 map = 100%,  reduce = 22%
2014-03-23 14:56:31,092 Stage-7 map = 100%,  reduce = 26%
2014-03-23 14:56:32,140 Stage-7 map = 100%,  reduce = 30%
2014-03-23 14:56:33,172 Stage-7 map = 100%,  reduce = 31%
2014-03-23 14:56:34,186 Stage-7 map = 100%,  reduce = 32%
2014-03-23 14:56:39,250 Stage-7 map = 100%,  reduce = 33%
2014-03-23 14:57:18,417 Stage-7 map = 100%,  reduce = 34%
2014-03-23 14:57:19,462 Stage-7 map = 100%,  reduce = 42%
2014-03-23 14:57:20,490 Stage-7 map = 100%,  reduce = 66%
2014-03-23 14:57:21,539 Stage-7 map = 100%,  reduce = 75%
2014-03-23 14:57:22,565 Stage-7 map = 100%,  reduce = 81%
2014-03-23 14:57:23,610 Stage-7 map = 100%,  reduce = 89%
2014-03-23 14:57:24,627 Stage-7 map = 100%,  reduce = 95%
2014-03-23 14:57:25,644 Stage-7 map = 100%,  reduce = 97%
2014-03-23 14:57:26,696 Stage-7 map = 100%,  reduce = 98%
2014-03-23 14:57:27,744 Stage-7 map = 100%,  reduce = 99%
2014-03-23 14:57:29,777 Stage-7 map = 100%,  reduce = 100%
2014-03-23 14:59:01,821 Stage-7 map = 100%,  reduce = 100%
2014-03-23 15:00:11,222 Stage-7 map = 100%,  reduce = 100%
2014-03-23 15:01:36,718 Stage-7 map = 100%,  reduce = 100%
2014-03-23 15:02:37,740 Stage-7 map = 100%,  reduce = 100%
2014-03-23 15:04:46,167 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808170
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808252, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808252
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808252
2014-03-23 15:06:47,779 Stage-2 map = 0%,  reduce = 0%
2014-03-23 15:07:15,402 Stage-2 map = 100%,  reduce = 0%
2014-03-23 15:07:26,165 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808252
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808291, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808291
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808291
2014-03-23 15:07:34,863 Stage-8 map = 0%,  reduce = 0%
2014-03-23 15:07:45,420 Stage-8 map = 18%,  reduce = 0%
2014-03-23 15:07:47,100 Stage-8 map = 25%,  reduce = 0%
2014-03-23 15:07:49,220 Stage-8 map = 47%,  reduce = 0%
2014-03-23 15:07:50,329 Stage-8 map = 68%,  reduce = 0%
2014-03-23 15:07:51,483 Stage-8 map = 76%,  reduce = 0%
2014-03-23 15:07:52,908 Stage-8 map = 89%,  reduce = 0%
2014-03-23 15:07:54,345 Stage-8 map = 96%,  reduce = 0%
2014-03-23 15:09:02,498 Stage-8 map = 96%,  reduce = 0%
2014-03-23 15:09:05,460 Stage-8 map = 97%,  reduce = 32%
2014-03-23 15:09:07,463 Stage-8 map = 98%,  reduce = 32%
2014-03-23 15:09:09,766 Stage-8 map = 98%,  reduce = 33%
2014-03-23 15:09:44,370 Stage-8 map = 99%,  reduce = 33%
2014-03-23 15:10:01,687 Stage-8 map = 100%,  reduce = 33%
2014-03-23 15:11:05,663 Stage-8 map = 100%,  reduce = 33%
2014-03-23 15:11:11,447 Stage-8 map = 100%,  reduce = 100%
2014-03-23 15:12:19,425 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808291
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808354, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808354
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808354
2014-03-23 15:12:34,814 Stage-6 map = 0%,  reduce = 0%
2014-03-23 15:12:38,999 Stage-6 map = 12%,  reduce = 0%
2014-03-23 15:12:40,100 Stage-6 map = 24%,  reduce = 0%
2014-03-23 15:12:42,318 Stage-6 map = 41%,  reduce = 0%
2014-03-23 15:12:43,324 Stage-6 map = 59%,  reduce = 0%
2014-03-23 15:12:44,339 Stage-6 map = 71%,  reduce = 0%
2014-03-23 15:12:45,376 Stage-6 map = 82%,  reduce = 0%
2014-03-23 15:12:51,961 Stage-6 map = 94%,  reduce = 0%
2014-03-23 15:13:21,628 Stage-6 map = 100%,  reduce = 0%
2014-03-23 15:13:28,623 Stage-6 map = 100%,  reduce = 31%
2014-03-23 15:13:31,860 Stage-6 map = 100%,  reduce = 100%
2014-03-23 15:14:45,148 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808354
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808411, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808411
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808411
2014-03-23 15:15:06,126 Stage-3 map = 0%,  reduce = 0%
2014-03-23 15:15:11,967 Stage-3 map = 67%,  reduce = 0%
2014-03-23 15:15:14,865 Stage-3 map = 100%,  reduce = 0%
2014-03-23 15:15:21,694 Stage-3 map = 100%,  reduce = 33%
2014-03-23 15:15:22,897 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808411
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0322
12 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-23_14-40-03_405_7579125949752053564/-ext-10000
OK
Time taken: 2121.118 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0322 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-22' and dd>='2014-03-21' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-22' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-22' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403231515_457393925.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808449, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808449
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808449
2014-03-23 15:15:39,569 Stage-1 map = 0%,  reduce = 0%
2014-03-23 15:15:45,973 Stage-1 map = 2%,  reduce = 0%
2014-03-23 15:15:47,660 Stage-1 map = 7%,  reduce = 0%
2014-03-23 15:15:48,813 Stage-1 map = 17%,  reduce = 0%
2014-03-23 15:15:49,889 Stage-1 map = 39%,  reduce = 0%
2014-03-23 15:15:50,916 Stage-1 map = 54%,  reduce = 0%
2014-03-23 15:15:52,017 Stage-1 map = 64%,  reduce = 0%
2014-03-23 15:15:53,030 Stage-1 map = 73%,  reduce = 0%
2014-03-23 15:15:54,106 Stage-1 map = 78%,  reduce = 0%
2014-03-23 15:15:55,347 Stage-1 map = 85%,  reduce = 0%
2014-03-23 15:15:56,376 Stage-1 map = 90%,  reduce = 0%
2014-03-23 15:15:57,386 Stage-1 map = 93%,  reduce = 0%
2014-03-23 15:15:59,246 Stage-1 map = 94%,  reduce = 0%
2014-03-23 15:16:00,674 Stage-1 map = 97%,  reduce = 0%
2014-03-23 15:16:01,756 Stage-1 map = 98%,  reduce = 0%
2014-03-23 15:16:03,834 Stage-1 map = 99%,  reduce = 0%
2014-03-23 15:16:05,812 Stage-1 map = 100%,  reduce = 0%
2014-03-23 15:17:07,693 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808449
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808525, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808525
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808525
2014-03-23 15:17:18,438 Stage-3 map = 0%,  reduce = 0%
2014-03-23 15:17:29,576 Stage-3 map = 14%,  reduce = 0%
2014-03-23 15:17:32,620 Stage-3 map = 41%,  reduce = 0%
2014-03-23 15:17:34,494 Stage-3 map = 77%,  reduce = 0%
2014-03-23 15:17:35,632 Stage-3 map = 92%,  reduce = 0%
2014-03-23 15:17:37,780 Stage-3 map = 100%,  reduce = 0%
2014-03-23 15:17:46,224 Stage-3 map = 100%,  reduce = 16%
2014-03-23 15:17:54,695 Stage-3 map = 100%,  reduce = 66%
2014-03-23 15:17:56,868 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808525
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808584, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808584
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808584
2014-03-23 15:18:06,355 Stage-4 map = 0%,  reduce = 0%
2014-03-23 15:18:09,265 Stage-4 map = 2%,  reduce = 0%
2014-03-23 15:18:10,410 Stage-4 map = 12%,  reduce = 0%
2014-03-23 15:18:11,560 Stage-4 map = 18%,  reduce = 0%
2014-03-23 15:18:13,098 Stage-4 map = 20%,  reduce = 0%
2014-03-23 15:18:14,359 Stage-4 map = 26%,  reduce = 0%
2014-03-23 15:18:15,747 Stage-4 map = 39%,  reduce = 0%
2014-03-23 15:18:17,008 Stage-4 map = 53%,  reduce = 0%
2014-03-23 15:18:18,036 Stage-4 map = 61%,  reduce = 0%
2014-03-23 15:18:19,529 Stage-4 map = 70%,  reduce = 0%
2014-03-23 15:18:20,802 Stage-4 map = 79%,  reduce = 0%
2014-03-23 15:18:22,144 Stage-4 map = 91%,  reduce = 0%
2014-03-23 15:18:23,259 Stage-4 map = 95%,  reduce = 0%
2014-03-23 15:18:25,291 Stage-4 map = 98%,  reduce = 0%
2014-03-23 15:18:26,561 Stage-4 map = 99%,  reduce = 0%
2014-03-23 15:18:29,439 Stage-4 map = 100%,  reduce = 0%
2014-03-23 15:18:33,336 Stage-4 map = 100%,  reduce = 32%
2014-03-23 15:18:35,575 Stage-4 map = 100%,  reduce = 33%
2014-03-23 15:19:44,031 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808584
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808703, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808703
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808703
2014-03-23 15:20:34,549 Stage-2 map = 0%,  reduce = 0%
2014-03-23 15:20:46,069 Stage-2 map = 100%,  reduce = 0%
2014-03-23 15:21:14,659 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808703
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0322
17 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-23_15-15-26_748_4026178100429485310/-ext-10000
OK
Time taken: 350.108 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0322 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0322 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403231521_1804418777.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808744, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808744
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808744
2014-03-23 15:25:08,829 Stage-4 map = 0%,  reduce = 0%
2014-03-23 15:25:11,883 Stage-4 map = 100%,  reduce = 0%
2014-03-23 15:25:30,857 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808744
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808804, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808804
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808804
2014-03-23 15:26:21,473 Stage-1 map = 0%,  reduce = 0%
2014-03-23 15:26:35,848 Stage-1 map = 33%,  reduce = 0%
2014-03-23 15:26:37,502 Stage-1 map = 100%,  reduce = 0%
2014-03-23 15:26:47,841 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808804
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808842, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808842
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808842
2014-03-23 15:26:58,206 Stage-2 map = 0%,  reduce = 0%
2014-03-23 15:27:02,511 Stage-2 map = 100%,  reduce = 0%
2014-03-23 15:27:11,713 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808842
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22
12 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22
OK
Time taken: 354.63 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22/* > /home/group_dataanalysis/yhd_Report/res-2014-03-22/output-1
14/03/23 15:27:13 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/23 15:27:13 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0322 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403231527_58751244.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808884, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808884
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808884
2014-03-23 15:27:25,343 Stage-1 map = 0%,  reduce = 0%
2014-03-23 15:27:29,885 Stage-1 map = 50%,  reduce = 0%
2014-03-23 15:27:30,896 Stage-1 map = 100%,  reduce = 0%
2014-03-23 15:27:38,849 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808884
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808903, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808903
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808903
2014-03-23 15:27:45,807 Stage-2 map = 0%,  reduce = 0%
2014-03-23 15:27:50,845 Stage-2 map = 50%,  reduce = 0%
2014-03-23 15:27:52,858 Stage-2 map = 100%,  reduce = 0%
2014-03-23 15:28:03,091 Stage-2 map = 100%,  reduce = 33%
2014-03-23 15:28:04,101 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808903
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_808912, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_808912
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_808912
2014-03-23 15:28:10,099 Stage-3 map = 0%,  reduce = 0%
2014-03-23 15:28:15,156 Stage-3 map = 100%,  reduce = 0%
2014-03-23 15:28:23,712 Stage-3 map = 100%,  reduce = 17%
2014-03-23 15:28:27,843 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_808912
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22
OK
Time taken: 77.357 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-22/* > /home/group_dataanalysis/yhd_Report/res-2014-03-22/output-2
14/03/23 15:28:34 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/23 15:28:34 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395559714.71
total time is :  0.0652339458466 seconds
size is       :  6116 KB
speed is      :  0.0937548682765 MB/s
140322SFXA9U 20
140322VFV498 38
140322AG18GN 38
140322GG0SEP 38
140322WFXPRR 20
140322SFX8NX 34
140322KFWXPC 20
140322LFX0GS 20
140322XFUX2U 20
140322LFTR43 20
140322YFVRVK 20
140322AFT6GK 20
140322FFXC3L 38
140322EG1C8G 38
140322JG030R 3
140322LFTPFH 20
140322MFS9A7 3
#######################SAVE DATE TO FILE#########################
#######################MAIL FILE#########################
<logging.Logger instance at 0x2b3c2b0e4fc8>
{'loglevel': 'debug', 'debug': False, 'logfile': None}
['res-2014-03-22', '0322', '2014-03-22']
res-2014-03-22/yhd_dsp_report_0322.xls
mail_exchange -s '[2014-03-22]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-22/yhd_dsp_report_0322.xls' -u 'wentao_wang' -p '2238681Xwww' 
Main       2014-03-23 15:28:36,244 INFO  executing: mail_exchange -s '[2014-03-22]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-22/yhd_dsp_report_0322.xls' -u 'wentao_wang' -p '2238681Xwww' 
20140323_152839
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0323 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-23' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-23' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-23' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403241440_433801150.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_831795, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_831795
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_831795
2014-03-24 14:42:48,502 Stage-1 map = 0%,  reduce = 0%
2014-03-24 14:43:53,350 Stage-1 map = 0%,  reduce = 0%
2014-03-24 14:46:04,457 Stage-1 map = 85%,  reduce = 0%
2014-03-24 14:46:05,875 Stage-1 map = 96%,  reduce = 0%
2014-03-24 14:46:06,979 Stage-1 map = 100%,  reduce = 0%
2014-03-24 14:46:33,415 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_831795
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 19
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_831836, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_831836
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_831836
2014-03-24 14:46:59,819 Stage-5 map = 0%,  reduce = 0%
2014-03-24 14:47:14,581 Stage-5 map = 1%,  reduce = 0%
2014-03-24 14:47:15,638 Stage-5 map = 2%,  reduce = 0%
2014-03-24 14:47:16,763 Stage-5 map = 3%,  reduce = 0%
2014-03-24 14:47:17,924 Stage-5 map = 6%,  reduce = 0%
2014-03-24 14:47:18,941 Stage-5 map = 12%,  reduce = 0%
2014-03-24 14:47:20,301 Stage-5 map = 29%,  reduce = 0%
2014-03-24 14:47:21,597 Stage-5 map = 39%,  reduce = 0%
2014-03-24 14:47:22,761 Stage-5 map = 50%,  reduce = 0%
2014-03-24 14:47:23,874 Stage-5 map = 57%,  reduce = 0%
2014-03-24 14:47:24,958 Stage-5 map = 63%,  reduce = 0%
2014-03-24 14:47:25,971 Stage-5 map = 67%,  reduce = 0%
2014-03-24 14:47:27,062 Stage-5 map = 70%,  reduce = 0%
2014-03-24 14:47:29,195 Stage-5 map = 71%,  reduce = 0%
2014-03-24 14:47:36,576 Stage-5 map = 72%,  reduce = 0%
2014-03-24 14:48:09,309 Stage-5 map = 77%,  reduce = 0%
2014-03-24 14:48:10,905 Stage-5 map = 93%,  reduce = 0%
2014-03-24 14:48:16,276 Stage-5 map = 96%,  reduce = 0%
2014-03-24 14:48:17,461 Stage-5 map = 98%,  reduce = 0%
2014-03-24 14:48:18,542 Stage-5 map = 99%,  reduce = 0%
2014-03-24 14:48:20,943 Stage-5 map = 100%,  reduce = 0%
2014-03-24 14:48:28,888 Stage-5 map = 100%,  reduce = 13%
2014-03-24 14:48:29,952 Stage-5 map = 100%,  reduce = 30%
2014-03-24 14:48:32,725 Stage-5 map = 100%,  reduce = 35%
2014-03-24 14:49:40,196 Stage-5 map = 100%,  reduce = 56%
2014-03-24 14:50:48,418 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_831836
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 525
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_831882, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_831882
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_831882
2014-03-24 14:50:59,726 Stage-7 map = 0%,  reduce = 0%
2014-03-24 14:51:09,931 Stage-7 map = 1%,  reduce = 0%
2014-03-24 14:51:13,653 Stage-7 map = 3%,  reduce = 0%
2014-03-24 14:51:14,660 Stage-7 map = 4%,  reduce = 0%
2014-03-24 14:51:18,983 Stage-7 map = 5%,  reduce = 0%
2014-03-24 14:51:23,147 Stage-7 map = 6%,  reduce = 0%
2014-03-24 14:51:28,395 Stage-7 map = 7%,  reduce = 0%
2014-03-24 14:51:31,452 Stage-7 map = 8%,  reduce = 0%
2014-03-24 14:51:33,593 Stage-7 map = 9%,  reduce = 0%
2014-03-24 14:51:36,627 Stage-7 map = 10%,  reduce = 0%
2014-03-24 14:51:38,769 Stage-7 map = 11%,  reduce = 0%
2014-03-24 14:51:40,941 Stage-7 map = 12%,  reduce = 0%
2014-03-24 14:51:43,240 Stage-7 map = 13%,  reduce = 0%
2014-03-24 14:51:45,255 Stage-7 map = 14%,  reduce = 0%
2014-03-24 14:51:47,290 Stage-7 map = 15%,  reduce = 0%
2014-03-24 14:51:49,391 Stage-7 map = 16%,  reduce = 0%
2014-03-24 14:51:51,414 Stage-7 map = 17%,  reduce = 0%
2014-03-24 14:51:53,479 Stage-7 map = 18%,  reduce = 0%
2014-03-24 14:51:55,511 Stage-7 map = 19%,  reduce = 0%
2014-03-24 14:51:57,596 Stage-7 map = 20%,  reduce = 0%
2014-03-24 14:51:59,670 Stage-7 map = 21%,  reduce = 0%
2014-03-24 14:52:01,698 Stage-7 map = 22%,  reduce = 0%
2014-03-24 14:52:03,820 Stage-7 map = 23%,  reduce = 0%
2014-03-24 14:52:05,008 Stage-7 map = 24%,  reduce = 0%
2014-03-24 14:52:07,072 Stage-7 map = 25%,  reduce = 0%
2014-03-24 14:52:09,106 Stage-7 map = 26%,  reduce = 0%
2014-03-24 14:52:11,127 Stage-7 map = 27%,  reduce = 0%
2014-03-24 14:52:13,151 Stage-7 map = 28%,  reduce = 0%
2014-03-24 14:52:15,181 Stage-7 map = 29%,  reduce = 0%
2014-03-24 14:52:16,191 Stage-7 map = 30%,  reduce = 0%
2014-03-24 14:52:18,228 Stage-7 map = 31%,  reduce = 0%
2014-03-24 14:52:20,280 Stage-7 map = 32%,  reduce = 0%
2014-03-24 14:53:12,837 Stage-7 map = 33%,  reduce = 0%
2014-03-24 14:53:15,938 Stage-7 map = 38%,  reduce = 0%
2014-03-24 14:53:17,006 Stage-7 map = 52%,  reduce = 0%
2014-03-24 14:53:18,023 Stage-7 map = 53%,  reduce = 0%
2014-03-24 14:53:24,193 Stage-7 map = 54%,  reduce = 0%
2014-03-24 14:53:26,221 Stage-7 map = 55%,  reduce = 0%
2014-03-24 14:53:27,232 Stage-7 map = 56%,  reduce = 0%
2014-03-24 14:53:29,252 Stage-7 map = 57%,  reduce = 0%
2014-03-24 14:53:30,263 Stage-7 map = 58%,  reduce = 0%
2014-03-24 14:53:31,281 Stage-7 map = 59%,  reduce = 0%
2014-03-24 14:53:32,294 Stage-7 map = 60%,  reduce = 0%
2014-03-24 14:53:34,016 Stage-7 map = 62%,  reduce = 0%
2014-03-24 14:53:35,037 Stage-7 map = 63%,  reduce = 0%
2014-03-24 14:53:36,048 Stage-7 map = 65%,  reduce = 0%
2014-03-24 14:53:37,060 Stage-7 map = 66%,  reduce = 0%
2014-03-24 14:53:38,419 Stage-7 map = 67%,  reduce = 0%
2014-03-24 14:53:39,650 Stage-7 map = 68%,  reduce = 0%
2014-03-24 14:53:40,731 Stage-7 map = 69%,  reduce = 0%
2014-03-24 14:53:41,879 Stage-7 map = 70%,  reduce = 0%
2014-03-24 14:53:42,892 Stage-7 map = 72%,  reduce = 0%
2014-03-24 14:53:43,905 Stage-7 map = 73%,  reduce = 0%
2014-03-24 14:53:44,918 Stage-7 map = 74%,  reduce = 0%
2014-03-24 14:53:45,937 Stage-7 map = 75%,  reduce = 0%
2014-03-24 14:53:46,950 Stage-7 map = 76%,  reduce = 0%
2014-03-24 14:53:47,962 Stage-7 map = 77%,  reduce = 0%
2014-03-24 14:53:48,975 Stage-7 map = 78%,  reduce = 0%
2014-03-24 14:53:49,986 Stage-7 map = 79%,  reduce = 0%
2014-03-24 14:53:52,293 Stage-7 map = 80%,  reduce = 0%
2014-03-24 14:53:53,322 Stage-7 map = 82%,  reduce = 0%
2014-03-24 14:53:55,388 Stage-7 map = 83%,  reduce = 0%
2014-03-24 14:53:56,424 Stage-7 map = 84%,  reduce = 0%
2014-03-24 14:53:57,474 Stage-7 map = 85%,  reduce = 0%
2014-03-24 14:53:59,519 Stage-7 map = 86%,  reduce = 0%
2014-03-24 14:54:01,568 Stage-7 map = 87%,  reduce = 0%
2014-03-24 14:54:03,654 Stage-7 map = 88%,  reduce = 0%
2014-03-24 14:54:05,718 Stage-7 map = 89%,  reduce = 0%
2014-03-24 14:54:09,486 Stage-7 map = 90%,  reduce = 0%
2014-03-24 14:54:13,450 Stage-7 map = 91%,  reduce = 0%
2014-03-24 14:54:14,481 Stage-7 map = 92%,  reduce = 0%
2014-03-24 14:54:17,728 Stage-7 map = 93%,  reduce = 0%
2014-03-24 14:54:22,098 Stage-7 map = 94%,  reduce = 0%
2014-03-24 14:54:27,511 Stage-7 map = 95%,  reduce = 0%
2014-03-24 14:54:36,556 Stage-7 map = 96%,  reduce = 0%
2014-03-24 14:54:42,031 Stage-7 map = 97%,  reduce = 0%
2014-03-24 14:54:46,666 Stage-7 map = 98%,  reduce = 0%
2014-03-24 14:54:51,332 Stage-7 map = 99%,  reduce = 0%
2014-03-24 14:54:57,895 Stage-7 map = 100%,  reduce = 0%
2014-03-24 14:55:03,046 Stage-7 map = 100%,  reduce = 2%
2014-03-24 14:55:04,058 Stage-7 map = 100%,  reduce = 4%
2014-03-24 14:55:05,092 Stage-7 map = 100%,  reduce = 6%
2014-03-24 14:55:06,104 Stage-7 map = 100%,  reduce = 7%
2014-03-24 14:55:07,178 Stage-7 map = 100%,  reduce = 11%
2014-03-24 14:55:08,189 Stage-7 map = 100%,  reduce = 13%
2014-03-24 14:55:09,212 Stage-7 map = 100%,  reduce = 14%
2014-03-24 14:55:10,225 Stage-7 map = 100%,  reduce = 19%
2014-03-24 14:55:11,252 Stage-7 map = 100%,  reduce = 22%
2014-03-24 14:55:12,267 Stage-7 map = 100%,  reduce = 25%
2014-03-24 14:55:13,308 Stage-7 map = 100%,  reduce = 26%
2014-03-24 14:55:14,359 Stage-7 map = 100%,  reduce = 27%
2014-03-24 14:55:22,947 Stage-7 map = 100%,  reduce = 29%
2014-03-24 14:55:26,453 Stage-7 map = 100%,  reduce = 30%
2014-03-24 14:55:27,567 Stage-7 map = 100%,  reduce = 31%
2014-03-24 14:55:28,624 Stage-7 map = 100%,  reduce = 32%
2014-03-24 14:55:37,172 Stage-7 map = 100%,  reduce = 34%
2014-03-24 14:55:39,617 Stage-7 map = 100%,  reduce = 49%
2014-03-24 14:55:40,763 Stage-7 map = 100%,  reduce = 78%
2014-03-24 14:55:41,800 Stage-7 map = 100%,  reduce = 82%
2014-03-24 14:55:44,543 Stage-7 map = 100%,  reduce = 85%
2014-03-24 14:55:45,778 Stage-7 map = 100%,  reduce = 96%
2014-03-24 14:55:50,301 Stage-7 map = 100%,  reduce = 97%
2014-03-24 14:55:55,104 Stage-7 map = 100%,  reduce = 98%
2014-03-24 14:55:57,190 Stage-7 map = 100%,  reduce = 100%
2014-03-24 14:56:57,858 Stage-7 map = 100%,  reduce = 100%
2014-03-24 14:58:14,577 Stage-7 map = 100%,  reduce = 100%
2014-03-24 14:59:16,140 Stage-7 map = 100%,  reduce = 100%
2014-03-24 15:00:16,676 Stage-7 map = 100%,  reduce = 100%
2014-03-24 15:01:28,257 Stage-7 map = 100%,  reduce = 100%
2014-03-24 15:02:36,240 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_831882
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832071, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832071
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832071
2014-03-24 15:04:48,778 Stage-2 map = 0%,  reduce = 0%
2014-03-24 15:05:48,822 Stage-2 map = 0%,  reduce = 0%
2014-03-24 15:07:52,344 Stage-2 map = 100%,  reduce = 0%
2014-03-24 15:09:46,952 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832071
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832126, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832126
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832126
2014-03-24 15:13:42,162 Stage-6 map = 0%,  reduce = 0%
2014-03-24 15:13:50,392 Stage-6 map = 5%,  reduce = 0%
2014-03-24 15:13:51,680 Stage-6 map = 16%,  reduce = 0%
2014-03-24 15:13:53,837 Stage-6 map = 63%,  reduce = 0%
2014-03-24 15:13:54,933 Stage-6 map = 74%,  reduce = 0%
2014-03-24 15:13:55,967 Stage-6 map = 79%,  reduce = 0%
2014-03-24 15:15:11,951 Stage-6 map = 79%,  reduce = 0%
2014-03-24 15:15:21,767 Stage-6 map = 100%,  reduce = 0%
2014-03-24 15:16:00,959 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832126
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832181, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832181
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832181
2014-03-24 15:16:44,779 Stage-8 map = 0%,  reduce = 0%
2014-03-24 15:16:50,409 Stage-8 map = 1%,  reduce = 0%
2014-03-24 15:17:02,792 Stage-8 map = 10%,  reduce = 0%
2014-03-24 15:17:06,008 Stage-8 map = 19%,  reduce = 0%
2014-03-24 15:17:13,338 Stage-8 map = 43%,  reduce = 0%
2014-03-24 15:17:16,193 Stage-8 map = 70%,  reduce = 0%
2014-03-24 15:17:17,931 Stage-8 map = 90%,  reduce = 0%
2014-03-24 15:17:19,214 Stage-8 map = 97%,  reduce = 0%
2014-03-24 15:17:20,504 Stage-8 map = 98%,  reduce = 0%
2014-03-24 15:17:28,718 Stage-8 map = 99%,  reduce = 0%
2014-03-24 15:17:32,243 Stage-8 map = 100%,  reduce = 32%
2014-03-24 15:17:33,983 Stage-8 map = 100%,  reduce = 33%
2014-03-24 15:17:35,293 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832181
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832252, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832252
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832252
2014-03-24 15:17:50,536 Stage-3 map = 0%,  reduce = 0%
2014-03-24 15:18:00,111 Stage-3 map = 67%,  reduce = 0%
2014-03-24 15:18:02,601 Stage-3 map = 100%,  reduce = 0%
2014-03-24 15:18:08,958 Stage-3 map = 100%,  reduce = 33%
2014-03-24 15:18:10,642 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832252
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0323
4 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-24_14-40-03_620_4222214682537056606/-ext-10000
OK
Time taken: 2287.482 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0323 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-23' and dd>='2014-03-22' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-23' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-23' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403241518_124957816.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832318, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832318
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832318
2014-03-24 15:19:33,642 Stage-1 map = 0%,  reduce = 0%
2014-03-24 15:19:38,046 Stage-1 map = 2%,  reduce = 0%
2014-03-24 15:20:48,310 Stage-1 map = 3%,  reduce = 0%
2014-03-24 15:20:53,346 Stage-1 map = 68%,  reduce = 0%
2014-03-24 15:20:54,641 Stage-1 map = 74%,  reduce = 0%
2014-03-24 15:20:55,815 Stage-1 map = 78%,  reduce = 0%
2014-03-24 15:20:57,400 Stage-1 map = 87%,  reduce = 0%
2014-03-24 15:20:58,684 Stage-1 map = 94%,  reduce = 0%
2014-03-24 15:21:06,192 Stage-1 map = 95%,  reduce = 0%
2014-03-24 15:21:08,384 Stage-1 map = 98%,  reduce = 0%
2014-03-24 15:21:11,777 Stage-1 map = 98%,  reduce = 33%
2014-03-24 15:22:13,630 Stage-1 map = 98%,  reduce = 33%
2014-03-24 15:22:46,277 Stage-1 map = 100%,  reduce = 33%
2014-03-24 15:22:53,801 Stage-1 map = 100%,  reduce = 100%
2014-03-24 15:24:23,244 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832318
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832407, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832407
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832407
2014-03-24 15:26:29,442 Stage-3 map = 0%,  reduce = 0%
2014-03-24 15:26:32,750 Stage-3 map = 1%,  reduce = 0%
2014-03-24 15:26:33,861 Stage-3 map = 5%,  reduce = 0%
2014-03-24 15:26:35,136 Stage-3 map = 12%,  reduce = 0%
2014-03-24 15:26:36,528 Stage-3 map = 30%,  reduce = 0%
2014-03-24 15:26:38,147 Stage-3 map = 53%,  reduce = 0%
2014-03-24 15:26:39,304 Stage-3 map = 65%,  reduce = 0%
2014-03-24 15:26:40,551 Stage-3 map = 70%,  reduce = 0%
2014-03-24 15:26:41,864 Stage-3 map = 83%,  reduce = 0%
2014-03-24 15:26:44,155 Stage-3 map = 90%,  reduce = 0%
2014-03-24 15:26:45,501 Stage-3 map = 95%,  reduce = 0%
2014-03-24 15:26:46,735 Stage-3 map = 98%,  reduce = 0%
2014-03-24 15:26:50,260 Stage-3 map = 99%,  reduce = 0%
2014-03-24 15:26:54,136 Stage-3 map = 100%,  reduce = 0%
2014-03-24 15:26:55,288 Stage-3 map = 100%,  reduce = 33%
2014-03-24 15:27:00,959 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832407
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832497, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832497
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832497
2014-03-24 15:27:09,273 Stage-4 map = 0%,  reduce = 0%
2014-03-24 15:27:11,833 Stage-4 map = 1%,  reduce = 0%
2014-03-24 15:27:14,477 Stage-4 map = 18%,  reduce = 0%
2014-03-24 15:27:17,187 Stage-4 map = 37%,  reduce = 0%
2014-03-24 15:27:18,833 Stage-4 map = 50%,  reduce = 0%
2014-03-24 15:27:20,585 Stage-4 map = 67%,  reduce = 0%
2014-03-24 15:27:21,703 Stage-4 map = 73%,  reduce = 0%
2014-03-24 15:27:22,919 Stage-4 map = 78%,  reduce = 0%
2014-03-24 15:27:24,156 Stage-4 map = 83%,  reduce = 0%
2014-03-24 15:27:25,301 Stage-4 map = 90%,  reduce = 0%
2014-03-24 15:27:26,491 Stage-4 map = 96%,  reduce = 0%
2014-03-24 15:27:27,577 Stage-4 map = 99%,  reduce = 0%
2014-03-24 15:27:29,024 Stage-4 map = 100%,  reduce = 0%
2014-03-24 15:27:34,645 Stage-4 map = 100%,  reduce = 33%
2014-03-24 15:27:39,717 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832497
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832532, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832532
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832532
2014-03-24 15:27:48,081 Stage-2 map = 0%,  reduce = 0%
2014-03-24 15:27:53,062 Stage-2 map = 100%,  reduce = 0%
2014-03-24 15:28:01,723 Stage-2 map = 100%,  reduce = 33%
2014-03-24 15:28:02,878 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832532
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0323
17 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-24_15-18-13_724_8133850521392380252/-ext-10000
OK
Time taken: 592.166 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0323 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0323 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403241528_2126185612.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832547, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832547
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832547
2014-03-24 15:28:24,203 Stage-4 map = 0%,  reduce = 0%
2014-03-24 15:28:36,364 Stage-4 map = 100%,  reduce = 0%
2014-03-24 15:28:42,623 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832547
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832580, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832580
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832580
2014-03-24 15:28:55,623 Stage-1 map = 0%,  reduce = 0%
2014-03-24 15:29:00,063 Stage-1 map = 67%,  reduce = 0%
2014-03-24 15:29:01,541 Stage-1 map = 100%,  reduce = 0%
2014-03-24 15:29:08,262 Stage-1 map = 100%,  reduce = 33%
2014-03-24 15:29:09,432 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832580
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832597, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832597
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832597
2014-03-24 15:29:15,934 Stage-2 map = 0%,  reduce = 0%
2014-03-24 15:29:28,533 Stage-2 map = 100%,  reduce = 0%
2014-03-24 15:29:44,889 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832597
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23
4 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23
OK
Time taken: 100.437 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23/* > /home/group_dataanalysis/yhd_Report/res-2014-03-23/output-1
14/03/24 15:29:49 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/24 15:29:49 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0323 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403241529_1099753808.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832622, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832622
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832622
2014-03-24 15:30:04,049 Stage-1 map = 0%,  reduce = 0%
2014-03-24 15:30:09,303 Stage-1 map = 100%,  reduce = 0%
2014-03-24 15:30:17,567 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832622
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832647, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832647
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832647
2014-03-24 15:30:24,483 Stage-2 map = 0%,  reduce = 0%
2014-03-24 15:30:28,860 Stage-2 map = 50%,  reduce = 0%
2014-03-24 15:31:09,938 Stage-2 map = 100%,  reduce = 0%
2014-03-24 15:31:22,644 Stage-2 map = 100%,  reduce = 17%
2014-03-24 15:31:24,322 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832647
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_832676, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_832676
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_832676
2014-03-24 15:31:59,435 Stage-3 map = 0%,  reduce = 0%
2014-03-24 15:32:10,905 Stage-3 map = 100%,  reduce = 0%
2014-03-24 15:33:02,671 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_832676
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23
17 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23
OK
Time taken: 193.293 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-23/* > /home/group_dataanalysis/yhd_Report/res-2014-03-23/output-2
14/03/24 15:33:05 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/24 15:33:05 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395646385.51
total time is :  0.0521612167358 seconds
size is       :  6133 KB
speed is      :  0.117577778737 MB/s
140323MG58PR 20
140323FG3X55 20
140323MG7CL6 38
140323BG6L7Y 20
140323YG7KNU 20
140323QG4T1Q 20
140323MG7BAE 34
140323RG5F7Q 20
140323EG7BTR 20
140323XG2G0E 3
140323CG7GU6 38
140323CG6VP2 38
140323JG4LH2 3
140323FG89K3 38
140323MG7C47 38
140323GG8BP0 3
140323HG8FBM 3
#######################SAVE DATE TO FILE#########################
#######################MAIL FILE#########################
<logging.Logger instance at 0x2ba364c00fc8>
{'loglevel': 'debug', 'debug': False, 'logfile': None}
['res-2014-03-23', '0323', '2014-03-23']
res-2014-03-23/yhd_dsp_report_0323.xls
mail_exchange -s '[2014-03-23]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-23/yhd_dsp_report_0323.xls' -u 'wentao_wang' -p '2238681Xwww' 
Main       2014-03-24 15:33:06,812 INFO  executing: mail_exchange -s '[2014-03-23]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-23/yhd_dsp_report_0323.xls' -u 'wentao_wang' -p '2238681Xwww' 
20140324_153308
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0324 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-24' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-24' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-24' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403251440_12865684.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_855652, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_855652
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_855652
2014-03-25 14:40:23,313 Stage-1 map = 0%,  reduce = 0%
2014-03-25 14:40:27,382 Stage-1 map = 6%,  reduce = 0%
2014-03-25 14:40:28,396 Stage-1 map = 12%,  reduce = 0%
2014-03-25 14:40:29,417 Stage-1 map = 22%,  reduce = 0%
2014-03-25 14:40:30,443 Stage-1 map = 31%,  reduce = 0%
2014-03-25 14:40:31,464 Stage-1 map = 41%,  reduce = 0%
2014-03-25 14:40:32,475 Stage-1 map = 50%,  reduce = 0%
2014-03-25 14:40:33,489 Stage-1 map = 57%,  reduce = 0%
2014-03-25 14:40:34,499 Stage-1 map = 65%,  reduce = 0%
2014-03-25 14:40:35,512 Stage-1 map = 69%,  reduce = 0%
2014-03-25 14:40:36,533 Stage-1 map = 71%,  reduce = 0%
2014-03-25 14:40:37,547 Stage-1 map = 73%,  reduce = 0%
2014-03-25 14:40:38,563 Stage-1 map = 74%,  reduce = 0%
2014-03-25 14:40:39,582 Stage-1 map = 75%,  reduce = 0%
2014-03-25 14:40:41,602 Stage-1 map = 76%,  reduce = 0%
2014-03-25 14:40:42,613 Stage-1 map = 77%,  reduce = 0%
2014-03-25 14:40:44,639 Stage-1 map = 78%,  reduce = 0%
2014-03-25 14:40:46,668 Stage-1 map = 79%,  reduce = 0%
2014-03-25 14:40:51,735 Stage-1 map = 84%,  reduce = 0%
2014-03-25 14:40:54,778 Stage-1 map = 88%,  reduce = 0%
2014-03-25 14:41:01,870 Stage-1 map = 89%,  reduce = 0%
2014-03-25 14:41:06,919 Stage-1 map = 90%,  reduce = 0%
2014-03-25 14:41:17,162 Stage-1 map = 91%,  reduce = 0%
2014-03-25 14:41:18,170 Stage-1 map = 92%,  reduce = 0%
2014-03-25 14:41:36,869 Stage-1 map = 93%,  reduce = 0%
2014-03-25 14:41:37,949 Stage-1 map = 94%,  reduce = 0%
2014-03-25 14:41:45,469 Stage-1 map = 95%,  reduce = 0%
2014-03-25 14:41:58,716 Stage-1 map = 96%,  reduce = 0%
2014-03-25 14:42:00,734 Stage-1 map = 97%,  reduce = 0%
2014-03-25 14:42:05,796 Stage-1 map = 98%,  reduce = 30%
2014-03-25 14:42:09,849 Stage-1 map = 98%,  reduce = 33%
2014-03-25 14:42:11,870 Stage-1 map = 99%,  reduce = 33%
2014-03-25 14:42:18,023 Stage-1 map = 100%,  reduce = 33%
2014-03-25 14:43:14,739 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_855652
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 14
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_855688, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_855688
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_855688
2014-03-25 14:44:34,970 Stage-5 map = 0%,  reduce = 0%
2014-03-25 14:44:43,092 Stage-5 map = 2%,  reduce = 0%
2014-03-25 14:44:44,105 Stage-5 map = 6%,  reduce = 0%
2014-03-25 14:44:45,147 Stage-5 map = 13%,  reduce = 0%
2014-03-25 14:44:46,166 Stage-5 map = 26%,  reduce = 0%
2014-03-25 14:44:51,538 Stage-5 map = 38%,  reduce = 0%
2014-03-25 14:45:56,533 Stage-5 map = 60%,  reduce = 0%
2014-03-25 14:45:58,214 Stage-5 map = 92%,  reduce = 0%
2014-03-25 14:46:10,511 Stage-5 map = 93%,  reduce = 0%
2014-03-25 14:46:12,535 Stage-5 map = 94%,  reduce = 0%
2014-03-25 14:46:14,551 Stage-5 map = 95%,  reduce = 0%
2014-03-25 14:46:17,644 Stage-5 map = 96%,  reduce = 0%
2014-03-25 14:46:18,661 Stage-5 map = 97%,  reduce = 0%
2014-03-25 14:46:20,802 Stage-5 map = 98%,  reduce = 0%
2014-03-25 14:46:22,839 Stage-5 map = 99%,  reduce = 0%
2014-03-25 14:46:28,136 Stage-5 map = 100%,  reduce = 0%
2014-03-25 14:46:30,182 Stage-5 map = 100%,  reduce = 4%
2014-03-25 14:46:31,212 Stage-5 map = 100%,  reduce = 24%
2014-03-25 14:46:32,253 Stage-5 map = 100%,  reduce = 29%
2014-03-25 14:46:33,276 Stage-5 map = 100%,  reduce = 32%
2014-03-25 14:46:34,289 Stage-5 map = 100%,  reduce = 33%
2014-03-25 14:47:59,420 Stage-5 map = 100%,  reduce = 33%
2014-03-25 14:49:27,050 Stage-5 map = 100%,  reduce = 33%
2014-03-25 14:49:31,291 Stage-5 map = 100%,  reduce = 38%
2014-03-25 14:49:33,315 Stage-5 map = 100%,  reduce = 47%
2014-03-25 14:49:34,327 Stage-5 map = 100%,  reduce = 60%
2014-03-25 14:49:35,361 Stage-5 map = 100%,  reduce = 65%
2014-03-25 14:49:36,675 Stage-5 map = 100%,  reduce = 82%
2014-03-25 14:49:37,692 Stage-5 map = 100%,  reduce = 96%
2014-03-25 14:49:39,830 Stage-5 map = 100%,  reduce = 99%
2014-03-25 14:49:42,870 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_855688
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 490
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_855736, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_855736
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_855736
2014-03-25 14:50:06,869 Stage-7 map = 0%,  reduce = 0%
2014-03-25 14:50:17,067 Stage-7 map = 1%,  reduce = 0%
2014-03-25 14:50:22,095 Stage-7 map = 2%,  reduce = 0%
2014-03-25 14:50:35,478 Stage-7 map = 3%,  reduce = 0%
2014-03-25 14:50:46,002 Stage-7 map = 4%,  reduce = 0%
2014-03-25 14:50:59,682 Stage-7 map = 5%,  reduce = 0%
2014-03-25 14:52:12,861 Stage-7 map = 5%,  reduce = 0%
2014-03-25 14:54:22,594 Stage-7 map = 40%,  reduce = 0%
2014-03-25 14:54:25,023 Stage-7 map = 56%,  reduce = 0%
2014-03-25 14:54:26,195 Stage-7 map = 63%,  reduce = 0%
2014-03-25 14:54:32,482 Stage-7 map = 64%,  reduce = 0%
2014-03-25 14:54:33,503 Stage-7 map = 65%,  reduce = 0%
2014-03-25 14:54:35,567 Stage-7 map = 66%,  reduce = 0%
2014-03-25 14:54:36,580 Stage-7 map = 67%,  reduce = 0%
2014-03-25 14:54:37,591 Stage-7 map = 68%,  reduce = 0%
2014-03-25 14:54:38,618 Stage-7 map = 69%,  reduce = 0%
2014-03-25 14:54:39,658 Stage-7 map = 70%,  reduce = 0%
2014-03-25 14:54:40,670 Stage-7 map = 71%,  reduce = 0%
2014-03-25 14:54:41,687 Stage-7 map = 72%,  reduce = 0%
2014-03-25 14:54:42,698 Stage-7 map = 74%,  reduce = 0%
2014-03-25 14:54:43,722 Stage-7 map = 75%,  reduce = 0%
2014-03-25 14:54:44,789 Stage-7 map = 77%,  reduce = 0%
2014-03-25 14:54:45,810 Stage-7 map = 78%,  reduce = 0%
2014-03-25 14:54:46,868 Stage-7 map = 79%,  reduce = 0%
2014-03-25 14:54:47,878 Stage-7 map = 80%,  reduce = 0%
2014-03-25 14:54:48,894 Stage-7 map = 82%,  reduce = 0%
2014-03-25 14:54:50,921 Stage-7 map = 84%,  reduce = 0%
2014-03-25 14:54:51,945 Stage-7 map = 85%,  reduce = 0%
2014-03-25 14:54:52,980 Stage-7 map = 86%,  reduce = 0%
2014-03-25 14:54:54,089 Stage-7 map = 87%,  reduce = 0%
2014-03-25 14:54:55,100 Stage-7 map = 88%,  reduce = 0%
2014-03-25 14:54:56,111 Stage-7 map = 89%,  reduce = 0%
2014-03-25 14:54:57,122 Stage-7 map = 90%,  reduce = 0%
2014-03-25 14:54:58,146 Stage-7 map = 91%,  reduce = 0%
2014-03-25 14:55:00,209 Stage-7 map = 92%,  reduce = 0%
2014-03-25 14:55:01,231 Stage-7 map = 93%,  reduce = 0%
2014-03-25 14:55:03,522 Stage-7 map = 94%,  reduce = 0%
2014-03-25 14:55:05,549 Stage-7 map = 95%,  reduce = 0%
2014-03-25 14:55:07,580 Stage-7 map = 96%,  reduce = 0%
2014-03-25 14:55:13,783 Stage-7 map = 97%,  reduce = 0%
2014-03-25 14:56:17,369 Stage-7 map = 97%,  reduce = 0%
2014-03-25 14:57:24,381 Stage-7 map = 100%,  reduce = 0%
2014-03-25 14:57:28,922 Stage-7 map = 100%,  reduce = 1%
2014-03-25 14:57:32,075 Stage-7 map = 100%,  reduce = 2%
2014-03-25 14:57:33,092 Stage-7 map = 100%,  reduce = 3%
2014-03-25 14:57:34,105 Stage-7 map = 100%,  reduce = 4%
2014-03-25 14:57:35,126 Stage-7 map = 100%,  reduce = 5%
2014-03-25 14:57:36,142 Stage-7 map = 100%,  reduce = 6%
2014-03-25 14:57:37,174 Stage-7 map = 100%,  reduce = 9%
2014-03-25 14:57:38,206 Stage-7 map = 100%,  reduce = 10%
2014-03-25 14:57:39,223 Stage-7 map = 100%,  reduce = 11%
2014-03-25 14:58:11,386 Stage-7 map = 100%,  reduce = 16%
2014-03-25 14:58:12,485 Stage-7 map = 100%,  reduce = 18%
2014-03-25 14:58:14,520 Stage-7 map = 100%,  reduce = 19%
2014-03-25 14:58:16,567 Stage-7 map = 100%,  reduce = 20%
2014-03-25 14:58:17,588 Stage-7 map = 100%,  reduce = 21%
2014-03-25 14:58:19,636 Stage-7 map = 100%,  reduce = 22%
2014-03-25 14:58:20,660 Stage-7 map = 100%,  reduce = 23%
2014-03-25 14:58:21,677 Stage-7 map = 100%,  reduce = 25%
2014-03-25 14:58:22,694 Stage-7 map = 100%,  reduce = 26%
2014-03-25 14:58:23,719 Stage-7 map = 100%,  reduce = 28%
2014-03-25 14:58:25,746 Stage-7 map = 100%,  reduce = 29%
2014-03-25 14:58:26,760 Stage-7 map = 100%,  reduce = 30%
2014-03-25 14:58:28,804 Stage-7 map = 100%,  reduce = 31%
2014-03-25 14:58:29,831 Stage-7 map = 100%,  reduce = 32%
2014-03-25 14:58:30,877 Stage-7 map = 100%,  reduce = 33%
2014-03-25 14:58:31,895 Stage-7 map = 100%,  reduce = 34%
2014-03-25 14:58:32,914 Stage-7 map = 100%,  reduce = 35%
2014-03-25 14:58:38,006 Stage-7 map = 100%,  reduce = 36%
2014-03-25 14:58:39,020 Stage-7 map = 100%,  reduce = 37%
2014-03-25 14:58:40,042 Stage-7 map = 100%,  reduce = 38%
2014-03-25 14:58:41,058 Stage-7 map = 100%,  reduce = 39%
2014-03-25 14:58:42,073 Stage-7 map = 100%,  reduce = 40%
2014-03-25 14:58:45,161 Stage-7 map = 100%,  reduce = 41%
2014-03-25 14:59:51,055 Stage-7 map = 100%,  reduce = 42%
2014-03-25 14:59:54,901 Stage-7 map = 100%,  reduce = 56%
2014-03-25 14:59:56,003 Stage-7 map = 100%,  reduce = 65%
2014-03-25 15:00:01,114 Stage-7 map = 100%,  reduce = 66%
2014-03-25 15:00:09,417 Stage-7 map = 100%,  reduce = 68%
2014-03-25 15:00:10,585 Stage-7 map = 100%,  reduce = 71%
2014-03-25 15:00:13,699 Stage-7 map = 100%,  reduce = 72%
2014-03-25 15:00:14,748 Stage-7 map = 100%,  reduce = 73%
2014-03-25 15:00:15,789 Stage-7 map = 100%,  reduce = 74%
2014-03-25 15:00:16,824 Stage-7 map = 100%,  reduce = 75%
2014-03-25 15:00:18,927 Stage-7 map = 100%,  reduce = 76%
2014-03-25 15:00:20,044 Stage-7 map = 100%,  reduce = 77%
2014-03-25 15:00:22,106 Stage-7 map = 100%,  reduce = 78%
2014-03-25 15:01:06,482 Stage-7 map = 100%,  reduce = 79%
2014-03-25 15:01:09,551 Stage-7 map = 100%,  reduce = 80%
2014-03-25 15:01:16,798 Stage-7 map = 100%,  reduce = 81%
2014-03-25 15:01:53,670 Stage-7 map = 100%,  reduce = 98%
2014-03-25 15:01:54,767 Stage-7 map = 100%,  reduce = 100%
2014-03-25 15:02:55,150 Stage-7 map = 100%,  reduce = 100%
2014-03-25 15:05:29,853 Stage-7 map = 100%,  reduce = 100%
2014-03-25 15:07:44,584 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_855736
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_855898, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_855898
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_855898
2014-03-25 15:08:15,387 Stage-6 map = 0%,  reduce = 0%
2014-03-25 15:08:16,923 Stage-6 map = 14%,  reduce = 0%
2014-03-25 15:08:18,488 Stage-6 map = 29%,  reduce = 0%
2014-03-25 15:08:19,976 Stage-6 map = 57%,  reduce = 0%
2014-03-25 15:08:21,094 Stage-6 map = 86%,  reduce = 0%
2014-03-25 15:08:25,719 Stage-6 map = 93%,  reduce = 0%
2014-03-25 15:08:27,768 Stage-6 map = 100%,  reduce = 0%
2014-03-25 15:08:35,945 Stage-6 map = 100%,  reduce = 31%
2014-03-25 15:08:38,001 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_855898
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_855937, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_855937
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_855937
2014-03-25 15:08:43,736 Stage-2 map = 0%,  reduce = 0%
2014-03-25 15:08:51,807 Stage-2 map = 100%,  reduce = 0%
2014-03-25 15:08:59,943 Stage-2 map = 100%,  reduce = 33%
2014-03-25 15:09:00,966 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_855937
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_855951, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_855951
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_855951
2014-03-25 15:10:37,216 Stage-8 map = 0%,  reduce = 0%
2014-03-25 15:10:41,811 Stage-8 map = 1%,  reduce = 0%
2014-03-25 15:10:43,241 Stage-8 map = 7%,  reduce = 0%
2014-03-25 15:10:44,294 Stage-8 map = 26%,  reduce = 0%
2014-03-25 15:10:45,321 Stage-8 map = 32%,  reduce = 0%
2014-03-25 15:10:46,674 Stage-8 map = 40%,  reduce = 0%
2014-03-25 15:10:47,918 Stage-8 map = 48%,  reduce = 0%
2014-03-25 15:11:57,296 Stage-8 map = 67%,  reduce = 0%
2014-03-25 15:12:00,369 Stage-8 map = 82%,  reduce = 0%
2014-03-25 15:12:01,694 Stage-8 map = 94%,  reduce = 0%
2014-03-25 15:13:13,882 Stage-8 map = 96%,  reduce = 0%
2014-03-25 15:13:16,570 Stage-8 map = 99%,  reduce = 0%
2014-03-25 15:14:42,701 Stage-8 map = 100%,  reduce = 0%
2014-03-25 15:14:46,383 Stage-8 map = 100%,  reduce = 33%
2014-03-25 15:14:54,051 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_855951
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856040, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856040
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856040
2014-03-25 15:15:01,200 Stage-3 map = 0%,  reduce = 0%
2014-03-25 15:15:08,454 Stage-3 map = 33%,  reduce = 0%
2014-03-25 15:15:10,467 Stage-3 map = 67%,  reduce = 0%
2014-03-25 15:16:14,427 Stage-3 map = 67%,  reduce = 0%
2014-03-25 15:16:56,542 Stage-3 map = 100%,  reduce = 0%
2014-03-25 15:17:13,903 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856040
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0324
12 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-25_14-40-03_116_7630311194419151073/-ext-10000
OK
Time taken: 2232.197 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0324 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-24' and dd>='2014-03-23' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-24' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-24' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403251517_292758810.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856111, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856111
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856111
2014-03-25 15:17:32,943 Stage-1 map = 0%,  reduce = 0%
2014-03-25 15:17:36,966 Stage-1 map = 3%,  reduce = 0%
2014-03-25 15:17:43,058 Stage-1 map = 11%,  reduce = 0%
2014-03-25 15:17:45,880 Stage-1 map = 68%,  reduce = 0%
2014-03-25 15:17:47,567 Stage-1 map = 79%,  reduce = 0%
2014-03-25 15:17:48,993 Stage-1 map = 82%,  reduce = 0%
2014-03-25 15:17:51,167 Stage-1 map = 88%,  reduce = 0%
2014-03-25 15:17:52,787 Stage-1 map = 95%,  reduce = 0%
2014-03-25 15:18:15,484 Stage-1 map = 95%,  reduce = 32%
2014-03-25 15:18:16,574 Stage-1 map = 99%,  reduce = 32%
2014-03-25 15:19:02,075 Stage-1 map = 100%,  reduce = 33%
2014-03-25 15:19:30,819 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856111
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856244, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856244
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856244
2014-03-25 15:20:46,904 Stage-3 map = 0%,  reduce = 0%
2014-03-25 15:23:37,838 Stage-3 map = 0%,  reduce = 0%
2014-03-25 15:23:40,097 Stage-3 map = 12%,  reduce = 0%
2014-03-25 15:23:42,127 Stage-3 map = 13%,  reduce = 0%
2014-03-25 15:23:43,151 Stage-3 map = 15%,  reduce = 0%
2014-03-25 15:24:22,436 Stage-3 map = 18%,  reduce = 0%
2014-03-25 15:24:55,908 Stage-3 map = 48%,  reduce = 0%
2014-03-25 15:24:58,083 Stage-3 map = 74%,  reduce = 0%
2014-03-25 15:24:59,872 Stage-3 map = 82%,  reduce = 0%
2014-03-25 15:25:01,197 Stage-3 map = 88%,  reduce = 0%
2014-03-25 15:25:03,680 Stage-3 map = 91%,  reduce = 0%
2014-03-25 15:25:07,114 Stage-3 map = 93%,  reduce = 0%
2014-03-25 15:25:08,357 Stage-3 map = 94%,  reduce = 0%
2014-03-25 15:25:09,505 Stage-3 map = 97%,  reduce = 0%
2014-03-25 15:25:14,959 Stage-3 map = 98%,  reduce = 0%
2014-03-25 15:25:16,945 Stage-3 map = 99%,  reduce = 0%
2014-03-25 15:25:29,795 Stage-3 map = 99%,  reduce = 33%
2014-03-25 15:25:31,334 Stage-3 map = 100%,  reduce = 33%
2014-03-25 15:26:26,927 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856244
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856352, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856352
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856352
2014-03-25 15:26:46,770 Stage-4 map = 0%,  reduce = 0%
2014-03-25 15:26:51,137 Stage-4 map = 7%,  reduce = 0%
2014-03-25 15:26:52,497 Stage-4 map = 14%,  reduce = 0%
2014-03-25 15:26:53,798 Stage-4 map = 19%,  reduce = 0%
2014-03-25 15:26:55,385 Stage-4 map = 32%,  reduce = 0%
2014-03-25 15:26:56,520 Stage-4 map = 41%,  reduce = 0%
2014-03-25 15:26:57,541 Stage-4 map = 43%,  reduce = 0%
2014-03-25 15:26:58,632 Stage-4 map = 48%,  reduce = 0%
2014-03-25 15:26:59,665 Stage-4 map = 53%,  reduce = 0%
2014-03-25 15:27:00,710 Stage-4 map = 59%,  reduce = 0%
2014-03-25 15:27:01,914 Stage-4 map = 67%,  reduce = 0%
2014-03-25 15:27:04,117 Stage-4 map = 77%,  reduce = 0%
2014-03-25 15:27:05,722 Stage-4 map = 91%,  reduce = 0%
2014-03-25 15:27:06,784 Stage-4 map = 98%,  reduce = 0%
2014-03-25 15:27:08,857 Stage-4 map = 99%,  reduce = 0%
2014-03-25 15:27:10,936 Stage-4 map = 100%,  reduce = 0%
2014-03-25 15:27:16,146 Stage-4 map = 100%,  reduce = 26%
2014-03-25 15:27:18,663 Stage-4 map = 100%,  reduce = 33%
2014-03-25 15:27:39,258 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856352
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856421, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856421
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856421
2014-03-25 15:27:45,900 Stage-2 map = 0%,  reduce = 0%
2014-03-25 15:28:00,270 Stage-2 map = 100%,  reduce = 0%
2014-03-25 15:28:21,651 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856421
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0324
33 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-25_15-17-17_145_2944876516335104251/-ext-10000
OK
Time taken: 665.817 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0324 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0324 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403251528_1577326127.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856451, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856451
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856451
2014-03-25 15:28:36,154 Stage-4 map = 0%,  reduce = 0%
2014-03-25 15:28:40,699 Stage-4 map = 100%,  reduce = 0%
2014-03-25 15:28:52,251 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856451
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856483, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856483
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856483
2014-03-25 15:28:58,357 Stage-1 map = 0%,  reduce = 0%
2014-03-25 15:29:05,157 Stage-1 map = 33%,  reduce = 0%
2014-03-25 15:29:10,217 Stage-1 map = 67%,  reduce = 0%
2014-03-25 15:29:13,456 Stage-1 map = 100%,  reduce = 0%
2014-03-25 15:29:20,709 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856483
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856507, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856507
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856507
2014-03-25 15:29:29,423 Stage-2 map = 0%,  reduce = 0%
2014-03-25 15:29:52,551 Stage-2 map = 100%,  reduce = 0%
2014-03-25 15:30:01,044 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856507
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24
12 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24
OK
Time taken: 98.602 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24/* > /home/group_dataanalysis/yhd_Report/res-2014-03-24/output-1
14/03/25 15:30:04 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/25 15:30:04 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0324 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403251530_1787425167.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856555, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856555
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856555
2014-03-25 15:30:14,895 Stage-1 map = 0%,  reduce = 0%
2014-03-25 15:30:18,503 Stage-1 map = 100%,  reduce = 0%
2014-03-25 15:30:25,910 Stage-1 map = 100%,  reduce = 33%
2014-03-25 15:31:10,276 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856555
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856575, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856575
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856575
2014-03-25 15:31:52,084 Stage-2 map = 0%,  reduce = 0%
2014-03-25 15:31:56,280 Stage-2 map = 50%,  reduce = 0%
2014-03-25 15:31:57,332 Stage-2 map = 100%,  reduce = 0%
2014-03-25 15:32:12,895 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856575
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_856591, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_856591
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_856591
2014-03-25 15:32:18,392 Stage-3 map = 0%,  reduce = 0%
2014-03-25 15:32:31,698 Stage-3 map = 50%,  reduce = 0%
2014-03-25 15:32:51,229 Stage-3 map = 100%,  reduce = 0%
2014-03-25 15:33:00,177 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_856591
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24
33 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24
OK
Time taken: 181.372 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-24/* > /home/group_dataanalysis/yhd_Report/res-2014-03-24/output-2
14/03/25 15:33:13 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/25 15:33:13 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395732794.24
total time is :  0.101182937622 seconds
size is       :  11680 KB
speed is      :  0.115434482083 MB/s
140324FGMTHW 20
140324NGBGTJ 20
140324PGFB7Q 20
140324QGC7CJ 34
140324KGC4MR 20
140324QGBHJ7 4
140324CGC0HV 20
140324QGBX2A 34
140324QGCBYS 20
140324YGGKMR 20
140324HGCCMF 20
140324PGF44R 20
140324UGK1VU 20
140324RGEPPE 20
140324PGBCFE 24
140324BGFG5Q 20
140324HGEC1Q 20
140324SGCNYR 20
140324XGBF3K 24
140324UGK44V 20
140324YGHAH3 3
140324UGK4L7 20
140324SGEM3F 34
140324CGH123 20
140324LGC5RU 24
140324UGCAGX 20
140324SGBTS2 20
140324MGH16Y 3
140324UGG96F 20
140324JGKF25 38
140324MGF4CB 20
140324LGGCYU 20
140324AGK0P8 38
#######################SAVE DATE TO FILE#########################
#######################MAIL FILE#########################
<logging.Logger instance at 0x2b1d83c78fc8>
{'loglevel': 'debug', 'debug': False, 'logfile': None}
['res-2014-03-24', '0324', '2014-03-24']
res-2014-03-24/yhd_dsp_report_0324.xls
mail_exchange -s '[2014-03-24]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-24/yhd_dsp_report_0324.xls' -u 'wentao_wang' -p '2238681Xwww' 
Main       2014-03-25 15:33:15,796 INFO  executing: mail_exchange -s '[2014-03-24]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-24/yhd_dsp_report_0324.xls' -u 'wentao_wang' -p '2238681Xwww' 
20140325_153318
hive -e "
create table if not exists user_wangwentao_yhd_bidt_0325 as
select a.sid,
if(a.bidn is null,0,a.bidn) bidn,
if(b.winn is null,0,b.winn) winn,
if(b.ws is null,0,b.ws) ws,
if(c.clk is null,0,c.clk) clk,
if(a.bidprice is null,0,a.bidprice) bidprice,
if(a.bidfloor is null,0,a.bidfloor) bidfloor
from 

(select sid,count(*) bidn,sum(bidprice)/pow(10,6) bidprice,sum(bidfloor)/pow(10,6) bidfloor from
(select 
vsolutionid_ sid,
ifcbids_.pricebidfloor_ bidfloor,
ifcbids_.pricebid_   bidprice
from  pb_ifc_bidlog where dd='2014-03-25' and ifcbids_.pricebid_>0) rawbidt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution1 on rawbidt.sid=bsolution1.id  group by sid)a left outer join
(select sid,count(*) winn, sum(winprice)/pow(10,9)  ws
from
(select sawlog_.rawlog_.solutionid_ sid,sawlog_.rawlog_.sellingprice_ winprice
from pb_showlog where dd='2014-03-25' and product='ifc' and db='ifc') showt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on showt.sid=bsolution2.id group by sid) b on a.sid=b.sid left outer join
(select sid,count(*) clk
from
(select sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd='2014-03-25' and product='ifc' and db='ifc') clickt join
(select id from b_solution where advertiser_id=49 and campaign_id=132) bsolution2 on clickt.sid=bsolution2.id group by sid)c on a.sid=c.sid
;
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403261440_2143967225.txt
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_878861, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_878861
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_878861
2014-03-26 14:42:14,633 Stage-1 map = 0%,  reduce = 0%
2014-03-26 14:43:23,947 Stage-1 map = 2%,  reduce = 0%
2014-03-26 14:43:28,418 Stage-1 map = 28%,  reduce = 0%
2014-03-26 14:43:29,494 Stage-1 map = 49%,  reduce = 0%
2014-03-26 14:44:23,310 Stage-1 map = 62%,  reduce = 0%
2014-03-26 14:44:27,162 Stage-1 map = 85%,  reduce = 0%
2014-03-26 14:44:28,702 Stage-1 map = 98%,  reduce = 0%
2014-03-26 14:44:29,721 Stage-1 map = 99%,  reduce = 0%
2014-03-26 14:44:31,874 Stage-1 map = 100%,  reduce = 0%
2014-03-26 14:44:34,931 Stage-1 map = 100%,  reduce = 33%
2014-03-26 14:44:43,174 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_878861
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 15
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_878894, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_878894
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_878894
2014-03-26 14:46:34,112 Stage-5 map = 0%,  reduce = 0%
2014-03-26 14:46:43,964 Stage-5 map = 4%,  reduce = 0%
2014-03-26 14:46:44,979 Stage-5 map = 8%,  reduce = 0%
2014-03-26 14:46:46,007 Stage-5 map = 22%,  reduce = 0%
2014-03-26 14:46:47,021 Stage-5 map = 38%,  reduce = 0%
2014-03-26 14:46:48,033 Stage-5 map = 48%,  reduce = 0%
2014-03-26 14:46:49,063 Stage-5 map = 56%,  reduce = 0%
2014-03-26 14:46:50,124 Stage-5 map = 64%,  reduce = 0%
2014-03-26 14:47:58,114 Stage-5 map = 69%,  reduce = 0%
2014-03-26 14:48:00,372 Stage-5 map = 82%,  reduce = 0%
2014-03-26 14:48:01,409 Stage-5 map = 86%,  reduce = 0%
2014-03-26 14:48:05,480 Stage-5 map = 87%,  reduce = 0%
2014-03-26 14:48:08,660 Stage-5 map = 88%,  reduce = 0%
2014-03-26 14:48:11,720 Stage-5 map = 89%,  reduce = 0%
2014-03-26 14:48:12,896 Stage-5 map = 90%,  reduce = 0%
2014-03-26 14:48:14,928 Stage-5 map = 91%,  reduce = 0%
2014-03-26 14:48:16,973 Stage-5 map = 92%,  reduce = 0%
2014-03-26 14:48:19,022 Stage-5 map = 93%,  reduce = 0%
2014-03-26 14:48:22,238 Stage-5 map = 94%,  reduce = 0%
2014-03-26 14:48:24,268 Stage-5 map = 95%,  reduce = 0%
2014-03-26 14:48:31,627 Stage-5 map = 96%,  reduce = 0%
2014-03-26 14:48:36,730 Stage-5 map = 96%,  reduce = 22%
2014-03-26 14:48:38,771 Stage-5 map = 96%,  reduce = 23%
2014-03-26 14:48:43,925 Stage-5 map = 96%,  reduce = 27%
2014-03-26 14:48:44,953 Stage-5 map = 96%,  reduce = 30%
2014-03-26 14:48:45,976 Stage-5 map = 96%,  reduce = 32%
2014-03-26 14:48:59,532 Stage-5 map = 97%,  reduce = 32%
2014-03-26 14:50:23,656 Stage-5 map = 97%,  reduce = 32%
2014-03-26 14:50:26,450 Stage-5 map = 99%,  reduce = 32%
2014-03-26 14:50:30,658 Stage-5 map = 99%,  reduce = 33%
2014-03-26 14:50:39,758 Stage-5 map = 100%,  reduce = 33%
2014-03-26 14:50:59,076 Stage-5 map = 100%,  reduce = 38%
2014-03-26 14:51:01,107 Stage-5 map = 100%,  reduce = 42%
2014-03-26 14:51:02,124 Stage-5 map = 100%,  reduce = 51%
2014-03-26 14:51:03,134 Stage-5 map = 100%,  reduce = 82%
2014-03-26 14:51:04,141 Stage-5 map = 100%,  reduce = 89%
2014-03-26 14:51:05,157 Stage-5 map = 100%,  reduce = 91%
2014-03-26 14:51:06,168 Stage-5 map = 100%,  reduce = 94%
2014-03-26 14:51:07,176 Stage-5 map = 100%,  reduce = 97%
2014-03-26 14:51:09,323 Stage-5 map = 100%,  reduce = 99%
2014-03-26 14:51:13,688 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201402191020_878894
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 327
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_878947, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_878947
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_878947
2014-03-26 14:52:51,449 Stage-7 map = 0%,  reduce = 0%
2014-03-26 14:53:17,306 Stage-7 map = 3%,  reduce = 0%
2014-03-26 14:53:28,847 Stage-7 map = 4%,  reduce = 0%
2014-03-26 14:54:31,937 Stage-7 map = 6%,  reduce = 0%
2014-03-26 14:54:38,099 Stage-7 map = 7%,  reduce = 0%
2014-03-26 14:54:53,767 Stage-7 map = 8%,  reduce = 0%
2014-03-26 14:54:55,845 Stage-7 map = 9%,  reduce = 0%
2014-03-26 14:55:01,027 Stage-7 map = 10%,  reduce = 0%
2014-03-26 14:55:06,423 Stage-7 map = 11%,  reduce = 0%
2014-03-26 14:55:10,553 Stage-7 map = 12%,  reduce = 0%
2014-03-26 14:55:14,642 Stage-7 map = 13%,  reduce = 0%
2014-03-26 14:55:17,679 Stage-7 map = 14%,  reduce = 0%
2014-03-26 14:55:21,733 Stage-7 map = 15%,  reduce = 0%
2014-03-26 14:55:24,844 Stage-7 map = 16%,  reduce = 0%
2014-03-26 14:55:28,872 Stage-7 map = 17%,  reduce = 0%
2014-03-26 14:55:33,248 Stage-7 map = 18%,  reduce = 0%
2014-03-26 14:55:37,420 Stage-7 map = 19%,  reduce = 0%
2014-03-26 14:55:40,484 Stage-7 map = 20%,  reduce = 0%
2014-03-26 14:55:43,538 Stage-7 map = 21%,  reduce = 0%
2014-03-26 14:55:47,713 Stage-7 map = 22%,  reduce = 0%
2014-03-26 14:55:53,446 Stage-7 map = 23%,  reduce = 0%
2014-03-26 14:56:34,140 Stage-7 map = 24%,  reduce = 0%
2014-03-26 14:56:36,811 Stage-7 map = 29%,  reduce = 0%
2014-03-26 14:56:37,884 Stage-7 map = 31%,  reduce = 0%
2014-03-26 14:56:43,996 Stage-7 map = 32%,  reduce = 0%
2014-03-26 14:56:48,153 Stage-7 map = 33%,  reduce = 0%
2014-03-26 14:56:51,321 Stage-7 map = 34%,  reduce = 0%
2014-03-26 14:56:54,390 Stage-7 map = 35%,  reduce = 0%
2014-03-26 14:56:56,575 Stage-7 map = 36%,  reduce = 0%
2014-03-26 14:56:58,818 Stage-7 map = 37%,  reduce = 0%
2014-03-26 14:57:05,240 Stage-7 map = 38%,  reduce = 0%
2014-03-26 14:57:06,916 Stage-7 map = 40%,  reduce = 0%
2014-03-26 14:57:07,977 Stage-7 map = 42%,  reduce = 0%
2014-03-26 14:57:08,997 Stage-7 map = 43%,  reduce = 0%
2014-03-26 14:57:11,103 Stage-7 map = 44%,  reduce = 0%
2014-03-26 14:57:12,128 Stage-7 map = 45%,  reduce = 0%
2014-03-26 14:57:14,257 Stage-7 map = 46%,  reduce = 0%
2014-03-26 14:57:15,345 Stage-7 map = 47%,  reduce = 0%
2014-03-26 14:57:16,465 Stage-7 map = 48%,  reduce = 0%
2014-03-26 14:57:18,517 Stage-7 map = 49%,  reduce = 0%
2014-03-26 14:57:20,580 Stage-7 map = 50%,  reduce = 0%
2014-03-26 14:57:21,619 Stage-7 map = 51%,  reduce = 0%
2014-03-26 14:57:23,823 Stage-7 map = 52%,  reduce = 0%
2014-03-26 14:57:26,046 Stage-7 map = 53%,  reduce = 0%
2014-03-26 14:57:33,514 Stage-7 map = 54%,  reduce = 0%
2014-03-26 14:57:34,693 Stage-7 map = 57%,  reduce = 0%
2014-03-26 14:57:36,051 Stage-7 map = 58%,  reduce = 0%
2014-03-26 14:57:37,304 Stage-7 map = 59%,  reduce = 0%
2014-03-26 14:57:38,355 Stage-7 map = 60%,  reduce = 0%
2014-03-26 14:57:40,413 Stage-7 map = 61%,  reduce = 0%
2014-03-26 14:57:41,484 Stage-7 map = 62%,  reduce = 0%
2014-03-26 14:57:43,573 Stage-7 map = 63%,  reduce = 0%
2014-03-26 14:57:44,593 Stage-7 map = 64%,  reduce = 0%
2014-03-26 14:57:45,707 Stage-7 map = 65%,  reduce = 0%
2014-03-26 14:57:46,801 Stage-7 map = 66%,  reduce = 0%
2014-03-26 14:57:48,834 Stage-7 map = 67%,  reduce = 0%
2014-03-26 14:57:49,845 Stage-7 map = 68%,  reduce = 0%
2014-03-26 14:57:50,996 Stage-7 map = 69%,  reduce = 0%
2014-03-26 14:57:52,111 Stage-7 map = 70%,  reduce = 0%
2014-03-26 14:57:54,284 Stage-7 map = 71%,  reduce = 0%
2014-03-26 14:57:56,471 Stage-7 map = 72%,  reduce = 0%
2014-03-26 14:57:57,515 Stage-7 map = 73%,  reduce = 0%
2014-03-26 14:57:59,567 Stage-7 map = 74%,  reduce = 0%
2014-03-26 14:58:01,854 Stage-7 map = 75%,  reduce = 0%
2014-03-26 14:58:04,032 Stage-7 map = 76%,  reduce = 0%
2014-03-26 14:58:06,317 Stage-7 map = 77%,  reduce = 0%
2014-03-26 14:58:08,379 Stage-7 map = 78%,  reduce = 0%
2014-03-26 14:58:09,418 Stage-7 map = 79%,  reduce = 0%
2014-03-26 14:58:11,446 Stage-7 map = 80%,  reduce = 0%
2014-03-26 14:59:39,073 Stage-7 map = 81%,  reduce = 0%
2014-03-26 14:59:42,504 Stage-7 map = 93%,  reduce = 0%
2014-03-26 14:59:43,662 Stage-7 map = 97%,  reduce = 0%
2014-03-26 14:59:50,809 Stage-7 map = 97%,  reduce = 2%
2014-03-26 14:59:51,874 Stage-7 map = 97%,  reduce = 3%
2014-03-26 14:59:52,911 Stage-7 map = 97%,  reduce = 4%
2014-03-26 14:59:54,032 Stage-7 map = 98%,  reduce = 6%
2014-03-26 14:59:55,044 Stage-7 map = 98%,  reduce = 10%
2014-03-26 14:59:56,078 Stage-7 map = 98%,  reduce = 12%
2014-03-26 14:59:57,095 Stage-7 map = 98%,  reduce = 14%
2014-03-26 14:59:58,111 Stage-7 map = 98%,  reduce = 17%
2014-03-26 14:59:59,130 Stage-7 map = 98%,  reduce = 18%
2014-03-26 15:00:00,154 Stage-7 map = 99%,  reduce = 19%
2014-03-26 15:00:01,167 Stage-7 map = 99%,  reduce = 22%
2014-03-26 15:00:02,193 Stage-7 map = 99%,  reduce = 25%
2014-03-26 15:00:03,242 Stage-7 map = 99%,  reduce = 26%
2014-03-26 15:00:04,255 Stage-7 map = 99%,  reduce = 28%
2014-03-26 15:00:05,266 Stage-7 map = 99%,  reduce = 30%
2014-03-26 15:00:07,369 Stage-7 map = 99%,  reduce = 31%
2014-03-26 15:00:13,759 Stage-7 map = 99%,  reduce = 32%
2014-03-26 15:00:16,110 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:00:35,738 Stage-7 map = 100%,  reduce = 33%
2014-03-26 15:01:44,858 Stage-7 map = 100%,  reduce = 33%
2014-03-26 15:01:48,009 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:02:01,343 Stage-7 map = 100%,  reduce = 31%
2014-03-26 15:02:02,364 Stage-7 map = 100%,  reduce = 30%
2014-03-26 15:02:12,795 Stage-7 map = 100%,  reduce = 31%
2014-03-26 15:02:14,863 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:02:16,904 Stage-7 map = 100%,  reduce = 33%
2014-03-26 15:03:04,215 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:04:11,725 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:05:23,190 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:05:27,280 Stage-7 map = 100%,  reduce = 31%
2014-03-26 15:07:18,690 Stage-7 map = 100%,  reduce = 31%
2014-03-26 15:09:28,738 Stage-7 map = 100%,  reduce = 31%
2014-03-26 15:10:27,890 Stage-7 map = 100%,  reduce = 30%
2014-03-26 15:10:49,784 Stage-7 map = 100%,  reduce = 29%
2014-03-26 15:11:23,510 Stage-7 map = 100%,  reduce = 28%
2014-03-26 15:12:45,171 Stage-7 map = 100%,  reduce = 28%
2014-03-26 15:13:55,544 Stage-7 map = 100%,  reduce = 28%
2014-03-26 15:14:15,809 Stage-7 map = 100%,  reduce = 27%
2014-03-26 15:14:32,545 Stage-7 map = 100%,  reduce = 28%
2014-03-26 15:14:55,085 Stage-7 map = 100%,  reduce = 29%
2014-03-26 15:16:19,993 Stage-7 map = 100%,  reduce = 29%
2014-03-26 15:16:38,748 Stage-7 map = 100%,  reduce = 27%
2014-03-26 15:17:21,951 Stage-7 map = 100%,  reduce = 26%
2014-03-26 15:17:36,865 Stage-7 map = 100%,  reduce = 27%
2014-03-26 15:18:57,348 Stage-7 map = 100%,  reduce = 27%
2014-03-26 15:19:02,024 Stage-7 map = 100%,  reduce = 26%
2014-03-26 15:19:32,390 Stage-7 map = 100%,  reduce = 29%
2014-03-26 15:19:34,195 Stage-7 map = 100%,  reduce = 32%
2014-03-26 15:19:35,357 Stage-7 map = 100%,  reduce = 33%
2014-03-26 15:20:13,009 Stage-7 map = 99%,  reduce = 33%
2014-03-26 15:21:13,090 Stage-7 map = 99%,  reduce = 33%
2014-03-26 15:21:16,685 Stage-7 map = 100%,  reduce = 33%
2014-03-26 15:23:26,514 Stage-7 map = 100%,  reduce = 33%
2014-03-26 15:24:26,086 Stage-7 map = 100%,  reduce = 74%
2014-03-26 15:25:11,195 Stage-7 map = 100%,  reduce = 91%
2014-03-26 15:25:23,540 Stage-7 map = 100%,  reduce = 99%
2014-03-26 15:25:30,879 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201402191020_878947
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879375, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879375
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879375
2014-03-26 15:26:33,193 Stage-6 map = 0%,  reduce = 0%
2014-03-26 15:26:37,827 Stage-6 map = 7%,  reduce = 0%
2014-03-26 15:26:39,977 Stage-6 map = 67%,  reduce = 0%
2014-03-26 15:26:41,426 Stage-6 map = 80%,  reduce = 0%
2014-03-26 15:26:43,153 Stage-6 map = 87%,  reduce = 0%
2014-03-26 15:27:01,424 Stage-6 map = 93%,  reduce = 0%
2014-03-26 15:28:01,623 Stage-6 map = 93%,  reduce = 0%
2014-03-26 15:28:54,385 Stage-6 map = 100%,  reduce = 0%
2014-03-26 15:29:03,679 Stage-6 map = 100%,  reduce = 31%
2014-03-26 15:29:08,273 Stage-6 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879375
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879473, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879473
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879473
2014-03-26 15:29:17,348 Stage-8 map = 0%,  reduce = 0%
2014-03-26 15:29:21,481 Stage-8 map = 20%,  reduce = 0%
2014-03-26 15:29:22,628 Stage-8 map = 31%,  reduce = 0%
2014-03-26 15:29:23,788 Stage-8 map = 44%,  reduce = 0%
2014-03-26 15:29:24,987 Stage-8 map = 61%,  reduce = 0%
2014-03-26 15:29:26,116 Stage-8 map = 71%,  reduce = 0%
2014-03-26 15:29:27,128 Stage-8 map = 79%,  reduce = 0%
2014-03-26 15:29:28,713 Stage-8 map = 87%,  reduce = 0%
2014-03-26 15:29:30,156 Stage-8 map = 94%,  reduce = 0%
2014-03-26 15:29:31,406 Stage-8 map = 96%,  reduce = 0%
2014-03-26 15:29:32,644 Stage-8 map = 98%,  reduce = 0%
2014-03-26 15:29:33,717 Stage-8 map = 99%,  reduce = 0%
2014-03-26 15:29:36,880 Stage-8 map = 100%,  reduce = 0%
2014-03-26 15:29:39,275 Stage-8 map = 100%,  reduce = 14%
2014-03-26 15:29:44,062 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879473
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879494, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879494
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879494
2014-03-26 15:29:48,255 Stage-2 map = 0%,  reduce = 0%
2014-03-26 15:29:58,044 Stage-2 map = 100%,  reduce = 0%
2014-03-26 15:30:11,332 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879494
Launching Job 7 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879518, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879518
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879518
2014-03-26 15:30:18,903 Stage-3 map = 0%,  reduce = 0%
2014-03-26 15:30:22,123 Stage-3 map = 33%,  reduce = 0%
2014-03-26 15:30:25,205 Stage-3 map = 100%,  reduce = 0%
2014-03-26 15:30:33,605 Stage-3 map = 100%,  reduce = 33%
2014-03-26 15:30:35,405 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879518
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_bidt_0325
4 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-26_14-40-02_944_4345896681411079066/-ext-10000
OK
Time taken: 3034.152 seconds
hive -e "

add file trans_order.py;
 
create table if not exists user_wangwentao_yhd_ordert_0325 as 
select transform(*) using 'trans_order.py' as (cookie,ts,bid,sid,orderid,ordervalue) from
( 
select cookie,ts,bid,sid,orderid,ordervalue
from
(
select ct.cookie cookie,ct.ts ts, cast(ct.bid as string) bid, cast(ct.sid as string) sid, cast('0' as string) orderid,cast('0' as string) ordervalue from
(select sawlog_.rawlog_.allyesid_ cookie,
 from_unixtime(sawlog_.rawlog_.timestamp_) ts,
sawlog_.rawlog_.bannerid_ bid,sawlog_.rawlog_.solutionid_ sid
from pb_clicklog where dd<='2014-03-25' and dd>='2014-03-24' and product='ifc' and db='ifc' and sawlog_.advertiserid_='49')ct join  
(
 
select
distinct rawlog_.allyesid_ cookie
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-25' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 

)ot on ct.cookie=ot.cookie

union all

 select cookie,ts,cast('' as string) bid,cast('' as string) sid, cast(orderid as string) orderid, cast(ordervalue as string) ordervalue from
(select
rawlog_.allyesid_ cookie, 
from_unixtime(rawlog_.timestamp_) ts,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2] orderid,
split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[3] ordervalue
from pb_idigger_tracklog_partial where
sitecode_= 'T-000049-01' and dd='2014-03-25' and product='idigger'  
and  db='ifc'  and length(split(parse_url(rawlog_.requesturl_,'QUERY','ecm'),'%60')[2])>0 order by orderid,ts

)ot2

)xa order by cookie,ts

)ordertmp
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403261530_352385901.txt
Added resource: trans_order.py
Total MapReduce jobs = 4
Launching Job 1 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879543, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879543
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879543
2014-03-26 15:31:15,553 Stage-1 map = 0%,  reduce = 0%
2014-03-26 15:31:19,786 Stage-1 map = 1%,  reduce = 0%
2014-03-26 15:31:22,126 Stage-1 map = 2%,  reduce = 0%
2014-03-26 15:31:28,862 Stage-1 map = 4%,  reduce = 0%
2014-03-26 15:31:31,214 Stage-1 map = 48%,  reduce = 0%
2014-03-26 15:31:32,433 Stage-1 map = 60%,  reduce = 0%
2014-03-26 15:31:33,489 Stage-1 map = 62%,  reduce = 0%
2014-03-26 15:31:39,446 Stage-1 map = 67%,  reduce = 0%
2014-03-26 15:31:42,404 Stage-1 map = 80%,  reduce = 0%
2014-03-26 15:31:43,703 Stage-1 map = 83%,  reduce = 0%
2014-03-26 15:31:46,118 Stage-1 map = 86%,  reduce = 0%
2014-03-26 15:31:47,380 Stage-1 map = 87%,  reduce = 0%
2014-03-26 15:31:48,721 Stage-1 map = 92%,  reduce = 0%
2014-03-26 15:31:49,881 Stage-1 map = 95%,  reduce = 0%
2014-03-26 15:31:50,962 Stage-1 map = 97%,  reduce = 0%
2014-03-26 15:31:56,139 Stage-1 map = 98%,  reduce = 0%
2014-03-26 15:31:59,222 Stage-1 map = 99%,  reduce = 0%
2014-03-26 15:32:01,871 Stage-1 map = 99%,  reduce = 31%
2014-03-26 15:32:08,513 Stage-1 map = 99%,  reduce = 33%
2014-03-26 15:32:19,496 Stage-1 map = 100%,  reduce = 33%
2014-03-26 15:32:25,172 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879543
Launching Job 2 out of 4
Number of reduce tasks not specified. Estimated from input data size: 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879594, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879594
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879594
2014-03-26 15:32:31,761 Stage-3 map = 0%,  reduce = 0%
2014-03-26 15:32:37,188 Stage-3 map = 4%,  reduce = 0%
2014-03-26 15:32:38,274 Stage-3 map = 8%,  reduce = 0%
2014-03-26 15:32:39,339 Stage-3 map = 11%,  reduce = 0%
2014-03-26 15:32:40,441 Stage-3 map = 14%,  reduce = 0%
2014-03-26 15:32:41,465 Stage-3 map = 19%,  reduce = 0%
2014-03-26 15:32:42,495 Stage-3 map = 21%,  reduce = 0%
2014-03-26 15:32:43,504 Stage-3 map = 31%,  reduce = 0%
2014-03-26 15:33:23,201 Stage-3 map = 35%,  reduce = 0%
2014-03-26 15:33:30,321 Stage-3 map = 51%,  reduce = 0%
2014-03-26 15:33:31,415 Stage-3 map = 55%,  reduce = 0%
2014-03-26 15:33:32,518 Stage-3 map = 62%,  reduce = 0%
2014-03-26 15:33:40,167 Stage-3 map = 66%,  reduce = 0%
2014-03-26 15:33:41,547 Stage-3 map = 97%,  reduce = 0%
2014-03-26 15:33:42,632 Stage-3 map = 98%,  reduce = 0%
2014-03-26 15:33:47,875 Stage-3 map = 99%,  reduce = 0%
2014-03-26 15:33:48,897 Stage-3 map = 100%,  reduce = 10%
2014-03-26 15:33:49,908 Stage-3 map = 100%,  reduce = 20%
2014-03-26 15:33:52,098 Stage-3 map = 100%,  reduce = 32%
2014-03-26 15:33:54,195 Stage-3 map = 100%,  reduce = 33%
2014-03-26 15:33:57,404 Stage-3 map = 100%,  reduce = 55%
2014-03-26 15:33:58,785 Stage-3 map = 100%,  reduce = 78%
2014-03-26 15:33:59,806 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879594
Launching Job 3 out of 4
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879626, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879626
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879626
2014-03-26 15:34:14,598 Stage-4 map = 0%,  reduce = 0%
2014-03-26 15:34:17,655 Stage-4 map = 6%,  reduce = 0%
2014-03-26 15:34:18,664 Stage-4 map = 13%,  reduce = 0%
2014-03-26 15:34:19,681 Stage-4 map = 25%,  reduce = 0%
2014-03-26 15:34:20,760 Stage-4 map = 40%,  reduce = 0%
2014-03-26 15:34:21,769 Stage-4 map = 60%,  reduce = 0%
2014-03-26 15:34:22,780 Stage-4 map = 74%,  reduce = 0%
2014-03-26 15:34:23,822 Stage-4 map = 80%,  reduce = 0%
2014-03-26 15:34:24,833 Stage-4 map = 86%,  reduce = 0%
2014-03-26 15:34:25,870 Stage-4 map = 88%,  reduce = 0%
2014-03-26 15:34:26,899 Stage-4 map = 89%,  reduce = 0%
2014-03-26 15:34:27,911 Stage-4 map = 91%,  reduce = 0%
2014-03-26 15:34:32,246 Stage-4 map = 92%,  reduce = 0%
2014-03-26 15:34:33,309 Stage-4 map = 93%,  reduce = 0%
2014-03-26 15:34:34,383 Stage-4 map = 94%,  reduce = 0%
2014-03-26 15:35:10,776 Stage-4 map = 95%,  reduce = 0%
2014-03-26 15:35:13,001 Stage-4 map = 97%,  reduce = 0%
2014-03-26 15:35:14,292 Stage-4 map = 98%,  reduce = 0%
2014-03-26 15:35:20,584 Stage-4 map = 98%,  reduce = 32%
2014-03-26 15:35:24,648 Stage-4 map = 98%,  reduce = 33%
2014-03-26 15:37:20,829 Stage-4 map = 99%,  reduce = 33%
2014-03-26 15:37:22,224 Stage-4 map = 100%,  reduce = 33%
2014-03-26 15:37:32,917 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879626
Launching Job 4 out of 4
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879667, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879667
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879667
2014-03-26 15:38:45,878 Stage-2 map = 0%,  reduce = 0%
2014-03-26 15:38:51,122 Stage-2 map = 50%,  reduce = 0%
2014-03-26 15:38:53,143 Stage-2 map = 100%,  reduce = 0%
2014-03-26 15:39:02,406 Stage-2 map = 100%,  reduce = 17%
2014-03-26 15:39:05,432 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879667
Moving data to: hdfs://nn.jumbo.allyes.com:9000/user/group_dataanalysis/warehouse/user_wangwentao_yhd_ordert_0325
8 Rows loaded to hdfs://nn.jumbo.allyes.com:9000/tmp/hive-group_dataanalysis/hive_2014-03-26_15-30-38_915_2486962074455941654/-ext-10000
OK
Time taken: 510.831 seconds
hive -e " 
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25'
select a.sid sid,c.name,bidn,winn,winrate,clk,ctr,ws,cpm,cpc,avgbidfloor,avgbidprice,ordernum,ordervaluesum
from
(select sid, bidn, winn,winn/bidn winrate, clk, clk/winn ctr, ws,
ws*1000/winn cpm,
ws/clk cpc,
bidfloor/bidn avgbidfloor,
bidprice/bidn avgbidprice 
from user_wangwentao_yhd_bidt_0325 )a left outer join
(select sid,count(*) ordernum,sum(ordervalue) ordervaluesum from user_wangwentao_yhd_ordert_0325 group by sid)b on a.sid=b.sid
left outer join
(select id,name from b_solution where advertiser_id='49')c on a.sid=c.id order by sid
"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403261539_1915714810.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879693, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879693
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879693
2014-03-26 15:39:19,220 Stage-4 map = 0%,  reduce = 0%
2014-03-26 15:39:24,352 Stage-4 map = 100%,  reduce = 0%
2014-03-26 15:39:55,696 Stage-4 map = 100%,  reduce = 33%
2014-03-26 15:39:56,922 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879693
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879708, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879708
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879708
2014-03-26 15:40:18,527 Stage-1 map = 0%,  reduce = 0%
2014-03-26 15:40:25,647 Stage-1 map = 67%,  reduce = 0%
2014-03-26 15:40:31,073 Stage-1 map = 100%,  reduce = 0%
2014-03-26 15:40:39,237 Stage-1 map = 100%,  reduce = 33%
2014-03-26 15:40:40,257 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879708
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879716, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879716
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879716
2014-03-26 15:42:01,725 Stage-2 map = 0%,  reduce = 0%
2014-03-26 15:42:09,221 Stage-2 map = 100%,  reduce = 0%
2014-03-26 15:43:15,514 Stage-2 map = 100%,  reduce = 0%
2014-03-26 15:43:18,299 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879716
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25
4 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25
OK
Time taken: 248.328 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25/* > /home/group_dataanalysis/yhd_Report/res-2014-03-25/output-1
14/03/26 15:43:20 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/26 15:43:20 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
hive -e "
  
insert overwrite  directory '/user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25'

select a.sid,b.name,a.bid,c.name,ts,orderid,ordervalue from
(select sid,bid,ts,orderid,ordervalue from user_wangwentao_yhd_ordert_0325 )a left outer join
(select id,name from b_solution where advertiser_id='49')b on a.sid=b.id left outer join
(select id,name from b_banner where advertiser_id='49')c on a.bid=c.id order by ts


;"
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.
Hive history file=/tmp/group_dataanalysis/hive_job_log_group_dataanalysis_201403261543_205447348.txt
Total MapReduce jobs = 3
Launching Job 1 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879735, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879735
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879735
2014-03-26 15:43:29,126 Stage-1 map = 0%,  reduce = 0%
2014-03-26 15:43:37,794 Stage-1 map = 50%,  reduce = 0%
2014-03-26 15:44:08,687 Stage-1 map = 100%,  reduce = 0%
2014-03-26 15:44:20,236 Stage-1 map = 100%,  reduce = 33%
2014-03-26 15:44:21,315 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879735
Launching Job 2 out of 3
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879753, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879753
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879753
2014-03-26 15:44:34,182 Stage-2 map = 0%,  reduce = 0%
2014-03-26 15:44:40,766 Stage-2 map = 100%,  reduce = 0%
2014-03-26 15:44:53,380 Stage-2 map = 100%,  reduce = 100%
2014-03-26 15:45:54,089 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879753
Launching Job 3 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201402191020_879782, Tracking URL = http://jt.jumbo.allyes.com:50030/jobdetails.jsp?jobid=job_201402191020_879782
Kill Command = /opt/hadoop/current/bin/hadoop job  -Dmapred.job.tracker=jt.jumbo.allyes.com:9001 -kill job_201402191020_879782
2014-03-26 15:46:56,677 Stage-3 map = 0%,  reduce = 0%
2014-03-26 15:47:39,636 Stage-3 map = 100%,  reduce = 0%
2014-03-26 15:48:25,630 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201402191020_879782
Moving data to: /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25
8 Rows loaded to /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25
OK
Time taken: 305.847 seconds
hadoop fs -text   /user/group_dataanalysis/yhd_dsp/order_parse/res-2014-03-25/* > /home/group_dataanalysis/yhd_Report/res-2014-03-25/output-2
14/03/26 15:48:28 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
14/03/26 15:48:28 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 4537f94556c9ba71ffca316514e0c0101f76b63b]
#######################DROP HIVE TABLE#########################
#######################UPDATE ORDER STATUS#########################
1395820109.47
total time is :  0.101403951645 seconds
size is       :  2942 KB
speed is      :  0.0290126760573 MB/s
140325TH0RNK 38
140325FGPXA4 20
140325TH0NAE 34
140325RGUNR7 38
140325AH00GH 34
140325XH0MSL 3
140325FGVVAQ 34
140325TGQKT6 20
#######################SAVE DATE TO FILE#########################
#######################MAIL FILE#########################
<logging.Logger instance at 0x2b7851b85fc8>
{'loglevel': 'debug', 'debug': False, 'logfile': None}
['res-2014-03-25', '0325', '2014-03-25']
res-2014-03-25/yhd_dsp_report_0325.xls
mail_exchange -s '[2014-03-25]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-25/yhd_dsp_report_0325.xls' -u 'wentao_wang' -p '2238681Xwww' 
Main       2014-03-26 15:48:30,924 INFO  executing: mail_exchange -s '[2014-03-25]yhd_dsp_performance' -f 'mail_body.txt' -r 'assure_xu@allyes.com,kewei_hong@allyes.com,yuying_shi@allyes.com,shuangping_shen@allyes.com,fake_li@allyes.com,hfeng@fountontech.com,lchen@fountontech.com' --cc_recipients 'tony_zhang@allyes.com,shawn_gao@allyes.com,hua.cao@fountontech.com,wentao_wang@allyes.com' -a 'res-2014-03-25/yhd_dsp_report_0325.xls' -u 'wentao_wang' -p '2238681Xwww' 
20140326_154833
